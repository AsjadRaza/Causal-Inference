{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Copy_of_Causal_Assignment_handout.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDEnfPD1LOed"
      },
      "source": [
        "# Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Snr_CjcABfhV",
        "outputId": "e3488521-4c0f-4478-d571-d5e1bb28dbf0"
      },
      "source": [
        "!pip install rpy2==2.9.6b"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting rpy2==2.9.6b\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8a/eb/2f7115990ee54eaa075f6c10d06a6225531b2a643b5779d30dda601cdbed/rpy2-2.9.6b0.tar.gz (194kB)\n",
            "\r\u001b[K     |█▊                              | 10kB 16.4MB/s eta 0:00:01\r\u001b[K     |███▍                            | 20kB 21.1MB/s eta 0:00:01\r\u001b[K     |█████                           | 30kB 14.3MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 40kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 51kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 61kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 71kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 81kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 92kB 6.3MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 102kB 6.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 112kB 6.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 122kB 6.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 133kB 6.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 143kB 6.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 153kB 6.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 163kB 6.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 174kB 6.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 184kB 6.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 194kB 6.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from rpy2==2.9.6b) (1.15.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from rpy2==2.9.6b) (2.11.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->rpy2==2.9.6b) (1.1.1)\n",
            "Building wheels for collected packages: rpy2\n",
            "  Building wheel for rpy2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rpy2: filename=rpy2-2.9.6b0-cp36-cp36m-linux_x86_64.whl size=316067 sha256=495029ee7f4d9e1960f87c758fbf495a04f93f958ca3a51c44d5d276f91621ba\n",
            "  Stored in directory: /root/.cache/pip/wheels/02/d6/dc/8c3faafb8cb7165a30d67f899ff7e88766e20260dda41ace88\n",
            "Successfully built rpy2\n",
            "Installing collected packages: rpy2\n",
            "  Found existing installation: rpy2 3.2.7\n",
            "    Uninstalling rpy2-3.2.7:\n",
            "      Successfully uninstalled rpy2-3.2.7\n",
            "Successfully installed rpy2-2.9.6b0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mVRUQreMkgc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25506efd-baa9-45ad-b765-4aaea12d62aa"
      },
      "source": [
        "%load_ext rpy2.ipython"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/rpy2/robjects/pandas2ri.py:17: FutureWarning: pandas.core.index is deprecated and will be removed in a future version.  The public classes are available in the top-level namespace.\n",
            "  from pandas.core.index import Index as PandasIndex\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gGjI1cMNsCX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10ed3c8e-7930-4d19-dd80-e565414ef117"
      },
      "source": [
        "%%R\n",
        "install.packages(\"grf\")\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "  warnings.warn(x, RRuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: also installing the dependencies ‘zoo’, ‘DiceKriging’, ‘lmtest’, ‘sandwich’, ‘RcppEigen’\n",
            "\n",
            "\n",
            "  warnings.warn(x, RRuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: trying URL 'https://cran.rstudio.com/src/contrib/zoo_1.8-8.tar.gz'\n",
            "\n",
            "  warnings.warn(x, RRuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: Content type 'application/x-gzip'\n",
            "  warnings.warn(x, RRuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning:  length 849487 bytes (829 KB)\n",
            "\n",
            "  warnings.warn(x, RRuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: =\n",
            "  warnings.warn(x, RRuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: \n",
            "\n",
            "  warnings.warn(x, RRuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: downloaded 829 KB\n",
            "\n",
            "\n",
            "  warnings.warn(x, RRuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: trying URL 'https://cran.rstudio.com/src/contrib/DiceKriging_1.5.8.tar.gz'\n",
            "\n",
            "  warnings.warn(x, RRuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning:  length 95896 bytes (93 KB)\n",
            "\n",
            "  warnings.warn(x, RRuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: downloaded 93 KB\n",
            "\n",
            "\n",
            "  warnings.warn(x, RRuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: trying URL 'https://cran.rstudio.com/src/contrib/lmtest_0.9-38.tar.gz'\n",
            "\n",
            "  warnings.warn(x, RRuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning:  length 227052 bytes (221 KB)\n",
            "\n",
            "  warnings.warn(x, RRuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: downloaded 221 KB\n",
            "\n",
            "\n",
            "  warnings.warn(x, RRuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: trying URL 'https://cran.rstudio.com/src/contrib/sandwich_3.0-0.tar.gz'\n",
            "\n",
            "  warnings.warn(x, RRuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning:  length 1445320 bytes (1.4 MB)\n",
            "\n",
            "  warnings.warn(x, RRuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: downloaded 1.4 MB\n",
            "\n",
            "\n",
            "  warnings.warn(x, RRuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: trying URL 'https://cran.rstudio.com/src/contrib/RcppEigen_0.3.3.7.0.tar.gz'\n",
            "\n",
            "  warnings.warn(x, RRuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning:  length 1643103 bytes (1.6 MB)\n",
            "\n",
            "  warnings.warn(x, RRuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: downloaded 1.6 MB\n",
            "\n",
            "\n",
            "  warnings.warn(x, RRuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: trying URL 'https://cran.rstudio.com/src/contrib/grf_1.2.0.tar.gz'\n",
            "\n",
            "  warnings.warn(x, RRuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning:  length 136352 bytes (133 KB)\n",
            "\n",
            "  warnings.warn(x, RRuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: downloaded 133 KB\n",
            "\n",
            "\n",
            "  warnings.warn(x, RRuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: \n",
            "  warnings.warn(x, RRuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: The downloaded source packages are in\n",
            "\t‘/tmp/RtmpuEDFqQ/downloaded_packages’\n",
            "  warnings.warn(x, RRuntimeWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2WB2i7qbusi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb5aefe6-5eff-41b5-9f09-04ddb8f8da25"
      },
      "source": [
        "!pip install justcause==0.3.2"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting justcause==0.3.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8d/ec/767d453b101a39a719167408e5687863fde44cee57b1a844b4ee157f707d/JustCause-0.3.2-py2.py3-none-any.whl (50kB)\n",
            "\r\u001b[K     |██████▌                         | 10kB 15.5MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 20kB 15.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 30kB 12.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 40kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 3.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from justcause==0.3.2) (0.22.2.post1)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.6/dist-packages (from justcause==0.3.2) (0.14.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from justcause==0.3.2) (1.18.5)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.6/dist-packages (from justcause==0.3.2) (1.1.5)\n",
            "Collecting causalml\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/be/5a/bd02301c37208c30b8b8556281e7bf6c72b5171369f9d3405f4d273ddc29/causalml-0.9.0.tar.gz (236kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 7.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: rpy2 in /usr/local/lib/python3.6/dist-packages (from justcause==0.3.2) (2.9.6b0)\n",
            "Collecting pygam\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/be/775033ef08a8945bec6ad7973b161ca909f852442e0d7cfb8d1a214de1ac/pygam-0.8.0-py2.py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 9.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->justcause==0.3.2) (0.17.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->justcause==0.3.2) (1.4.1)\n",
            "Requirement already satisfied: six>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pyarrow->justcause==0.3.2) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23->justcause==0.3.2) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23->justcause==0.3.2) (2018.9)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from causalml->justcause==0.3.2) (50.3.2)\n",
            "Requirement already satisfied: pip>=10.0 in /usr/local/lib/python3.6/dist-packages (from causalml->justcause==0.3.2) (19.3.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from causalml->justcause==0.3.2) (3.2.2)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from causalml->justcause==0.3.2) (0.10.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (from causalml->justcause==0.3.2) (0.11.0)\n",
            "Requirement already satisfied: Cython>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from causalml->justcause==0.3.2) (0.29.21)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.6/dist-packages (from causalml->justcause==0.3.2) (0.90)\n",
            "Requirement already satisfied: pydotplus in /usr/local/lib/python3.6/dist-packages (from causalml->justcause==0.3.2) (2.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from causalml->justcause==0.3.2) (4.41.1)\n",
            "Collecting shap\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/a3/c0eab9dd6a894165e2cb87504ff5b2710ac5ede3447d9138620b7341b6a2/shap-0.37.0.tar.gz (326kB)\n",
            "\u001b[K     |████████████████████████████████| 327kB 18.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from causalml->justcause==0.3.2) (0.3.3)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.6/dist-packages (from causalml->justcause==0.3.2) (2.2.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from causalml->justcause==0.3.2) (20.7)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from causalml->justcause==0.3.2) (2.4.3)\n",
            "Requirement already satisfied: tensorflow>=1.15.2 in /usr/local/lib/python3.6/dist-packages (from causalml->justcause==0.3.2) (2.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from rpy2->justcause==0.3.2) (2.11.2)\n",
            "Requirement already satisfied: progressbar2 in /usr/local/lib/python3.6/dist-packages (from pygam->justcause==0.3.2) (3.38.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pygam->justcause==0.3.2) (0.16.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->causalml->justcause==0.3.2) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->causalml->justcause==0.3.2) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->causalml->justcause==0.3.2) (2.4.7)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from statsmodels>=0.9.0->causalml->justcause==0.3.2) (0.5.1)\n",
            "Collecting slicer==0.0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/02/a6/c708c5a0f338e99cfbcb6288b88794525548e4fc1b8457feec2c552a81a4/slicer-0.0.3-py3-none-any.whl\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from shap->causalml->justcause==0.3.2) (0.48.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->causalml->justcause==0.3.2) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->causalml->justcause==0.3.2) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.15.2->causalml->justcause==0.3.2) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.15.2->causalml->justcause==0.3.2) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.15.2->causalml->justcause==0.3.2) (2.3.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.15.2->causalml->justcause==0.3.2) (3.3.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.15.2->causalml->justcause==0.3.2) (0.2.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.15.2->causalml->justcause==0.3.2) (0.3.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.15.2->causalml->justcause==0.3.2) (0.36.1)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.15.2->causalml->justcause==0.3.2) (3.12.4)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.15.2->causalml->justcause==0.3.2) (2.3.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.15.2->causalml->justcause==0.3.2) (1.34.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.15.2->causalml->justcause==0.3.2) (1.6.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.15.2->causalml->justcause==0.3.2) (1.12.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.15.2->causalml->justcause==0.3.2) (0.10.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->rpy2->justcause==0.3.2) (1.1.1)\n",
            "Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from progressbar2->pygam->justcause==0.3.2) (2.4.0)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba->shap->causalml->justcause==0.3.2) (0.31.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.15.2->causalml->justcause==0.3.2) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.15.2->causalml->justcause==0.3.2) (3.3.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.15.2->causalml->justcause==0.3.2) (0.4.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.15.2->causalml->justcause==0.3.2) (1.7.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.15.2->causalml->justcause==0.3.2) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.15.2->causalml->justcause==0.3.2) (1.17.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=1.15.2->causalml->justcause==0.3.2) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=1.15.2->causalml->justcause==0.3.2) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=1.15.2->causalml->justcause==0.3.2) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=1.15.2->causalml->justcause==0.3.2) (2.10)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow>=1.15.2->causalml->justcause==0.3.2) (3.1.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=1.15.2->causalml->justcause==0.3.2) (1.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=1.15.2->causalml->justcause==0.3.2) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=1.15.2->causalml->justcause==0.3.2) (4.1.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=1.15.2->causalml->justcause==0.3.2) (4.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow>=1.15.2->causalml->justcause==0.3.2) (3.4.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=1.15.2->causalml->justcause==0.3.2) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=1.15.2->causalml->justcause==0.3.2) (0.4.8)\n",
            "Building wheels for collected packages: causalml, shap\n",
            "  Building wheel for causalml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for causalml: filename=causalml-0.9.0-cp36-cp36m-linux_x86_64.whl size=482217 sha256=45daaf0af4ffd17785b876b66a54f42b4e72b05e8dd28b3fa06e4ccfa26a779e\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/c4/e2/451f0ebc2f9a7540256b0a705a2fd09864893131aee58d5af9\n",
            "  Building wheel for shap (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for shap: filename=shap-0.37.0-cp36-cp36m-linux_x86_64.whl size=463916 sha256=d2df913cd43a27f03baa4a81afeddda56c612489b03297d127159e496b945ccd\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/ad/b0/aa7815ec68850d66551ef618095eccb962c8f6022f1d3dd989\n",
            "Successfully built causalml shap\n",
            "Installing collected packages: slicer, shap, pygam, causalml, justcause\n",
            "Successfully installed causalml-0.9.0 justcause-0.3.2 pygam-0.8.0 shap-0.37.0 slicer-0.0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPzAPlfebvbX"
      },
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4UpBS6mNOCW"
      },
      "source": [
        "from justcause.data import Col\n",
        "from justcause.data.sets import load_ihdp\n",
        "from justcause.metrics import pehe_score, mean_absolute\n",
        "from justcause.evaluation import calc_scores, summarize_scores"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CfvwSmoLOe-"
      },
      "source": [
        "%load_ext autoreload\n",
        "\n",
        "%autoreload 2\n",
        "\n",
        "# Loading all required packages \n",
        "import itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jufTu9k5LOfL"
      },
      "source": [
        "#### Infant Health Development Program Data-Set used in this exercise\n",
        "\n",
        "- Original study constructed to study the effect of special child care for low birthweight, premature infants.\n",
        "- In total, six continuous and 19 binary pretreatment variables\n",
        "- Using the covariates of all instances in both treatment groups, the potential outcomes are generated synthetically\n",
        "- Finally, manipulation of observational study by omitting a non-random set of samples from the treatment group.\n",
        "- The way the subset is generated from the experimental data does not ensure complete overlap - latent confounder\n",
        "- Specifically, the observational subset is created by throwing away the set of all children with nonwhite mothers from the treatment group\n",
        "- Following data generation process used for potentia outcomes\n",
        "- After the adaptions from Hill, we are left with 139 instances in the treated group and 608 instances in the control group."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLQd0vO_LOfP"
      },
      "source": [
        "# 1. Running the causal models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yg5b4PoxLOfU"
      },
      "source": [
        "# We Import the IHDP data-set \n",
        "# There are 1000 replications in this data-set, each with a different individual treament effect\n",
        "# produced from an underlying generative function. \n",
        "# Check out https://justcause.readthedocs.io/en/latest/\n",
        "\n",
        "\n",
        "# We load 100 of the 1000 data-sets\n",
        "replications = load_ihdp(select_rep=np.arange(100))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyTH9JV_LOfh"
      },
      "source": [
        "# Defining global parameters\n",
        "train_size = 0.8        # Size of the training data-set \n",
        "random_state = 42        # Setting the random state\n",
        "\n",
        "n= 0       # number of the data-sets we look at \n",
        "\n",
        "metrics = [pehe_score, mean_absolute]    ## Defining the metrics that will be calculated below"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4FZXFZh0Lt7"
      },
      "source": [
        "```\r\n",
        "t     => treatment (0 or 1)\r\n",
        "\r\n",
        "y     => factual outcome\r\n",
        "\r\n",
        "y_cf  => counterfactial outcome\r\n",
        "\r\n",
        "y_0   => potential outcome of treatment t=0\r\n",
        "\r\n",
        "y_1   => potential outcome of treatment t=1\r\n",
        "\r\n",
        "ite   => indivisual treatment effect  (y_1 - y_0)\r\n",
        "\r\n",
        "```\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x31zQ4qEiq5H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "23802f00-6fc6-4b35-8928-4b45c107ab95"
      },
      "source": [
        "replications[0].head()"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sample_id</th>\n",
              "      <th>x_0</th>\n",
              "      <th>x_1</th>\n",
              "      <th>x_2</th>\n",
              "      <th>x_3</th>\n",
              "      <th>x_4</th>\n",
              "      <th>x_5</th>\n",
              "      <th>x_6</th>\n",
              "      <th>x_7</th>\n",
              "      <th>x_8</th>\n",
              "      <th>x_9</th>\n",
              "      <th>x_10</th>\n",
              "      <th>x_11</th>\n",
              "      <th>x_12</th>\n",
              "      <th>x_13</th>\n",
              "      <th>x_14</th>\n",
              "      <th>x_15</th>\n",
              "      <th>x_16</th>\n",
              "      <th>x_17</th>\n",
              "      <th>x_18</th>\n",
              "      <th>x_19</th>\n",
              "      <th>x_20</th>\n",
              "      <th>x_21</th>\n",
              "      <th>x_22</th>\n",
              "      <th>x_23</th>\n",
              "      <th>x_24</th>\n",
              "      <th>t</th>\n",
              "      <th>y</th>\n",
              "      <th>mu_1</th>\n",
              "      <th>mu_0</th>\n",
              "      <th>y_cf</th>\n",
              "      <th>ite</th>\n",
              "      <th>y_0</th>\n",
              "      <th>y_1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1.397395</td>\n",
              "      <td>0.996346</td>\n",
              "      <td>-1.105624</td>\n",
              "      <td>-0.879606</td>\n",
              "      <td>0.308569</td>\n",
              "      <td>-1.023402</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.771232</td>\n",
              "      <td>5.822879</td>\n",
              "      <td>1.164950</td>\n",
              "      <td>-0.298509</td>\n",
              "      <td>4.657928</td>\n",
              "      <td>1.164950</td>\n",
              "      <td>5.822879</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>1</td>\n",
              "      <td>0.269033</td>\n",
              "      <td>0.196818</td>\n",
              "      <td>0.383828</td>\n",
              "      <td>0.161703</td>\n",
              "      <td>-0.629189</td>\n",
              "      <td>1.460832</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.956273</td>\n",
              "      <td>6.920703</td>\n",
              "      <td>3.492099</td>\n",
              "      <td>5.783770</td>\n",
              "      <td>3.428604</td>\n",
              "      <td>3.492099</td>\n",
              "      <td>6.920703</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200</th>\n",
              "      <td>2</td>\n",
              "      <td>1.051537</td>\n",
              "      <td>1.795874</td>\n",
              "      <td>-1.105624</td>\n",
              "      <td>0.161703</td>\n",
              "      <td>-0.629189</td>\n",
              "      <td>0.963985</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.164164</td>\n",
              "      <td>6.821964</td>\n",
              "      <td>3.163769</td>\n",
              "      <td>7.055789</td>\n",
              "      <td>3.658195</td>\n",
              "      <td>3.163769</td>\n",
              "      <td>6.821964</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>300</th>\n",
              "      <td>3</td>\n",
              "      <td>0.662446</td>\n",
              "      <td>0.196818</td>\n",
              "      <td>-0.733261</td>\n",
              "      <td>-0.879606</td>\n",
              "      <td>0.371086</td>\n",
              "      <td>-0.692171</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6.172307</td>\n",
              "      <td>6.055371</td>\n",
              "      <td>1.469866</td>\n",
              "      <td>1.379697</td>\n",
              "      <td>4.585505</td>\n",
              "      <td>1.469866</td>\n",
              "      <td>6.055371</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>400</th>\n",
              "      <td>4</td>\n",
              "      <td>0.856992</td>\n",
              "      <td>1.795874</td>\n",
              "      <td>0.011465</td>\n",
              "      <td>-0.879606</td>\n",
              "      <td>0.558638</td>\n",
              "      <td>0.301522</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>7.834469</td>\n",
              "      <td>6.452848</td>\n",
              "      <td>2.187257</td>\n",
              "      <td>2.747986</td>\n",
              "      <td>4.265591</td>\n",
              "      <td>2.187257</td>\n",
              "      <td>6.452848</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     sample_id       x_0       x_1  ...       ite       y_0       y_1\n",
              "0            0  1.397395  0.996346  ...  4.657928  1.164950  5.822879\n",
              "100          1  0.269033  0.196818  ...  3.428604  3.492099  6.920703\n",
              "200          2  1.051537  1.795874  ...  3.658195  3.163769  6.821964\n",
              "300          3  0.662446  0.196818  ...  4.585505  1.469866  6.055371\n",
              "400          4  0.856992  1.795874  ...  4.265591  2.187257  6.452848\n",
              "\n",
              "[5 rows x 34 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQWUbE6HLOfw"
      },
      "source": [
        "## 1.1 S-Learner Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rT9UjEovLOf0"
      },
      "source": [
        "# Importing the relevant SLearner module\n",
        "\n",
        "from justcause.learners import SLearner\n",
        "\n",
        "\n",
        "# Defining the S-Learner function that returns the ITE\n",
        "# We define a function that takes the data, splits it up and returns individual treatment effect accuracies for the train and the test data-set\n",
        "# The function takes each, the train and test data separately and selects the relevant variales and coverts them into np arrays\n",
        "# The relevant variables have the followings names: x (the covariates), t (the treatment), y (the outcome)\n",
        "# Note that the treatment needs to be explicityl defined\n",
        "\n",
        "\n",
        "\n",
        "def basic_slearner(train, test, model):\n",
        "    \"\"\" \"\"\"\n",
        "    train_X, train_t, train_y = train.np.X, train.np.t, train.np.y\n",
        "    test_X, test_t, test_y = test.np.X, test.np.t, test.np.y\n",
        "    \n",
        "    slearner = model     # Select linear regression as a method to find the ITE for the S-Learner\n",
        "    slearner.fit(train_X, train_t, train_y)      # Fitting the s-learner with linear regression\n",
        "    return (\n",
        "        slearner.predict_ite(train_X, train_t, train_y),   # Returning the predicting values for ITE for train\n",
        "        slearner.predict_ite(test_X, test_t, test_y)       # Returning the prediction values for ITE for test\n",
        "    )\n",
        "\n"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxwuoQxwLOgD"
      },
      "source": [
        "results_df = list()     # We define the list that contains the results\n",
        "test_scores = list()    # Storing the test scores in a list\n",
        "train_scores = list()   # Storing the train scores in a list\n",
        "\n",
        "\n",
        "# Here we define the model that is going to be used for the S-learner\n",
        "# Please instantiate linear regression for the simple learner\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "\n",
        "##-----------------Question------------------###\n",
        "# Pass a LinearRegression Model into the S-Learner\n",
        "# No particular parameter-settings necessary\n",
        "\n",
        "\n",
        "model=SLearner(LinearRegression())\n",
        "\n",
        "\n",
        "for rep in replications:\n",
        "\n",
        "    train, test = train_test_split(\n",
        "        rep, train_size=train_size, random_state=random_state\n",
        "    )\n",
        "\n",
        "    # REPLACE this with the function you implemented and want to evaluate\n",
        "    train_ite, test_ite = basic_slearner(train, test, model)\n",
        "\n",
        "    # Calculate the scores and append them to a dataframe\n",
        "    train_scores.append(calc_scores(train[Col.ite], train_ite, metrics))\n",
        "    test_scores.append(calc_scores(test[Col.ite], test_ite, metrics))\n",
        "\n",
        "# Summarize the scores and save them in a dataframe\n",
        "train_result, test_result = summarize_scores(train_scores), summarize_scores(test_scores)\n",
        "train_result.update({'method': 'S-Learner LR', 'train': True})\n",
        "test_result.update({'method': 'S-Learner LR', 'train': False})"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Gb37xoDuLOgQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "95807711-109f-414b-8eec-8a67806e8d01"
      },
      "source": [
        "df_S_learner_LR=pd.DataFrame([train_result, test_result])\n",
        "df_S_learner_LR"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pehe_score-mean</th>\n",
              "      <th>pehe_score-median</th>\n",
              "      <th>pehe_score-std</th>\n",
              "      <th>mean_absolute-mean</th>\n",
              "      <th>mean_absolute-median</th>\n",
              "      <th>mean_absolute-std</th>\n",
              "      <th>method</th>\n",
              "      <th>train</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.633660</td>\n",
              "      <td>2.623297</td>\n",
              "      <td>8.362125</td>\n",
              "      <td>0.732443</td>\n",
              "      <td>0.238185</td>\n",
              "      <td>1.493276</td>\n",
              "      <td>S-Learner LR</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5.625971</td>\n",
              "      <td>2.635993</td>\n",
              "      <td>8.213626</td>\n",
              "      <td>1.292668</td>\n",
              "      <td>0.396246</td>\n",
              "      <td>2.474603</td>\n",
              "      <td>S-Learner LR</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   pehe_score-mean  pehe_score-median  ...        method  train\n",
              "0         5.633660           2.623297  ...  S-Learner LR   True\n",
              "1         5.625971           2.635993  ...  S-Learner LR  False\n",
              "\n",
              "[2 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfhFVRd4LOgk"
      },
      "source": [
        "### 1.1.1 S-Learner Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbbouLxaLOgo"
      },
      "source": [
        "# We run the same analysis again but only on an indvidual run of the data\n",
        "# The reason is that the data generating process is varied every time,... \n",
        "# ...so we can only look at individual runs of the ITE effect\n",
        "results_df = list()    # We define the list that contains the results\n",
        "test_scores = list()   # Storing the test scores in a list\n",
        "train_scores = list()  # Storing the train scores in a list\n",
        "\n",
        "\n",
        "#for rep in replications:\n",
        "\n",
        "train, test = train_test_split(\n",
        "        replications[n], train_size=train_size, random_state=random_state     # Use train_test_split  to split the data-set (replications[n]) \n",
        "    )\n",
        "\n",
        "# REPLACE this with the function you implemented and want to evaluate\n",
        "train_ite, test_ite = basic_slearner(train, test, model)         # using the pre-defined basic learner function to retunr train, test\n",
        "\n",
        "# Calculate the scores and append them to a dataframe\n",
        "train_scores.append(calc_scores(train[Col.ite], train_ite, metrics))    # Using the just cause API to calcualte the scores from the estimate ITE for the training data\n",
        "test_scores.append(calc_scores(test[Col.ite], test_ite, metrics))     # Using the just cause API to calcualte the scores from the estimate ITE for the test data\n",
        "\n",
        "# Summarize the scores and save them in a dataframe\n",
        "train_result, test_result = summarize_scores(train_scores), summarize_scores(test_scores)   #summary of the scores \n",
        "train_result.update({'method': 'S-Learner LR', 'train': True})\n",
        "test_result.update({'method': 'S-Learner LR', 'train': False})\n"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsxWY_9mLOg2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "outputId": "f3dea85a-cba9-4286-d407-7f74066c3ce9"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "colors = (0,0,0)\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.scatter(test.np.ite, test_ite,  c=colors, alpha=0.5)\n",
        "plt.title('Scatter of treatment effects')\n",
        "plt.xlabel('real treatment effect')\n",
        "plt.ylabel('estimated treatment effect')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# If the treatment effect is perfectly represented there should be a 45 degree line!"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSiTTv7KLOhI"
      },
      "source": [
        "## QUESTION 1\n",
        "\n",
        "IS THE S-LEARNER WITH LINEAR REGRESSION ESTIMATING INDIVIDUAL TREATMENT EFFECTS WELL? IF YES/NO -WHY SO?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wg4aUO_QPcqe"
      },
      "source": [
        "# ANSWER 1\r\n",
        "\r\n",
        "S-Learner with seems to be a poor estimator of ITE as it always return the same ITE no matter what the real treatment effect is. Model is predicting Average Treatement Effect but we want Indivisaul Treatment effects. \r\n",
        "\r\n",
        "Overall: Bad Estimation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJe9Cnj6LOhL"
      },
      "source": [
        "## 1.2 Propensity Score Weighing with Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7TpRNPdLOhP"
      },
      "source": [
        "# Importing the relevant PSWEstimator\n",
        "\n",
        "from justcause.learners import PSWEstimator\n",
        "\n",
        "\n",
        "#Defining the Propoensity Score weighing function that returns the ITE\n",
        "\n",
        "def propensity_score_weighing(train, test, model):\n",
        "    \"\"\" \"\"\"\n",
        "    train_X, train_t, train_y = train.np.X, train.np.t, train.np.y\n",
        "    test_X, test_t, test_y = test.np.X, test.np.t, test.np.y\n",
        "\n",
        "    pswestimator = model\n",
        "\n",
        "    return (\n",
        "        pswestimator.estimate_ate(train_X, train_t, train_y),\n",
        "        pswestimator.estimate_ate(test_X, test_t, test_y)\n",
        "    )\n"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNoIPNfgLOhb"
      },
      "source": [
        "results_df = list()\n",
        "test_scores = list()\n",
        "train_scores = list()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = PSWEstimator(propensity_learner=None, delta=0.001)\n",
        "\n",
        "\n",
        "for rep in replications:\n",
        "\n",
        "    train, test = train_test_split(\n",
        "        rep, train_size=train_size, random_state=random_state\n",
        "    )\n",
        "\n",
        "    # REPLACE this with the function you implemented and want to evaluate\n",
        "    train_ite, test_ite = propensity_score_weighing(train, test, model)\n",
        "\n",
        "    # Calculate the scores and append them to a dataframe\n",
        "    train_scores.append(calc_scores(train[Col.ite], train_ite, metrics))\n",
        "    test_scores.append(calc_scores(test[Col.ite], test_ite, metrics))\n",
        "\n",
        "# Summarize the scores and save them in a dataframe\n",
        "train_result, test_result = summarize_scores(train_scores), summarize_scores(test_scores)\n",
        "train_result.update({'method': 'PSW', 'train': True})\n",
        "test_result.update({'method': 'PSW', 'train': False})"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caopoaQrLOhm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "a25c6bf8-dc6e-4dc0-f133-055bad6e7059"
      },
      "source": [
        "df_PSW_LR=pd.DataFrame([train_result, test_result])\n",
        "df_PSW_LR"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pehe_score-mean</th>\n",
              "      <th>pehe_score-median</th>\n",
              "      <th>pehe_score-std</th>\n",
              "      <th>mean_absolute-mean</th>\n",
              "      <th>mean_absolute-median</th>\n",
              "      <th>mean_absolute-std</th>\n",
              "      <th>method</th>\n",
              "      <th>train</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.705081</td>\n",
              "      <td>2.699761</td>\n",
              "      <td>8.205818</td>\n",
              "      <td>1.234220</td>\n",
              "      <td>0.819094</td>\n",
              "      <td>1.140432</td>\n",
              "      <td>PSW</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5.695339</td>\n",
              "      <td>2.483461</td>\n",
              "      <td>8.217443</td>\n",
              "      <td>0.519767</td>\n",
              "      <td>0.482485</td>\n",
              "      <td>0.348374</td>\n",
              "      <td>PSW</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   pehe_score-mean  pehe_score-median  ...  method  train\n",
              "0         5.705081           2.699761  ...     PSW   True\n",
              "1         5.695339           2.483461  ...     PSW  False\n",
              "\n",
              "[2 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAcuhZhKLOhy"
      },
      "source": [
        "## 1.3 S-Learner Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtiDtGbyLOh3"
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "\n",
        "# Importing the relevant S-Learner estimator\n",
        "from justcause.learners import SLearner\n",
        "\n",
        "\n",
        "#Defining the S-Learner function that returns the ITE\n",
        "\n",
        "def basic_slearner(train, test, model):\n",
        "    \"\"\" \"\"\"\n",
        "    train_X, train_t, train_y = train.np.X, train.np.t, train.np.y\n",
        "    test_X, test_t, test_y = test.np.X, test.np.t, test.np.y\n",
        "\n",
        "    slearner = model  \n",
        "    slearner.fit(train_X, train_t, train_y)\n",
        "    return (\n",
        "        slearner.predict_ite(train_X, train_t, train_y),\n",
        "        slearner.predict_ite(test_X, test_t, test_y)\n",
        "    )\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XW0XLJ0ELOiG"
      },
      "source": [
        "results_df = list()\n",
        "test_scores = list()\n",
        "train_scores = list()\n",
        "\n",
        "#---------------------------Question--------------------------------#\n",
        "# Pass a RandomForestRegressor into the S-learner\n",
        "\n",
        "\n",
        "model = SLearner(RandomForestRegressor())\n",
        "\n",
        "for rep in replications:\n",
        "\n",
        "    train, test = train_test_split(\n",
        "        rep, train_size=train_size, random_state=random_state\n",
        "    )\n",
        "\n",
        "    # REPLACE this with the function you implemented and want to evaluate\n",
        "    train_ite, test_ite = basic_slearner(train, test, model )\n",
        "\n",
        "    # Calculate the scores and append them to a dataframe\n",
        "    train_scores.append(calc_scores(train[Col.ite], train_ite, metrics))\n",
        "    test_scores.append(calc_scores(test[Col.ite], test_ite, metrics))\n",
        "\n",
        "# Summarize the scores and save them in a dataframe\n",
        "train_result, test_result = summarize_scores(train_scores), summarize_scores(test_scores)\n",
        "train_result.update({'method': 'S-Learner RF', 'train': True})\n",
        "test_result.update({'method': 'S-Learner RF', 'train': False})"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "QgIkGA9MLOiR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "6cc649d2-b4c2-4c19-ce93-f9ea07ceec2e"
      },
      "source": [
        "df_S_learner_RF=pd.DataFrame([train_result, test_result])\n",
        "df_S_learner_RF"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pehe_score-mean</th>\n",
              "      <th>pehe_score-median</th>\n",
              "      <th>pehe_score-std</th>\n",
              "      <th>mean_absolute-mean</th>\n",
              "      <th>mean_absolute-median</th>\n",
              "      <th>mean_absolute-std</th>\n",
              "      <th>method</th>\n",
              "      <th>train</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3.107685</td>\n",
              "      <td>1.053549</td>\n",
              "      <td>4.760754</td>\n",
              "      <td>0.503079</td>\n",
              "      <td>0.132801</td>\n",
              "      <td>0.934136</td>\n",
              "      <td>S-Learner RF</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3.322921</td>\n",
              "      <td>1.238255</td>\n",
              "      <td>5.172362</td>\n",
              "      <td>0.446676</td>\n",
              "      <td>0.126003</td>\n",
              "      <td>1.036056</td>\n",
              "      <td>S-Learner RF</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   pehe_score-mean  pehe_score-median  ...        method  train\n",
              "0         3.107685           1.053549  ...  S-Learner RF   True\n",
              "1         3.322921           1.238255  ...  S-Learner RF  False\n",
              "\n",
              "[2 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyU9_8rcLOid"
      },
      "source": [
        "### 1.3.1 Random Forest Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQCaEOvQhFJP"
      },
      "source": [
        "There seemed to be a problem with cell below where the professor again defined the basic_slearner function here. So i changed the code where now i am calling the basic s-learner function here.   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSyGTy4mgA9Y"
      },
      "source": [
        "results_df = list()\r\n",
        "test_scores = list()\r\n",
        "train_scores = list()\r\n",
        "\r\n",
        "train, test = train_test_split(\r\n",
        "        replications[n], train_size=train_size, random_state=random_state\r\n",
        "    )\r\n",
        "\r\n",
        "# REPLACE this with the function you implemented and want to evaluate\r\n",
        "train_ite, test_ite = basic_slearner(train, test, model)\r\n",
        "\r\n",
        "# Calculate the scores and append them to a dataframe\r\n",
        "train_scores.append(calc_scores(train[Col.ite], train_ite, metrics))\r\n",
        "test_scores.append(calc_scores(test[Col.ite], test_ite, metrics))\r\n",
        "\r\n",
        "# Summarize the scores and save them in a dataframe\r\n",
        "train_result, test_result = summarize_scores(train_scores), summarize_scores(test_scores)\r\n",
        "train_result.update({'method': 'T-Learner LR', 'train': True})\r\n",
        "test_result.update({'method': 'T-Learner LR', 'train': False})"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "eF2mk_AxLOin",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "outputId": "0b80feaf-f28d-4762-bb21-094dfdb2872f"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "colors = (0,0,0)\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.scatter(test.np.ite, test_ite,  c=colors, alpha=0.5)\n",
        "plt.title('Scatter of treatment effects')\n",
        "plt.xlabel('real treatment effect')\n",
        "plt.ylabel('estimated treatment effect')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0t4K4CfLOit"
      },
      "source": [
        "## QUESTION 2\n",
        "\n",
        "IS THE S-LEARNER WITH RANDOM FOREST ESTIMATING INDIVIDUAL TREATMENT EFFECTS WELL? IF YES/NO -WHY SO?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6votZSUXmqk"
      },
      "source": [
        "# ANSWER 2\r\n",
        "1. S-learner with random forest regressor is producing better results than with Linear Regression. But still the resuts are not accurate. \r\n",
        "\r\n",
        "2. For 3 indivisuals i can see that the real treatment effect was positive but the model outputs negative treatment effect which can be disasterous.  \r\n",
        "\r\n",
        "3. For several indivisuals when Real Treatment Effect is low, the model predicts high and vice versa.    \r\n",
        "\r\n",
        "Overall: Better Estimation than Linear Regression with single learner. \r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vowzj2teLOiw"
      },
      "source": [
        "## 1.4 T-Learner Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "811XNGgLLOix"
      },
      "source": [
        "# Importing the relevant SLearner module\n",
        "\n",
        "from justcause.learners import TLearner\n",
        "\n",
        "\n",
        "#Defining the S-Learner function that returns the ITE\n",
        "\n",
        "def basic_tlearner(train, test, model):\n",
        "    \"\"\" \"\"\"\n",
        "    train_X, train_t, train_y = train.np.X, train.np.t, train.np.y\n",
        "    test_X, test_t, test_y = test.np.X, test.np.t, test.np.y\n",
        "\n",
        "    tlearner = model\n",
        "    tlearner.fit(train_X, train_t, train_y)\n",
        "    return (\n",
        "        tlearner.predict_ite(train_X, train_t, train_y),\n",
        "        tlearner.predict_ite(test_X, test_t, test_y)\n",
        "    )\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_GCgPWtLOi2"
      },
      "source": [
        "results_df = list()\n",
        "test_scores = list()\n",
        "train_scores = list()\n",
        "\n",
        "#------------------Question------------------------------#\n",
        "# Pass linear regression into the T-Learner\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "model = TLearner(LinearRegression())\n",
        "\n",
        "for rep in replications:\n",
        "\n",
        "    train, test = train_test_split(\n",
        "        rep, train_size=train_size, random_state=random_state\n",
        "    )\n",
        "\n",
        "    # REPLACE this with the function you implemented and want to evaluate\n",
        "    train_ite, test_ite = basic_tlearner(train, test, model)\n",
        "\n",
        "    # Calculate the scores and append them to a dataframe\n",
        "    train_scores.append(calc_scores(train[Col.ite], train_ite, metrics))\n",
        "    test_scores.append(calc_scores(test[Col.ite], test_ite, metrics))\n",
        "\n",
        "# Summarize the scores and save them in a dataframe\n",
        "train_result, test_result = summarize_scores(train_scores), summarize_scores(test_scores)\n",
        "train_result.update({'method': 'T-Learner LR', 'train': True})\n",
        "test_result.update({'method': 'T-Learner LR', 'train': False})"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "tMeJsu4zLOi6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "e9ee61c1-64ef-4eda-dd0a-4769a9086432"
      },
      "source": [
        "df_T_learner_LR=pd.DataFrame([train_result, test_result])\n",
        "df_T_learner_LR"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pehe_score-mean</th>\n",
              "      <th>pehe_score-median</th>\n",
              "      <th>pehe_score-std</th>\n",
              "      <th>mean_absolute-mean</th>\n",
              "      <th>mean_absolute-median</th>\n",
              "      <th>mean_absolute-std</th>\n",
              "      <th>method</th>\n",
              "      <th>train</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.276868</td>\n",
              "      <td>1.055409</td>\n",
              "      <td>3.319402</td>\n",
              "      <td>0.149960</td>\n",
              "      <td>0.133955</td>\n",
              "      <td>0.129121</td>\n",
              "      <td>T-Learner LR</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.337753</td>\n",
              "      <td>1.122464</td>\n",
              "      <td>3.263915</td>\n",
              "      <td>0.263287</td>\n",
              "      <td>0.195850</td>\n",
              "      <td>0.327846</td>\n",
              "      <td>T-Learner LR</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   pehe_score-mean  pehe_score-median  ...        method  train\n",
              "0         2.276868           1.055409  ...  T-Learner LR   True\n",
              "1         2.337753           1.122464  ...  T-Learner LR  False\n",
              "\n",
              "[2 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-k49nGyQLOi9"
      },
      "source": [
        "### 1.4.1 T-Learner Linear Regression Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9u2DMEWTLOi-"
      },
      "source": [
        "results_df = list()\n",
        "test_scores = list()\n",
        "train_scores = list()\n",
        "\n",
        "train, test = train_test_split(\n",
        "        replications[n], train_size=train_size, random_state=random_state\n",
        "    )\n",
        "\n",
        "# REPLACE this with the function you implemented and want to evaluate\n",
        "train_ite, test_ite = basic_tlearner(train, test, model)\n",
        "\n",
        "# Calculate the scores and append them to a dataframe\n",
        "train_scores.append(calc_scores(train[Col.ite], train_ite, metrics))\n",
        "test_scores.append(calc_scores(test[Col.ite], test_ite, metrics))\n",
        "\n",
        "# Summarize the scores and save them in a dataframe\n",
        "train_result, test_result = summarize_scores(train_scores), summarize_scores(test_scores)\n",
        "train_result.update({'method': 'T-Learner LR', 'train': True})\n",
        "test_result.update({'method': 'T-Learner LR', 'train': False})"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "VtFgO5PZLOjC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "outputId": "26e1ae28-81c0-42f6-8219-3d775ce20918"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "colors = (0,0,0)\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.scatter(test.np.ite, test_ite,  c=colors, alpha=0.5)\n",
        "plt.title('Scatter of treatment effects')\n",
        "plt.xlabel('real treatment effect')\n",
        "plt.ylabel('estimated treatment effect')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61rEzJoLLOjF"
      },
      "source": [
        "## QUESTION 3\n",
        "\n",
        "IS THE T-LEARNER WITH LINEAR REGRESSION ESTIMATING INDIVIDUAL TREATMENT EFFECTS WELL? IF YES/NO -WHY SO?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Iy1NXa2lsB2"
      },
      "source": [
        "# ANSWER 3\r\n",
        "\r\n",
        "For several indivisuals when Real Treatment Effect is low, the model predicts high and vice versa.  \r\n",
        "\r\n",
        "Overall: Better estimation than S-learner with Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RO36FBY1LOjG"
      },
      "source": [
        "## 1.5 T-Learner Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjA-HNpCLOjH"
      },
      "source": [
        "# Importing the relevant SLearner module\n",
        "\n",
        "from justcause.learners import TLearner\n",
        "\n",
        "\n",
        "#Defining the S-Learner function that returns the ITE\n",
        "\n",
        "def basic_tlearner(train, test, model):\n",
        "    \"\"\" \"\"\"\n",
        "    train_X, train_t, train_y = train.np.X, train.np.t, train.np.y\n",
        "    test_X, test_t, test_y = test.np.X, test.np.t, test.np.y\n",
        "\n",
        "    tlearner = model\n",
        "    tlearner.fit(train_X, train_t, train_y)\n",
        "    return (\n",
        "        tlearner.predict_ite(train_X, train_t, train_y),\n",
        "        tlearner.predict_ite(test_X, test_t, test_y)\n",
        "    )\n",
        "\n"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMXoNfexLOjL"
      },
      "source": [
        "results_df = list()\n",
        "test_scores = list()\n",
        "train_scores = list()\n",
        "\n",
        "#--------------------Question----------------------------------#\n",
        "# Pass a Random Forest into the T-Learner\n",
        "\n",
        "model = TLearner(RandomForestRegressor())\n",
        "\n",
        "for rep in replications:\n",
        "\n",
        "    train, test = train_test_split(\n",
        "        rep, train_size=train_size, random_state=random_state\n",
        "    )\n",
        "\n",
        "    # REPLACE this with the function you implemented and want to evaluate\n",
        "    train_ite, test_ite = basic_tlearner(train, test, model)\n",
        "\n",
        "    # Calculate the scores and append them to a dataframe\n",
        "    train_scores.append(calc_scores(train[Col.ite], train_ite, metrics))\n",
        "    test_scores.append(calc_scores(test[Col.ite], test_ite, metrics))\n",
        "\n",
        "# Summarize the scores and save them in a dataframe\n",
        "train_result, test_result = summarize_scores(train_scores), summarize_scores(test_scores)\n",
        "train_result.update({'method': 'T-Learner RF', 'train': True})\n",
        "test_result.update({'method': 'T-Learner RF', 'train': False})"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "psT80zSaLOjP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "ff5c9d8e-4040-4464-c4b6-f5d2f98e1846"
      },
      "source": [
        "df_T_learner_RF=pd.DataFrame([train_result, test_result])\n",
        "df_T_learner_RF"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pehe_score-mean</th>\n",
              "      <th>pehe_score-median</th>\n",
              "      <th>pehe_score-std</th>\n",
              "      <th>mean_absolute-mean</th>\n",
              "      <th>mean_absolute-median</th>\n",
              "      <th>mean_absolute-std</th>\n",
              "      <th>method</th>\n",
              "      <th>train</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.775321</td>\n",
              "      <td>0.955023</td>\n",
              "      <td>2.232431</td>\n",
              "      <td>0.125264</td>\n",
              "      <td>0.098101</td>\n",
              "      <td>0.125193</td>\n",
              "      <td>T-Learner RF</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.508553</td>\n",
              "      <td>1.118640</td>\n",
              "      <td>3.512167</td>\n",
              "      <td>0.211167</td>\n",
              "      <td>0.123882</td>\n",
              "      <td>0.288221</td>\n",
              "      <td>T-Learner RF</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   pehe_score-mean  pehe_score-median  ...        method  train\n",
              "0         1.775321           0.955023  ...  T-Learner RF   True\n",
              "1         2.508553           1.118640  ...  T-Learner RF  False\n",
              "\n",
              "[2 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSU2Kyj9LOjT"
      },
      "source": [
        "### 1.5.1 T-Learner with Random Forrest Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKNP1x2KLOjU"
      },
      "source": [
        "results_df = list()\n",
        "test_scores = list()\n",
        "train_scores = list()\n",
        "\n",
        "\n",
        "\n",
        "train, test = train_test_split(\n",
        "        replications[n], train_size=train_size, random_state=random_state\n",
        "    )\n",
        "\n",
        "# REPLACE this with the function you implemented and want to evaluate\n",
        "train_ite, test_ite = basic_tlearner(train, test, model)\n",
        "\n",
        "# Calculate the scores and append them to a dataframe\n",
        "train_scores.append(calc_scores(train[Col.ite], train_ite, metrics))\n",
        "test_scores.append(calc_scores(test[Col.ite], test_ite, metrics))\n",
        "\n",
        "# Summarize the scores and save them in a dataframe\n",
        "train_result, test_result = summarize_scores(train_scores), summarize_scores(test_scores)\n",
        "train_result.update({'method': 'T-Learner RF', 'train': True})\n",
        "test_result.update({'method': 'T-Learner RF', 'train': False})"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ha8GizLtLOjY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "outputId": "75bebbff-3db9-408b-dcc2-366bcd5ccd4e"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "colors = (0,0,0)\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.scatter(test.np.ite, test_ite,  c=colors, alpha=0.5)\n",
        "plt.title('Scatter of treatment effects')\n",
        "plt.xlabel('real treatment effect')\n",
        "plt.ylabel('estimated treatment effect')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6x_BryEaLOjd"
      },
      "source": [
        "## QUESTION 4\n",
        "\n",
        "IS THE T-LEARNER WITH RANDOM FOREST ESTIMATING INDIVIDUAL TREATMENT EFFECTS WELL? IF YES/NO -WHY SO?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUzv7em_mwCO"
      },
      "source": [
        "# ANSWER 3\r\n",
        "\r\n",
        "1. For 3 indivisuals i can see that the real treatment effect was positive but the model outputs negative treatment effect which can be disasterous.  \r\n",
        "\r\n",
        "2. For several indivisuals when Real Treatment Effect is low, the model predicts high and vice versa.  \r\n",
        "\r\n",
        "Overall: Better Estimation than Linear Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jh0sdhdKLOjf"
      },
      "source": [
        "## 1.6 Causal Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GWzTUTlLOjg"
      },
      "source": [
        "# Importing the relevant SLearner module\n",
        "\n",
        "from justcause.learners import CausalForest\n",
        "\n",
        "\n",
        "#Defining the causal_forest function that returns the ITE\n",
        "\n",
        "def causal_forest(train, test, model):\n",
        "    \"\"\" \"\"\"\n",
        "    train_X, train_t, train_y = train.np.X, train.np.t, train.np.y\n",
        "    test_X, test_t, test_y = test.np.X, test.np.t, test.np.y\n",
        "\n",
        "    causalforest = model\n",
        "    causalforest.fit(train_X, train_t, train_y)\n",
        "    return (\n",
        "        causalforest.predict_ite(train_X, train_t, train_y),\n",
        "        causalforest.predict_ite(test_X, test_t, test_y)\n",
        "    )\n"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZY-qvwfHLOjk"
      },
      "source": [
        "results_df = list()\n",
        "test_scores = list()\n",
        "train_scores = list()\n",
        "\n",
        "model = CausalForest(random_state=random_state)\n",
        "\n",
        "for rep in replications:\n",
        "\n",
        "    train, test = train_test_split(\n",
        "        rep, train_size=train_size, random_state=random_state\n",
        "    )\n",
        "\n",
        "    # REPLACE this with the function you implemented and want to evaluate\n",
        "    train_ite, test_ite = causal_forest(train, test, model)\n",
        "\n",
        "    # Calculate the scores and append them to a dataframe\n",
        "    train_scores.append(calc_scores(train[Col.ite], train_ite, metrics))\n",
        "    test_scores.append(calc_scores(test[Col.ite], test_ite, metrics))\n",
        "\n",
        "# Summarize the scores and save them in a dataframe\n",
        "train_result, test_result = summarize_scores(train_scores), summarize_scores(test_scores)\n",
        "train_result.update({'method': 'causal forest', 'train': True})\n",
        "test_result.update({'method': 'causal forest', 'train': False})"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B78davrGLOjn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "75eae730-a5ae-4bf8-d314-3e497d28cd62"
      },
      "source": [
        "df_causal_forest=pd.DataFrame([train_result, test_result])\n",
        "df_causal_forest"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pehe_score-mean</th>\n",
              "      <th>pehe_score-median</th>\n",
              "      <th>pehe_score-std</th>\n",
              "      <th>mean_absolute-mean</th>\n",
              "      <th>mean_absolute-median</th>\n",
              "      <th>mean_absolute-std</th>\n",
              "      <th>method</th>\n",
              "      <th>train</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4.183684</td>\n",
              "      <td>1.867887</td>\n",
              "      <td>6.357736</td>\n",
              "      <td>0.400383</td>\n",
              "      <td>0.167071</td>\n",
              "      <td>0.710400</td>\n",
              "      <td>causal forest</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.197395</td>\n",
              "      <td>1.861331</td>\n",
              "      <td>6.171123</td>\n",
              "      <td>0.773380</td>\n",
              "      <td>0.296180</td>\n",
              "      <td>1.380762</td>\n",
              "      <td>causal forest</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   pehe_score-mean  pehe_score-median  ...         method  train\n",
              "0         4.183684           1.867887  ...  causal forest   True\n",
              "1         4.197395           1.861331  ...  causal forest  False\n",
              "\n",
              "[2 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfQBHXUTLOjt"
      },
      "source": [
        "### 1.6.1 Causal Forest Visualization "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4c3Y-XDLOju"
      },
      "source": [
        "random_state = 1\n",
        "\n",
        "results_df = list()\n",
        "test_scores = list()\n",
        "train_scores = list()\n",
        "\n",
        "\n",
        "\n",
        "train, test = train_test_split(\n",
        "        replications[n], train_size=train_size, random_state=random_state\n",
        "    )\n",
        "\n",
        "# REPLACE this with the function you implemented and want to evaluate\n",
        "train_ite, test_ite = causal_forest(train, test, model)\n",
        "\n",
        "# Calculate the scores and append them to a dataframe\n",
        "train_scores.append(calc_scores(train[Col.ite], train_ite, metrics))\n",
        "test_scores.append(calc_scores(test[Col.ite], test_ite, metrics))\n",
        "\n",
        "# Summarize the scores and save them in a dataframe\n",
        "train_result, test_result = summarize_scores(train_scores), summarize_scores(test_scores)\n",
        "train_result.update({'method': 'causal forest', 'train': True})\n",
        "test_result.update({'method': 'causal forest', 'train': False})"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3mIANgDLOjw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "outputId": "3910bb2f-f192-4f60-f641-b1e7d061398a"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "colors = (0,0,0)\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.scatter(test.np.ite, test_ite,  c=colors, alpha=0.5)\n",
        "plt.title('Scatter of treatment effects')\n",
        "plt.xlabel('real treatment effect')\n",
        "plt.ylabel('estimated treatment effect')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ingDVHQLOj0"
      },
      "source": [
        "## QUESTION 5\n",
        "\n",
        "IS THE CAUSAL FOREST WITH LINEAR REGRESSION ESTIMATING INDIVIDUAL TREATMENT EFFECTS WELL? IF YES/NO -WHY SO?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2ZK5EFkpnF4"
      },
      "source": [
        "# ANSWER 5\r\n",
        "\r\n",
        "1. Model is always predicting positive treatment effect which is not good. We have 2 indivisuals with negative real treatment effect.\r\n",
        "\r\n",
        "2. I can see huge deviations in estimated treatment effect from the true treatments effects.\r\n",
        "\r\n",
        "Overall: Bad Estimation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnFLlHMKLOj1"
      },
      "source": [
        "## 1.7 Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXE4Usi1LOj2"
      },
      "source": [
        "# Importing the relevant SLearner module\n",
        "\n",
        "from justcause.learners import DragonNet\n",
        "\n",
        "\n",
        "#Defining the S-Learner function that returns the ITE\n",
        "\n",
        "def causal_forest(train, test, model):\n",
        "    \"\"\" \"\"\"\n",
        "    train_X, train_t, train_y = train.np.X, train.np.t, train.np.y\n",
        "    test_X, test_t, test_y = test.np.X, test.np.t, test.np.y\n",
        "\n",
        "    dragonnet = model\n",
        "    dragonnet.fit(train_X, train_t, train_y)\n",
        "    return (\n",
        "        dragonnet.predict_ite(train_X, train_t, train_y),\n",
        "        dragonnet.predict_ite(test_X, test_t, test_y)\n",
        "    )\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeJ5_9NBLOj5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37927ecb-0051-426d-c147-824c52c6909d"
      },
      "source": [
        "results_df = list()\n",
        "test_scores = list()\n",
        "train_scores = list()\n",
        "\n",
        "\n",
        "#---------------------------Question----------------------------#\n",
        "# Set the model to the DragonNet neural network from JustCause\n",
        "\n",
        "\n",
        "model = DragonNet(learning_rate=0.001, \n",
        "                  num_epochs=50,\n",
        "                  batch_size=512,\n",
        "                  validation_split=0.1)\n",
        "\n",
        "\n",
        "\n",
        "for rep in replications:\n",
        "\n",
        "    train, test = train_test_split(\n",
        "        rep, train_size=train_size, random_state=random_state\n",
        "    )\n",
        "\n",
        "    # REPLACE this with the function you implemented and want to evaluate\n",
        "    train_ite, test_ite = causal_forest(train, test, model)\n",
        "\n",
        "    # Calculate the scores and append them to a dataframe\n",
        "    train_scores.append(calc_scores(train[Col.ite], train_ite, metrics))\n",
        "    test_scores.append(calc_scores(test[Col.ite], test_ite, metrics))\n",
        "\n",
        "# Summarize the scores and save them in a dataframe\n",
        "train_result, test_result = summarize_scores(train_scores), summarize_scores(test_scores)\n",
        "train_result.update({'method': 'Dragonnet', 'train': True})\n",
        "test_result.update({'method': 'Dragonnet', 'train': False})"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 148391.2500 - regression_loss: 39880.6016 - val_loss: 13966.4707 - val_regression_loss: 6948.1104\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 119983.9922 - regression_loss: 32907.1055 - val_loss: 10250.8330 - val_regression_loss: 5096.9053\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 89767.6562 - regression_loss: 24101.8457 - val_loss: 6216.3589 - val_regression_loss: 3086.6904\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 53236.3203 - regression_loss: 14363.3584 - val_loss: 3148.3289 - val_regression_loss: 1559.6901\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 24987.6055 - regression_loss: 6670.6494 - val_loss: 2967.3386 - val_regression_loss: 1474.9260\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 19108.4746 - regression_loss: 5328.2710 - val_loss: 4169.2510 - val_regression_loss: 2076.3445\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 27861.4004 - regression_loss: 7527.0981 - val_loss: 3409.2031 - val_regression_loss: 1692.3655\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 22677.6172 - regression_loss: 6108.0210 - val_loss: 2046.2467 - val_regression_loss: 1006.8558\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 13475.4629 - regression_loss: 3617.8193 - val_loss: 1327.4515 - val_regression_loss: 645.0078\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 9441.6387 - regression_loss: 2513.4453 - val_loss: 1319.6919 - val_regression_loss: 639.8721\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 10309.2598 - regression_loss: 2739.7754 - val_loss: 1539.9160 - val_regression_loss: 749.6227\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 12529.1006 - regression_loss: 3311.9172 - val_loss: 1619.6035 - val_regression_loss: 789.8193\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 13071.1279 - regression_loss: 3460.1558 - val_loss: 1477.4547 - val_regression_loss: 719.6321\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 11303.4121 - regression_loss: 3044.8940 - val_loss: 1221.1949 - val_regression_loss: 592.6756\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 9113.0137 - regression_loss: 2376.2488 - val_loss: 1020.0554 - val_regression_loss: 493.3215\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 6759.9009 - regression_loss: 1788.3450 - val_loss: 994.2473 - val_regression_loss: 481.4336\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 6271.7495 - regression_loss: 1654.9912 - val_loss: 1101.9032 - val_regression_loss: 535.9557\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 6878.2432 - regression_loss: 1836.9458 - val_loss: 1172.2495 - val_regression_loss: 571.4308\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 7521.0415 - regression_loss: 1968.0875 - val_loss: 1092.8406 - val_regression_loss: 531.6415\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 6825.8350 - regression_loss: 1804.4023 - val_loss: 946.2956 - val_regression_loss: 457.9811\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 5861.8843 - regression_loss: 1531.1461 - val_loss: 843.7797 - val_regression_loss: 406.1016\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 5112.1611 - regression_loss: 1355.1500 - val_loss: 814.3103 - val_regression_loss: 390.6942\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 5177.7451 - regression_loss: 1336.9520 - val_loss: 808.3467 - val_regression_loss: 387.1609\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 5304.0234 - regression_loss: 1361.0475 - val_loss: 780.3557 - val_regression_loss: 372.8282\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 5139.3086 - regression_loss: 1316.7587 - val_loss: 732.6952 - val_regression_loss: 348.8994\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 4737.4609 - regression_loss: 1210.9199 - val_loss: 696.0742 - val_regression_loss: 330.6913\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 4351.7788 - regression_loss: 1116.2162 - val_loss: 686.8763 - val_regression_loss: 326.3330\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 4199.1172 - regression_loss: 1072.4982 - val_loss: 687.8996 - val_regression_loss: 327.1482\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 3950.0959 - regression_loss: 1050.0258 - val_loss: 675.1241 - val_regression_loss: 321.0118\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 3906.6680 - regression_loss: 1005.5624 - val_loss: 650.8597 - val_regression_loss: 308.9938\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 3695.0386 - regression_loss: 944.6192 - val_loss: 630.7601 - val_regression_loss: 298.8974\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 3514.9597 - regression_loss: 900.0263 - val_loss: 618.2686 - val_regression_loss: 292.5047\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3365.0945 - regression_loss: 876.8944 - val_loss: 606.0828 - val_regression_loss: 286.2704\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 3325.7920 - regression_loss: 849.5157 - val_loss: 586.5991 - val_regression_loss: 276.4453\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3187.1174 - regression_loss: 803.1144 - val_loss: 568.4402 - val_regression_loss: 267.3187\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2989.3560 - regression_loss: 760.2887 - val_loss: 556.5647 - val_regression_loss: 261.3503\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2841.5845 - regression_loss: 732.2829 - val_loss: 546.1013 - val_regression_loss: 256.0695\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2876.0388 - regression_loss: 709.9534 - val_loss: 531.0116 - val_regression_loss: 248.4920\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 2686.0198 - regression_loss: 679.6315 - val_loss: 514.9382 - val_regression_loss: 240.4301\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2613.9727 - regression_loss: 649.3759 - val_loss: 503.7529 - val_regression_loss: 234.8532\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2563.7188 - regression_loss: 626.6095 - val_loss: 496.2603 - val_regression_loss: 231.1794\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2420.8535 - regression_loss: 607.1352 - val_loss: 488.8997 - val_regression_loss: 227.5992\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 2419.4280 - regression_loss: 586.3612 - val_loss: 483.3130 - val_regression_loss: 224.8991\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2314.1184 - regression_loss: 568.5936 - val_loss: 477.9510 - val_regression_loss: 222.2636\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 2247.5190 - regression_loss: 553.1185 - val_loss: 469.0625 - val_regression_loss: 217.7534\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2241.7156 - regression_loss: 537.9603 - val_loss: 458.1897 - val_regression_loss: 212.1842\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2128.1401 - regression_loss: 521.0830 - val_loss: 448.3305 - val_regression_loss: 207.1504\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2113.6736 - regression_loss: 508.2855 - val_loss: 440.0027 - val_regression_loss: 202.9609\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 2080.8413 - regression_loss: 496.9667 - val_loss: 433.0248 - val_regression_loss: 199.5295\n",
            "***************************** elapsed_time is:  6.2789318561553955\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 220ms/step - loss: 844450.6875 - regression_loss: 233488.3281 - val_loss: 95422.3750 - val_regression_loss: 47593.0273\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 787848.6250 - regression_loss: 220881.4062 - val_loss: 89344.7031 - val_regression_loss: 44564.6172\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 761768.3750 - regression_loss: 206821.1406 - val_loss: 80752.4219 - val_regression_loss: 40280.7969\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 692973.0625 - regression_loss: 186960.4375 - val_loss: 69499.6484 - val_regression_loss: 34669.8281\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 591080.3125 - regression_loss: 161187.4844 - val_loss: 56041.3477 - val_regression_loss: 27959.1953\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 486084.0938 - regression_loss: 130747.9375 - val_loss: 41347.5117 - val_regression_loss: 20632.5469\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 360492.1250 - regression_loss: 97689.8359 - val_loss: 26975.1074 - val_regression_loss: 13466.6494\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 245150.6250 - regression_loss: 65629.2266 - val_loss: 15673.9033 - val_regression_loss: 7834.3223\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 152036.9531 - regression_loss: 40766.5977 - val_loss: 11885.1689 - val_regression_loss: 5952.3496\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 115394.7031 - regression_loss: 32837.1836 - val_loss: 16424.8848 - val_regression_loss: 8219.3350\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 158767.2031 - regression_loss: 43769.9805 - val_loss: 17487.1855 - val_regression_loss: 8734.2285\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 172370.7188 - regression_loss: 47214.0547 - val_loss: 13080.3477 - val_regression_loss: 6523.5063\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 145144.6406 - regression_loss: 38751.2031 - val_loss: 9155.9746 - val_regression_loss: 4560.6074\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 114647.6484 - regression_loss: 31001.0938 - val_loss: 8186.9868 - val_regression_loss: 4075.7686\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 107795.9766 - regression_loss: 29279.9648 - val_loss: 8906.0889 - val_regression_loss: 4434.2812\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 112051.0781 - regression_loss: 30386.4922 - val_loss: 9484.1787 - val_regression_loss: 4722.8325\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 115314.8672 - regression_loss: 31273.3906 - val_loss: 9282.5225 - val_regression_loss: 4622.5889\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 110253.2734 - regression_loss: 30282.0410 - val_loss: 8509.0674 - val_regression_loss: 4237.4795\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 103763.0156 - regression_loss: 28076.1133 - val_loss: 7617.7622 - val_regression_loss: 3794.3430\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 94195.0703 - regression_loss: 25696.6113 - val_loss: 7013.4194 - val_regression_loss: 3495.2991\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 88455.5781 - regression_loss: 24052.3242 - val_loss: 6853.6450 - val_regression_loss: 3418.6252\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 85463.3828 - regression_loss: 23314.4434 - val_loss: 6926.5054 - val_regression_loss: 3457.4175\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 84602.5156 - regression_loss: 22975.4082 - val_loss: 6783.0381 - val_regression_loss: 3386.3289\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 82097.2891 - regression_loss: 22125.3477 - val_loss: 6259.0059 - val_regression_loss: 3123.0486\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 76457.6719 - regression_loss: 20428.4902 - val_loss: 5676.5044 - val_regression_loss: 2829.3030\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 67635.4844 - regression_loss: 18547.5371 - val_loss: 5378.7090 - val_regression_loss: 2677.6721\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 62996.1250 - regression_loss: 17193.8926 - val_loss: 5335.4502 - val_regression_loss: 2654.1648\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 57622.5117 - regression_loss: 16393.5273 - val_loss: 5271.5659 - val_regression_loss: 2621.5364\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 58050.7734 - regression_loss: 15617.3721 - val_loss: 5033.1748 - val_regression_loss: 2503.0615\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 53873.0586 - regression_loss: 14448.3555 - val_loss: 4721.1006 - val_regression_loss: 2348.6521\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 49236.4141 - regression_loss: 13117.4639 - val_loss: 4494.7432 - val_regression_loss: 2237.3740\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 44176.2188 - regression_loss: 11976.2129 - val_loss: 4334.4038 - val_regression_loss: 2158.3491\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 42112.7734 - regression_loss: 11150.4043 - val_loss: 4067.4067 - val_regression_loss: 2024.6122\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 37727.0547 - regression_loss: 10316.1768 - val_loss: 3633.3474 - val_regression_loss: 1805.8021\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 35510.4414 - regression_loss: 9399.3438 - val_loss: 3223.7971 - val_regression_loss: 1598.7445\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 32411.6953 - regression_loss: 8639.4385 - val_loss: 2921.7947 - val_regression_loss: 1445.8839\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 30587.4062 - regression_loss: 8087.1035 - val_loss: 2692.9326 - val_regression_loss: 1330.7328\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 28386.8359 - regression_loss: 7513.0693 - val_loss: 2521.0693 - val_regression_loss: 1245.3773\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 26224.9277 - regression_loss: 6914.9849 - val_loss: 2425.6399 - val_regression_loss: 1198.9492\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 24026.7402 - regression_loss: 6385.1460 - val_loss: 2313.4751 - val_regression_loss: 1143.7170\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 20550.1855 - regression_loss: 5918.0693 - val_loss: 2069.6816 - val_regression_loss: 1021.5509\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 18076.0645 - regression_loss: 5418.2715 - val_loss: 1819.8884 - val_regression_loss: 896.0380\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 16468.4453 - regression_loss: 4989.4634 - val_loss: 1632.9899 - val_regression_loss: 802.3195\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 15455.1758 - regression_loss: 4633.5596 - val_loss: 1535.3556 - val_regression_loss: 754.3072\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 16401.7793 - regression_loss: 4292.5830 - val_loss: 1523.3263 - val_regression_loss: 749.6552\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 15021.0498 - regression_loss: 3988.7356 - val_loss: 1498.5012 - val_regression_loss: 737.9675\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 13640.4697 - regression_loss: 3747.4299 - val_loss: 1390.7020 - val_regression_loss: 683.9104\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 13251.3594 - regression_loss: 3506.5718 - val_loss: 1285.1107 - val_regression_loss: 630.6298\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 11000.0166 - regression_loss: 3323.6199 - val_loss: 1221.0365 - val_regression_loss: 598.2471\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 11974.1631 - regression_loss: 3184.4932 - val_loss: 1259.9491 - val_regression_loss: 618.5021\n",
            "***************************** elapsed_time is:  5.9949212074279785\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 243ms/step - loss: 187662.2500 - regression_loss: 50606.9062 - val_loss: 23472.7461 - val_regression_loss: 11736.7070\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 158341.6094 - regression_loss: 44375.7695 - val_loss: 20603.4238 - val_regression_loss: 10298.3789\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 143368.3594 - regression_loss: 38514.6562 - val_loss: 17031.5664 - val_regression_loss: 8507.6016\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 116893.3047 - regression_loss: 31355.5547 - val_loss: 12969.2412 - val_regression_loss: 6470.3677\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 86177.3125 - regression_loss: 23444.4551 - val_loss: 9145.6963 - val_regression_loss: 4551.5449\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 60949.3164 - regression_loss: 16660.9102 - val_loss: 6701.1816 - val_regression_loss: 3321.9021\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 49755.7031 - regression_loss: 13246.2139 - val_loss: 6072.0542 - val_regression_loss: 3002.8381\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 50052.9961 - regression_loss: 13483.7305 - val_loss: 5384.6909 - val_regression_loss: 2662.0754\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 46861.0234 - regression_loss: 12538.1348 - val_loss: 4409.3638 - val_regression_loss: 2181.3423\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 39044.6367 - regression_loss: 10303.3467 - val_loss: 3957.0227 - val_regression_loss: 1962.2185\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 33710.9648 - regression_loss: 8906.1963 - val_loss: 4171.1606 - val_regression_loss: 2074.4783\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 33776.6836 - regression_loss: 9035.3096 - val_loss: 4526.8779 - val_regression_loss: 2254.9314\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 35714.4414 - regression_loss: 9485.2969 - val_loss: 4440.9380 - val_regression_loss: 2211.9944\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 33329.3711 - regression_loss: 9085.7861 - val_loss: 3906.5117 - val_regression_loss: 1942.9352\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 29367.8184 - regression_loss: 7947.5811 - val_loss: 3259.9653 - val_regression_loss: 1616.6688\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 25960.2051 - regression_loss: 6866.2446 - val_loss: 2820.9263 - val_regression_loss: 1393.7048\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 23887.2246 - regression_loss: 6370.1665 - val_loss: 2640.0591 - val_regression_loss: 1300.2112\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 23539.9199 - regression_loss: 6342.5845 - val_loss: 2482.6313 - val_regression_loss: 1219.8420\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 22901.7148 - regression_loss: 6116.8594 - val_loss: 2261.7422 - val_regression_loss: 1109.6310\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 20821.5820 - regression_loss: 5507.5493 - val_loss: 2130.0740 - val_regression_loss: 1045.5741\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 18935.8789 - regression_loss: 5006.8394 - val_loss: 2082.7473 - val_regression_loss: 1023.9523\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 15967.8047 - regression_loss: 4753.5908 - val_loss: 1915.8032 - val_regression_loss: 941.6396\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 16735.1562 - regression_loss: 4404.0283 - val_loss: 1571.5519 - val_regression_loss: 769.3442\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 14443.2100 - regression_loss: 3818.3303 - val_loss: 1320.1870 - val_regression_loss: 643.0328\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 13096.5439 - regression_loss: 3465.7600 - val_loss: 1206.5760 - val_regression_loss: 586.1033\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 12514.9932 - regression_loss: 3281.8450 - val_loss: 1082.8843 - val_regression_loss: 525.0804\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 11121.4434 - regression_loss: 2931.6433 - val_loss: 984.9638 - val_regression_loss: 477.1700\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 10230.3975 - regression_loss: 2648.0105 - val_loss: 916.6906 - val_regression_loss: 443.3737\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 9090.4756 - regression_loss: 2463.9001 - val_loss: 776.7701 - val_regression_loss: 372.4002\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 8563.0684 - regression_loss: 2208.4919 - val_loss: 650.3782 - val_regression_loss: 307.1124\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 7333.5918 - regression_loss: 1944.6515 - val_loss: 609.5187 - val_regression_loss: 285.2305\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 7070.1104 - regression_loss: 1823.5735 - val_loss: 526.2474 - val_regression_loss: 243.7314\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 6469.3442 - regression_loss: 1645.7329 - val_loss: 464.6982 - val_regression_loss: 213.7888\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 5896.5825 - regression_loss: 1542.5012 - val_loss: 428.6721 - val_regression_loss: 195.9708\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 5647.0610 - regression_loss: 1446.9919 - val_loss: 416.1149 - val_regression_loss: 189.1197\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 5178.2202 - regression_loss: 1322.4852 - val_loss: 438.8364 - val_regression_loss: 200.1546\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 4871.7192 - regression_loss: 1251.8636 - val_loss: 401.8257 - val_regression_loss: 182.2558\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 4541.2671 - regression_loss: 1161.0956 - val_loss: 365.3652 - val_regression_loss: 164.7647\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 4297.0835 - regression_loss: 1107.0471 - val_loss: 370.8251 - val_regression_loss: 167.2787\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 4157.4683 - regression_loss: 1048.0225 - val_loss: 405.3634 - val_regression_loss: 183.8437\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3979.3875 - regression_loss: 997.5793 - val_loss: 415.1974 - val_regression_loss: 188.4002\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3803.8945 - regression_loss: 959.5070 - val_loss: 382.6846 - val_regression_loss: 172.2337\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 3709.8320 - regression_loss: 922.3055 - val_loss: 368.9210 - val_regression_loss: 165.2782\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 3237.9126 - regression_loss: 890.9089 - val_loss: 374.3785 - val_regression_loss: 167.6859\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 3140.3235 - regression_loss: 858.9128 - val_loss: 398.7379 - val_regression_loss: 179.5267\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 3265.3403 - regression_loss: 833.9694 - val_loss: 366.0988 - val_regression_loss: 163.5710\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 3216.9697 - regression_loss: 802.2263 - val_loss: 338.9614 - val_regression_loss: 150.4029\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3111.3352 - regression_loss: 780.1527 - val_loss: 339.3269 - val_regression_loss: 150.6182\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3053.2227 - regression_loss: 754.8721 - val_loss: 354.3605 - val_regression_loss: 157.9999\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2946.0747 - regression_loss: 733.5997 - val_loss: 333.7200 - val_regression_loss: 147.9402\n",
            "***************************** elapsed_time is:  6.454598426818848\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 183ms/step - loss: 19180.8203 - regression_loss: 5081.6543 - val_loss: 1403.9348 - val_regression_loss: 680.2911\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 13483.0215 - regression_loss: 3512.0930 - val_loss: 934.5988 - val_regression_loss: 444.6577\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 8465.7305 - regression_loss: 2223.9978 - val_loss: 671.7075 - val_regression_loss: 312.1618\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 5561.3296 - regression_loss: 1419.6804 - val_loss: 586.7928 - val_regression_loss: 270.1346\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 4626.5425 - regression_loss: 1148.0164 - val_loss: 317.2383 - val_regression_loss: 137.5312\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2560.3569 - regression_loss: 603.8667 - val_loss: 306.2517 - val_regression_loss: 134.3728\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2557.4138 - regression_loss: 632.1516 - val_loss: 409.6559 - val_regression_loss: 187.2861\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 3456.2346 - regression_loss: 872.4960 - val_loss: 330.2702 - val_regression_loss: 147.7083\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2759.6025 - regression_loss: 685.2087 - val_loss: 274.4539 - val_regression_loss: 119.4428\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2273.0706 - regression_loss: 543.4305 - val_loss: 263.7453 - val_regression_loss: 113.6891\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2083.9609 - regression_loss: 499.5912 - val_loss: 249.0704 - val_regression_loss: 106.1469\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1933.7622 - regression_loss: 462.2476 - val_loss: 254.8427 - val_regression_loss: 109.0459\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2012.1185 - regression_loss: 481.3391 - val_loss: 261.8955 - val_regression_loss: 112.7322\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2072.2122 - regression_loss: 505.4624 - val_loss: 244.9561 - val_regression_loss: 104.5009\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2013.7413 - regression_loss: 473.1537 - val_loss: 222.4420 - val_regression_loss: 93.5328\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1764.3298 - regression_loss: 419.5499 - val_loss: 215.3703 - val_regression_loss: 90.3363\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1714.8862 - regression_loss: 399.7390 - val_loss: 210.7905 - val_regression_loss: 88.3975\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1646.6213 - regression_loss: 391.5521 - val_loss: 207.6060 - val_regression_loss: 87.0833\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1573.2869 - regression_loss: 384.9426 - val_loss: 213.3878 - val_regression_loss: 90.1114\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1666.8610 - regression_loss: 392.3179 - val_loss: 216.9804 - val_regression_loss: 91.8705\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1615.1565 - regression_loss: 387.6165 - val_loss: 213.2066 - val_regression_loss: 89.8091\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1585.9962 - regression_loss: 367.1553 - val_loss: 211.8232 - val_regression_loss: 88.9150\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1533.1632 - regression_loss: 357.2001 - val_loss: 210.4264 - val_regression_loss: 88.0789\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1536.0271 - regression_loss: 351.5018 - val_loss: 207.2324 - val_regression_loss: 86.4452\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1506.3938 - regression_loss: 348.0658 - val_loss: 205.9952 - val_regression_loss: 85.8458\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1495.8707 - regression_loss: 349.2846 - val_loss: 203.7511 - val_regression_loss: 84.7528\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1472.6216 - regression_loss: 343.1493 - val_loss: 201.5769 - val_regression_loss: 83.6995\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1448.6580 - regression_loss: 334.8495 - val_loss: 201.5188 - val_regression_loss: 83.7153\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1448.2499 - regression_loss: 329.8912 - val_loss: 200.5741 - val_regression_loss: 83.3063\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1428.4492 - regression_loss: 327.2275 - val_loss: 198.7935 - val_regression_loss: 82.4684\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1402.5504 - regression_loss: 324.7830 - val_loss: 196.6911 - val_regression_loss: 81.4184\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1399.4983 - regression_loss: 321.3783 - val_loss: 195.0202 - val_regression_loss: 80.5136\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 1397.3187 - regression_loss: 315.3606 - val_loss: 194.7005 - val_regression_loss: 80.2595\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1349.1270 - regression_loss: 311.2611 - val_loss: 194.6832 - val_regression_loss: 80.1718\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1348.3401 - regression_loss: 307.7582 - val_loss: 194.3161 - val_regression_loss: 79.9490\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1318.5933 - regression_loss: 304.1547 - val_loss: 193.8753 - val_regression_loss: 79.7268\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1323.4215 - regression_loss: 300.9458 - val_loss: 193.8040 - val_regression_loss: 79.6959\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1332.5387 - regression_loss: 297.2274 - val_loss: 193.4315 - val_regression_loss: 79.5285\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1326.4855 - regression_loss: 293.6398 - val_loss: 192.3623 - val_regression_loss: 79.0433\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1271.0432 - regression_loss: 290.6994 - val_loss: 191.2752 - val_regression_loss: 78.5511\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1285.3170 - regression_loss: 288.0747 - val_loss: 191.2859 - val_regression_loss: 78.5564\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1258.0579 - regression_loss: 285.0829 - val_loss: 192.3596 - val_regression_loss: 79.0435\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1263.9629 - regression_loss: 282.1373 - val_loss: 193.1297 - val_regression_loss: 79.4050\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1240.2981 - regression_loss: 279.6985 - val_loss: 193.1338 - val_regression_loss: 79.4209\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1242.9385 - regression_loss: 277.7597 - val_loss: 193.1034 - val_regression_loss: 79.4093\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1222.8693 - regression_loss: 275.5872 - val_loss: 193.4928 - val_regression_loss: 79.6162\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1252.4900 - regression_loss: 274.0674 - val_loss: 193.9241 - val_regression_loss: 79.8627\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 1224.8242 - regression_loss: 272.8097 - val_loss: 193.3338 - val_regression_loss: 79.6074\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1227.4655 - regression_loss: 271.7702 - val_loss: 192.8889 - val_regression_loss: 79.4143\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1231.1130 - regression_loss: 270.7030 - val_loss: 193.3660 - val_regression_loss: 79.6313\n",
            "***************************** elapsed_time is:  5.960366725921631\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 161ms/step - loss: 7400.9785 - regression_loss: 1885.7095 - val_loss: 649.3391 - val_regression_loss: 301.9071\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 4689.9590 - regression_loss: 1192.1787 - val_loss: 468.1853 - val_regression_loss: 212.2887\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 3600.9065 - regression_loss: 893.4825 - val_loss: 300.8884 - val_regression_loss: 128.5183\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2160.6091 - regression_loss: 496.6221 - val_loss: 234.9543 - val_regression_loss: 94.9805\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 1417.5887 - regression_loss: 304.1993 - val_loss: 248.1547 - val_regression_loss: 101.9508\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1716.4006 - regression_loss: 386.2483 - val_loss: 224.9529 - val_regression_loss: 91.9384\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 1919.7017 - regression_loss: 435.4689 - val_loss: 184.2245 - val_regression_loss: 73.0257\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1578.2570 - regression_loss: 357.2535 - val_loss: 166.9086 - val_regression_loss: 65.2541\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1261.5886 - regression_loss: 271.8510 - val_loss: 191.0058 - val_regression_loss: 77.9814\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1279.6088 - regression_loss: 282.7177 - val_loss: 205.2908 - val_regression_loss: 85.8063\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1375.6713 - regression_loss: 312.9266 - val_loss: 191.3040 - val_regression_loss: 79.4121\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1382.9886 - regression_loss: 310.7956 - val_loss: 175.3305 - val_regression_loss: 71.6631\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1332.4154 - regression_loss: 297.8177 - val_loss: 164.3022 - val_regression_loss: 65.9073\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1218.0684 - regression_loss: 269.3282 - val_loss: 167.1300 - val_regression_loss: 66.7915\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1168.2335 - regression_loss: 255.1252 - val_loss: 173.4144 - val_regression_loss: 69.4514\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1198.9137 - regression_loss: 264.1465 - val_loss: 164.3646 - val_regression_loss: 64.7288\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1217.6038 - regression_loss: 264.6343 - val_loss: 153.7938 - val_regression_loss: 59.5846\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1220.8085 - regression_loss: 262.4200 - val_loss: 150.6064 - val_regression_loss: 58.3141\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1158.9276 - regression_loss: 254.2997 - val_loss: 156.6391 - val_regression_loss: 61.5957\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 1146.1058 - regression_loss: 250.5100 - val_loss: 163.4541 - val_regression_loss: 65.1717\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1168.6527 - regression_loss: 254.9936 - val_loss: 162.5207 - val_regression_loss: 64.8148\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1165.0831 - regression_loss: 256.8411 - val_loss: 156.8181 - val_regression_loss: 62.0020\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1173.4263 - regression_loss: 255.7684 - val_loss: 153.7369 - val_regression_loss: 60.3546\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1166.2109 - regression_loss: 251.9515 - val_loss: 155.4799 - val_regression_loss: 61.0039\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1137.7113 - regression_loss: 248.1037 - val_loss: 158.0672 - val_regression_loss: 62.0936\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1139.2952 - regression_loss: 248.4564 - val_loss: 154.7328 - val_regression_loss: 60.3622\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1146.8802 - regression_loss: 248.2925 - val_loss: 150.7450 - val_regression_loss: 58.3986\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1155.0370 - regression_loss: 248.3021 - val_loss: 151.0766 - val_regression_loss: 58.6032\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1119.8140 - regression_loss: 246.9644 - val_loss: 155.4946 - val_regression_loss: 60.8396\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1137.3064 - regression_loss: 246.2240 - val_loss: 158.3327 - val_regression_loss: 62.2864\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1117.4028 - regression_loss: 246.9453 - val_loss: 154.2181 - val_regression_loss: 60.2982\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1105.3456 - regression_loss: 247.0066 - val_loss: 150.5485 - val_regression_loss: 58.4781\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1127.0771 - regression_loss: 246.8938 - val_loss: 153.4584 - val_regression_loss: 59.8351\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1119.5507 - regression_loss: 245.4201 - val_loss: 156.4545 - val_regression_loss: 61.2461\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1101.8162 - regression_loss: 244.9632 - val_loss: 151.2502 - val_regression_loss: 58.6794\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1105.1036 - regression_loss: 244.1820 - val_loss: 149.0322 - val_regression_loss: 57.5981\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1105.8728 - regression_loss: 244.4197 - val_loss: 153.7674 - val_regression_loss: 59.9335\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1131.7690 - regression_loss: 243.1043 - val_loss: 156.9077 - val_regression_loss: 61.4957\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1121.6239 - regression_loss: 243.2226 - val_loss: 154.6518 - val_regression_loss: 60.3973\n",
            "Epoch 40/50\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 1156.9957 - regression_loss: 463.1189\n",
            "Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1106.0841 - regression_loss: 243.0641 - val_loss: 149.5987 - val_regression_loss: 57.9145\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1101.2826 - regression_loss: 243.7860 - val_loss: 151.8013 - val_regression_loss: 58.9836\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1094.7576 - regression_loss: 242.4571 - val_loss: 155.5666 - val_regression_loss: 60.8180\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1111.7782 - regression_loss: 241.9489 - val_loss: 156.2675 - val_regression_loss: 61.1559\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1104.6238 - regression_loss: 241.8481 - val_loss: 154.7412 - val_regression_loss: 60.3966\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1082.3713 - regression_loss: 241.5349 - val_loss: 152.3266 - val_regression_loss: 59.2017\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1083.9805 - regression_loss: 241.3958 - val_loss: 152.0203 - val_regression_loss: 59.0509\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1096.6943 - regression_loss: 241.5207 - val_loss: 154.1340 - val_regression_loss: 60.0778\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1104.0786 - regression_loss: 240.8310 - val_loss: 155.5521 - val_regression_loss: 60.7717\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1089.6327 - regression_loss: 240.7601 - val_loss: 155.1742 - val_regression_loss: 60.5731\n",
            "Epoch 50/50\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 1132.2778 - regression_loss: 450.3220\n",
            "Epoch 00050: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1083.0891 - regression_loss: 240.6320 - val_loss: 153.2760 - val_regression_loss: 59.6402\n",
            "***************************** elapsed_time is:  6.429349899291992\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 205ms/step - loss: 18753.7793 - regression_loss: 5007.5161 - val_loss: 1734.8007 - val_regression_loss: 846.2074\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 12872.0537 - regression_loss: 3421.1184 - val_loss: 1154.1876 - val_regression_loss: 554.1697\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 8497.6016 - regression_loss: 2198.7378 - val_loss: 768.9353 - val_regression_loss: 360.0548\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 5621.1772 - regression_loss: 1426.7299 - val_loss: 587.1592 - val_regression_loss: 269.9175\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 4122.3896 - regression_loss: 1043.6700 - val_loss: 382.6672 - val_regression_loss: 170.5894\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2501.9043 - regression_loss: 588.0109 - val_loss: 320.7159 - val_regression_loss: 143.0529\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1866.2395 - regression_loss: 445.5444 - val_loss: 442.3556 - val_regression_loss: 206.3566\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2674.4641 - regression_loss: 685.0414 - val_loss: 474.1356 - val_regression_loss: 222.8108\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2919.3359 - regression_loss: 739.1142 - val_loss: 379.1546 - val_regression_loss: 174.5894\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2226.0586 - regression_loss: 543.6692 - val_loss: 290.1552 - val_regression_loss: 128.8217\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1654.0824 - regression_loss: 383.0520 - val_loss: 263.9236 - val_regression_loss: 114.3997\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1581.4711 - regression_loss: 361.0052 - val_loss: 275.0766 - val_regression_loss: 119.0163\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1773.3604 - regression_loss: 407.6892 - val_loss: 283.4178 - val_regression_loss: 122.7986\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1851.2607 - regression_loss: 427.4543 - val_loss: 278.7858 - val_regression_loss: 120.6566\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1729.1531 - regression_loss: 403.1410 - val_loss: 270.3989 - val_regression_loss: 116.9975\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1628.6198 - regression_loss: 367.0861 - val_loss: 261.4048 - val_regression_loss: 113.1839\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1484.2032 - regression_loss: 338.9778 - val_loss: 252.6471 - val_regression_loss: 109.4769\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1440.7313 - regression_loss: 325.1793 - val_loss: 248.3800 - val_regression_loss: 107.8788\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1441.9602 - regression_loss: 330.2562 - val_loss: 246.8392 - val_regression_loss: 107.3892\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1453.7179 - regression_loss: 340.6061 - val_loss: 242.6557 - val_regression_loss: 105.2809\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1460.9529 - regression_loss: 336.2060 - val_loss: 235.5683 - val_regression_loss: 101.4887\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1416.8938 - regression_loss: 319.7476 - val_loss: 230.4239 - val_regression_loss: 98.5578\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1342.6941 - regression_loss: 304.6691 - val_loss: 226.4338 - val_regression_loss: 96.2094\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1334.4254 - regression_loss: 299.0380 - val_loss: 222.0368 - val_regression_loss: 93.7367\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1334.9052 - regression_loss: 298.3152 - val_loss: 216.6946 - val_regression_loss: 90.9092\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1331.3910 - regression_loss: 297.5555 - val_loss: 212.4399 - val_regression_loss: 88.7788\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1336.6741 - regression_loss: 295.6357 - val_loss: 209.6445 - val_regression_loss: 87.5212\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1275.5551 - regression_loss: 290.3112 - val_loss: 208.8968 - val_regression_loss: 87.3605\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1280.0298 - regression_loss: 286.6016 - val_loss: 208.9429 - val_regression_loss: 87.5923\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1278.4597 - regression_loss: 285.8274 - val_loss: 208.7350 - val_regression_loss: 87.6348\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1256.8385 - regression_loss: 285.9428 - val_loss: 206.5513 - val_regression_loss: 86.5866\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1287.5551 - regression_loss: 284.8841 - val_loss: 204.9731 - val_regression_loss: 85.7535\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1262.3992 - regression_loss: 282.8457 - val_loss: 204.6635 - val_regression_loss: 85.4935\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1258.1298 - regression_loss: 279.3771 - val_loss: 205.0886 - val_regression_loss: 85.5808\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1234.0745 - regression_loss: 277.3931 - val_loss: 204.5659 - val_regression_loss: 85.2078\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1229.0447 - regression_loss: 276.1339 - val_loss: 201.0688 - val_regression_loss: 83.3741\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1201.6707 - regression_loss: 274.6117 - val_loss: 198.6592 - val_regression_loss: 82.1523\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1235.0732 - regression_loss: 273.4128 - val_loss: 197.5040 - val_regression_loss: 81.6012\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1239.1958 - regression_loss: 272.1218 - val_loss: 198.0301 - val_regression_loss: 81.9201\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1227.5448 - regression_loss: 271.1768 - val_loss: 198.7563 - val_regression_loss: 82.3266\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1234.6929 - regression_loss: 270.6913 - val_loss: 198.5131 - val_regression_loss: 82.2074\n",
            "Epoch 42/50\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 1277.8654 - regression_loss: 521.2822\n",
            "Epoch 00042: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1220.8898 - regression_loss: 269.7295 - val_loss: 196.8842 - val_regression_loss: 81.3473\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1203.3239 - regression_loss: 268.7822 - val_loss: 197.1281 - val_regression_loss: 81.4383\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1198.0160 - regression_loss: 268.1992 - val_loss: 197.6106 - val_regression_loss: 81.6453\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1199.9500 - regression_loss: 267.7007 - val_loss: 198.7236 - val_regression_loss: 82.1752\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 1206.9285 - regression_loss: 267.0551 - val_loss: 198.6350 - val_regression_loss: 82.0947\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1206.5358 - regression_loss: 266.6987 - val_loss: 197.0907 - val_regression_loss: 81.2751\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1196.3976 - regression_loss: 265.9530 - val_loss: 195.8835 - val_regression_loss: 80.6383\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1211.0420 - regression_loss: 265.4806 - val_loss: 195.3961 - val_regression_loss: 80.3673\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1177.1102 - regression_loss: 265.0953 - val_loss: 195.3957 - val_regression_loss: 80.3493\n",
            "***************************** elapsed_time is:  5.8973963260650635\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 216ms/step - loss: 204535.9219 - regression_loss: 55860.0430 - val_loss: 22796.1328 - val_regression_loss: 11449.3506\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 181704.9688 - regression_loss: 49845.0781 - val_loss: 19976.3516 - val_regression_loss: 10036.1943\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 158020.3125 - regression_loss: 43429.7852 - val_loss: 16244.5928 - val_regression_loss: 8164.9795\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 127859.7266 - regression_loss: 35023.9883 - val_loss: 11636.8379 - val_regression_loss: 5851.9121\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 90997.6953 - regression_loss: 24753.2246 - val_loss: 6766.5435 - val_regression_loss: 3400.0181\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 51205.5703 - regression_loss: 13966.6650 - val_loss: 2929.5774 - val_regression_loss: 1448.8074\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 21234.7715 - regression_loss: 5672.6597 - val_loss: 1997.1073 - val_regression_loss: 919.0415\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 14750.0137 - regression_loss: 3722.4744 - val_loss: 3396.3572 - val_regression_loss: 1566.4222\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 26455.6777 - regression_loss: 6735.6226 - val_loss: 3253.0554 - val_regression_loss: 1513.5735\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 25992.5820 - regression_loss: 6515.8486 - val_loss: 2088.8044 - val_regression_loss: 977.2074\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 16321.4297 - regression_loss: 4075.6921 - val_loss: 1372.9305 - val_regression_loss: 656.1196\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 9794.3438 - regression_loss: 2542.6328 - val_loss: 1395.2567 - val_regression_loss: 688.5324\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 9315.9893 - regression_loss: 2563.2644 - val_loss: 1695.8527 - val_regression_loss: 847.8099\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 11695.7422 - regression_loss: 3167.1101 - val_loss: 1850.1434 - val_regression_loss: 925.9736\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 12496.4990 - regression_loss: 3468.1926 - val_loss: 1751.5792 - val_regression_loss: 873.2376\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 11757.8945 - regression_loss: 3233.9910 - val_loss: 1483.3925 - val_regression_loss: 733.3534\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 9764.2139 - regression_loss: 2638.2158 - val_loss: 1206.1115 - val_regression_loss: 588.1746\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 7924.7363 - regression_loss: 2050.8599 - val_loss: 1050.1318 - val_regression_loss: 504.0535\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 6642.7202 - regression_loss: 1718.5259 - val_loss: 1053.3860 - val_regression_loss: 501.0389\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 6643.0527 - regression_loss: 1728.2466 - val_loss: 1132.6921 - val_regression_loss: 538.2868\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 7332.7974 - regression_loss: 1887.1401 - val_loss: 1163.1506 - val_regression_loss: 553.5709\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 7667.0513 - regression_loss: 1933.9628 - val_loss: 1102.4467 - val_regression_loss: 525.2741\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 6966.9150 - regression_loss: 1787.1754 - val_loss: 1013.2464 - val_regression_loss: 483.8170\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 6219.6118 - regression_loss: 1589.5499 - val_loss: 961.3680 - val_regression_loss: 461.1776\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 5797.0977 - regression_loss: 1486.7490 - val_loss: 958.7252 - val_regression_loss: 462.6825\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 5755.6387 - regression_loss: 1486.5414 - val_loss: 974.6211 - val_regression_loss: 472.5169\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 5777.0273 - regression_loss: 1525.8733 - val_loss: 975.7319 - val_regression_loss: 473.9511\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 5825.8652 - regression_loss: 1528.0566 - val_loss: 951.8180 - val_regression_loss: 461.9271\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 5384.3979 - regression_loss: 1470.9575 - val_loss: 915.9879 - val_regression_loss: 443.2175\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 5386.0054 - regression_loss: 1390.7869 - val_loss: 886.1835 - val_regression_loss: 427.0862\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 5078.4751 - regression_loss: 1320.5736 - val_loss: 871.0207 - val_regression_loss: 418.1125\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 4974.3936 - regression_loss: 1284.1797 - val_loss: 863.8163 - val_regression_loss: 413.2982\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 4928.8018 - regression_loss: 1266.3641 - val_loss: 852.1025 - val_regression_loss: 406.6241\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 4863.7451 - regression_loss: 1238.8318 - val_loss: 832.1418 - val_regression_loss: 396.3246\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 4738.1597 - regression_loss: 1197.4863 - val_loss: 810.9391 - val_regression_loss: 385.9240\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 4452.4219 - regression_loss: 1153.2225 - val_loss: 795.4094 - val_regression_loss: 378.6731\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 4410.8350 - regression_loss: 1124.6241 - val_loss: 785.0043 - val_regression_loss: 374.1450\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 4247.5806 - regression_loss: 1103.7700 - val_loss: 774.6535 - val_regression_loss: 369.6722\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 4282.4932 - regression_loss: 1082.9117 - val_loss: 760.7222 - val_regression_loss: 363.2296\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3951.9312 - regression_loss: 1052.8888 - val_loss: 744.6342 - val_regression_loss: 355.4901\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3955.8660 - regression_loss: 1022.8977 - val_loss: 728.3093 - val_regression_loss: 347.3519\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3913.3357 - regression_loss: 995.3976 - val_loss: 712.7571 - val_regression_loss: 339.4934\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 3838.8911 - regression_loss: 972.2780 - val_loss: 696.3046 - val_regression_loss: 331.0985\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3781.9924 - regression_loss: 948.9635 - val_loss: 678.2095 - val_regression_loss: 321.8441\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3648.5564 - regression_loss: 924.0800 - val_loss: 660.0027 - val_regression_loss: 312.5785\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 3580.5596 - regression_loss: 898.3161 - val_loss: 643.2446 - val_regression_loss: 304.0797\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 3422.9648 - regression_loss: 877.0757 - val_loss: 628.3367 - val_regression_loss: 296.6373\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3375.6399 - regression_loss: 858.1696 - val_loss: 611.7945 - val_regression_loss: 288.3082\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3330.6326 - regression_loss: 839.6249 - val_loss: 594.0291 - val_regression_loss: 279.4127\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 3239.0789 - regression_loss: 819.5758 - val_loss: 577.5641 - val_regression_loss: 271.2335\n",
            "***************************** elapsed_time is:  5.99986457824707\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 166ms/step - loss: 23896.6953 - regression_loss: 6289.5459 - val_loss: 1951.5618 - val_regression_loss: 941.3129\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 17630.0234 - regression_loss: 4542.1001 - val_loss: 1298.2390 - val_regression_loss: 622.7516\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 11537.8145 - regression_loss: 3021.1140 - val_loss: 704.5651 - val_regression_loss: 335.7398\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 6166.6675 - regression_loss: 1645.6873 - val_loss: 514.5969 - val_regression_loss: 249.8465\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 4420.1572 - regression_loss: 1187.0519 - val_loss: 517.0244 - val_regression_loss: 249.1758\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 4081.1704 - regression_loss: 1073.8563 - val_loss: 424.0166 - val_regression_loss: 193.5792\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3087.3818 - regression_loss: 776.1785 - val_loss: 389.3429 - val_regression_loss: 168.0765\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 2837.2888 - regression_loss: 666.5320 - val_loss: 429.8612 - val_regression_loss: 184.8923\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 3161.6431 - regression_loss: 750.7458 - val_loss: 411.9753 - val_regression_loss: 177.1051\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 3137.2913 - regression_loss: 740.7319 - val_loss: 325.0800 - val_regression_loss: 137.3068\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2567.2178 - regression_loss: 600.2165 - val_loss: 246.1527 - val_regression_loss: 102.2442\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2019.7188 - regression_loss: 473.4690 - val_loss: 228.3755 - val_regression_loss: 97.2887\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1898.5204 - regression_loss: 459.7108 - val_loss: 255.2547 - val_regression_loss: 113.2469\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2111.8503 - regression_loss: 531.1981 - val_loss: 264.5477 - val_regression_loss: 118.5588\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2192.3513 - regression_loss: 549.7756 - val_loss: 239.1120 - val_regression_loss: 104.9043\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1987.5107 - regression_loss: 487.5880 - val_loss: 214.8362 - val_regression_loss: 90.9810\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1709.4028 - regression_loss: 426.3313 - val_loss: 212.9848 - val_regression_loss: 88.0908\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1655.5238 - regression_loss: 407.9527 - val_loss: 221.1359 - val_regression_loss: 90.5084\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1766.5554 - regression_loss: 407.4022 - val_loss: 225.9590 - val_regression_loss: 91.7153\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1713.3260 - regression_loss: 395.1647 - val_loss: 228.6667 - val_regression_loss: 92.4950\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1677.5192 - regression_loss: 380.3399 - val_loss: 231.4691 - val_regression_loss: 94.1160\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1681.9597 - regression_loss: 372.7948 - val_loss: 229.5793 - val_regression_loss: 94.1424\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1613.3689 - regression_loss: 367.9810 - val_loss: 222.5710 - val_regression_loss: 91.9051\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1579.0253 - regression_loss: 361.6472 - val_loss: 215.3352 - val_regression_loss: 89.3017\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1540.5709 - regression_loss: 360.5002 - val_loss: 212.9619 - val_regression_loss: 88.6385\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1561.2316 - regression_loss: 363.7106 - val_loss: 213.7036 - val_regression_loss: 89.1823\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1534.2469 - regression_loss: 362.1236 - val_loss: 215.3560 - val_regression_loss: 89.9458\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1512.6028 - regression_loss: 352.8321 - val_loss: 219.3209 - val_regression_loss: 91.6053\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1495.2609 - regression_loss: 344.7750 - val_loss: 223.9940 - val_regression_loss: 93.4452\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1496.7550 - regression_loss: 339.8135 - val_loss: 225.9810 - val_regression_loss: 93.9080\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1479.4393 - regression_loss: 334.6827 - val_loss: 226.0630 - val_regression_loss: 93.5847\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1460.6498 - regression_loss: 330.2712 - val_loss: 226.5356 - val_regression_loss: 93.7675\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1452.5944 - regression_loss: 328.4474 - val_loss: 227.3493 - val_regression_loss: 94.4811\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1468.6901 - regression_loss: 326.1989 - val_loss: 229.5757 - val_regression_loss: 96.1593\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1442.3391 - regression_loss: 323.8760 - val_loss: 232.4501 - val_regression_loss: 98.1017\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1432.7784 - regression_loss: 324.2843 - val_loss: 233.2761 - val_regression_loss: 98.7880\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1418.2461 - regression_loss: 323.7970 - val_loss: 232.1676 - val_regression_loss: 98.2358\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1376.4731 - regression_loss: 321.3764 - val_loss: 230.7653 - val_regression_loss: 97.3130\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1398.1908 - regression_loss: 317.8913 - val_loss: 231.1740 - val_regression_loss: 97.4209\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1390.5514 - regression_loss: 314.7726 - val_loss: 232.1077 - val_regression_loss: 97.8435\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1384.4210 - regression_loss: 311.9142 - val_loss: 233.3016 - val_regression_loss: 98.4485\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1382.2827 - regression_loss: 310.2353 - val_loss: 233.7485 - val_regression_loss: 98.6850\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1363.4778 - regression_loss: 308.8672 - val_loss: 232.9054 - val_regression_loss: 98.2265\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1378.6492 - regression_loss: 306.3738 - val_loss: 233.0410 - val_regression_loss: 98.4242\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1337.1898 - regression_loss: 305.1587 - val_loss: 233.0260 - val_regression_loss: 98.5598\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1330.0081 - regression_loss: 304.1554 - val_loss: 233.1691 - val_regression_loss: 98.8025\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1352.6846 - regression_loss: 302.9893 - val_loss: 232.7221 - val_regression_loss: 98.5818\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 1322.3141 - regression_loss: 301.3127 - val_loss: 231.2087 - val_regression_loss: 97.6958\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1322.9911 - regression_loss: 298.9339 - val_loss: 230.9809 - val_regression_loss: 97.5036\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1323.6580 - regression_loss: 297.1370 - val_loss: 230.1642 - val_regression_loss: 97.0567\n",
            "***************************** elapsed_time is:  6.195801734924316\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 218ms/step - loss: 782940.5625 - regression_loss: 212622.0938 - val_loss: 96802.2266 - val_regression_loss: 48370.6719\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 722805.2500 - regression_loss: 196498.9375 - val_loss: 88079.2266 - val_regression_loss: 44011.3594\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 643165.3750 - regression_loss: 177593.0000 - val_loss: 75926.3672 - val_regression_loss: 37937.2227\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 561816.4375 - regression_loss: 151578.5156 - val_loss: 60278.5273 - val_regression_loss: 30115.2539\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 431667.4062 - regression_loss: 118131.2422 - val_loss: 42552.6055 - val_regression_loss: 21253.3555\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 303159.4688 - regression_loss: 81130.9219 - val_loss: 25821.6582 - val_regression_loss: 12887.5664\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 174861.0469 - regression_loss: 47415.1211 - val_loss: 15095.2227 - val_regression_loss: 7522.6797\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 104247.2344 - regression_loss: 29238.5000 - val_loss: 14907.1240 - val_regression_loss: 7427.0254\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 127217.5938 - regression_loss: 34832.6836 - val_loss: 16584.1035 - val_regression_loss: 8266.7041\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 151980.4062 - regression_loss: 40934.4688 - val_loss: 13410.7285 - val_regression_loss: 6683.1606\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 120767.0078 - regression_loss: 32698.4512 - val_loss: 10076.3301 - val_regression_loss: 5019.0591\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 84319.8438 - regression_loss: 22591.5449 - val_loss: 9426.2715 - val_regression_loss: 4696.1880\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 67618.8125 - regression_loss: 18403.1309 - val_loss: 10822.0908 - val_regression_loss: 5395.2754\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 70809.0703 - regression_loss: 19435.4551 - val_loss: 12518.8037 - val_regression_loss: 6244.1450\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 81130.4375 - regression_loss: 21824.9922 - val_loss: 13258.0869 - val_regression_loss: 6613.8843\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 82102.1094 - regression_loss: 22947.5078 - val_loss: 12793.6387 - val_regression_loss: 6381.4907\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 78473.6797 - regression_loss: 22003.6406 - val_loss: 11425.4590 - val_regression_loss: 5697.0234\n",
            "Epoch 18/50\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 77048.7031 - regression_loss: 38408.6836\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 73550.6797 - regression_loss: 19677.3535 - val_loss: 9749.2041 - val_regression_loss: 4858.3374\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 64402.2812 - regression_loss: 17058.4004 - val_loss: 8997.1787 - val_regression_loss: 4481.9868\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 60025.2617 - regression_loss: 16008.5469 - val_loss: 8380.6904 - val_regression_loss: 4173.3853\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 55771.2852 - regression_loss: 15334.4707 - val_loss: 7936.7754 - val_regression_loss: 3951.0605\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 52545.1758 - regression_loss: 14971.0146 - val_loss: 7653.2666 - val_regression_loss: 3808.9358\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 54429.2148 - regression_loss: 14912.1973 - val_loss: 7469.4780 - val_regression_loss: 3716.6951\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 54545.4805 - regression_loss: 14926.7402 - val_loss: 7330.0229 - val_regression_loss: 3646.6614\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 53387.8594 - regression_loss: 14852.9336 - val_loss: 7202.3936 - val_regression_loss: 3582.6099\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 53752.8320 - regression_loss: 14627.1367 - val_loss: 7085.2314 - val_regression_loss: 3523.8840\n",
            "Epoch 27/50\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 56483.9922 - regression_loss: 28102.1309\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 53898.9414 - regression_loss: 14284.2588 - val_loss: 6996.3115 - val_regression_loss: 3479.3892\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 51490.0312 - regression_loss: 13887.2979 - val_loss: 6966.7490 - val_regression_loss: 3464.6418\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 49511.1641 - regression_loss: 13693.3281 - val_loss: 6947.4917 - val_regression_loss: 3455.0906\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 48707.8398 - regression_loss: 13510.7842 - val_loss: 6935.2808 - val_regression_loss: 3449.0916\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 46419.2383 - regression_loss: 13353.8330 - val_loss: 6926.6113 - val_regression_loss: 3444.8845\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 48845.0234 - regression_loss: 13206.6211 - val_loss: 6912.3120 - val_regression_loss: 3437.8716\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 48636.5781 - regression_loss: 13071.5498 - val_loss: 6889.0400 - val_regression_loss: 3426.3708\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 48170.9375 - regression_loss: 12944.0742 - val_loss: 6852.7554 - val_regression_loss: 3408.3582\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 45438.7461 - regression_loss: 12818.6953 - val_loss: 6803.1982 - val_regression_loss: 3383.6970\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 46218.3750 - regression_loss: 12690.0547 - val_loss: 6740.8325 - val_regression_loss: 3352.6152\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 44644.9727 - regression_loss: 12555.7021 - val_loss: 6669.8940 - val_regression_loss: 3317.2278\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 44387.3789 - regression_loss: 12421.9463 - val_loss: 6587.9087 - val_regression_loss: 3276.2947\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 44113.9766 - regression_loss: 12280.7529 - val_loss: 6502.0454 - val_regression_loss: 3233.4065\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 45773.7461 - regression_loss: 12141.0918 - val_loss: 6411.3516 - val_regression_loss: 3188.0774\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 45417.5703 - regression_loss: 12000.0771 - val_loss: 6320.0874 - val_regression_loss: 3142.4448\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 43850.4258 - regression_loss: 11854.7168 - val_loss: 6231.0078 - val_regression_loss: 3097.8857\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 43471.1445 - regression_loss: 11712.0127 - val_loss: 6146.2769 - val_regression_loss: 3055.4880\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 43611.8711 - regression_loss: 11569.6035 - val_loss: 6064.4790 - val_regression_loss: 3014.5427\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 42585.6797 - regression_loss: 11423.3066 - val_loss: 5987.7271 - val_regression_loss: 2976.1189\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 41379.5195 - regression_loss: 11273.1973 - val_loss: 5914.8579 - val_regression_loss: 2939.6372\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 41930.3320 - regression_loss: 11130.1934 - val_loss: 5844.7935 - val_regression_loss: 2904.5557\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 40396.2148 - regression_loss: 10980.3164 - val_loss: 5780.0093 - val_regression_loss: 2872.1211\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 40500.5195 - regression_loss: 10832.6348 - val_loss: 5714.3813 - val_regression_loss: 2839.2722\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 39659.8672 - regression_loss: 10680.3965 - val_loss: 5650.4614 - val_regression_loss: 2807.2852\n",
            "***************************** elapsed_time is:  6.228954792022705\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 231ms/step - loss: 19453.9316 - regression_loss: 5212.9399 - val_loss: 1590.1343 - val_regression_loss: 769.3557\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 13048.7109 - regression_loss: 3436.1770 - val_loss: 996.9244 - val_regression_loss: 475.0135\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 8338.7725 - regression_loss: 2134.3027 - val_loss: 735.2830 - val_regression_loss: 347.2464\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 5911.5615 - regression_loss: 1551.2523 - val_loss: 540.5533 - val_regression_loss: 250.7102\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 4276.7534 - regression_loss: 1087.7463 - val_loss: 291.3456 - val_regression_loss: 124.9106\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 2277.4976 - regression_loss: 547.9069 - val_loss: 291.2043 - val_regression_loss: 123.1786\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2428.8506 - regression_loss: 574.0502 - val_loss: 390.3187 - val_regression_loss: 172.4147\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3332.7249 - regression_loss: 813.3588 - val_loss: 343.6018 - val_regression_loss: 150.2769\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 2912.8953 - regression_loss: 708.3711 - val_loss: 249.9787 - val_regression_loss: 105.3737\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2094.6401 - regression_loss: 489.1594 - val_loss: 215.5567 - val_regression_loss: 90.0233\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1738.1802 - regression_loss: 401.3623 - val_loss: 231.1756 - val_regression_loss: 99.2024\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1774.4203 - regression_loss: 422.6988 - val_loss: 252.3240 - val_regression_loss: 110.5031\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1847.4460 - regression_loss: 449.6863 - val_loss: 266.2442 - val_regression_loss: 117.6835\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1885.6371 - regression_loss: 451.8869 - val_loss: 267.5768 - val_regression_loss: 118.3360\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1758.6376 - regression_loss: 423.6334 - val_loss: 252.7720 - val_regression_loss: 110.7903\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1598.3572 - regression_loss: 369.3283 - val_loss: 241.6257 - val_regression_loss: 104.9151\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 1480.7867 - regression_loss: 333.0661 - val_loss: 242.3786 - val_regression_loss: 104.8387\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1479.1234 - regression_loss: 333.2381 - val_loss: 243.9557 - val_regression_loss: 105.2273\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1511.0394 - regression_loss: 336.7766 - val_loss: 243.7321 - val_regression_loss: 104.9740\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1467.9260 - regression_loss: 334.1988 - val_loss: 239.8058 - val_regression_loss: 103.1902\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1466.7314 - regression_loss: 327.2052 - val_loss: 229.9823 - val_regression_loss: 98.6847\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1417.6349 - regression_loss: 311.3364 - val_loss: 227.0737 - val_regression_loss: 97.7107\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1399.6388 - regression_loss: 306.7752 - val_loss: 231.4735 - val_regression_loss: 100.2711\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1381.9137 - regression_loss: 308.6679 - val_loss: 236.3575 - val_regression_loss: 102.8502\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1345.3289 - regression_loss: 305.9717 - val_loss: 240.5983 - val_regression_loss: 104.9521\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1342.6737 - regression_loss: 303.9631 - val_loss: 237.2736 - val_regression_loss: 103.2200\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1344.4813 - regression_loss: 297.8317 - val_loss: 229.1610 - val_regression_loss: 99.0581\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1303.0967 - regression_loss: 291.7916 - val_loss: 222.8802 - val_regression_loss: 95.7410\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1294.0714 - regression_loss: 289.5350 - val_loss: 218.4807 - val_regression_loss: 93.3727\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1312.2185 - regression_loss: 287.5275 - val_loss: 215.2259 - val_regression_loss: 91.6578\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1286.5541 - regression_loss: 286.6136 - val_loss: 210.9014 - val_regression_loss: 89.5552\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1288.5555 - regression_loss: 284.4206 - val_loss: 209.3977 - val_regression_loss: 88.9674\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1259.1100 - regression_loss: 283.2281 - val_loss: 208.3459 - val_regression_loss: 88.5384\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 1264.2659 - regression_loss: 282.7286 - val_loss: 207.9827 - val_regression_loss: 88.3747\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1252.0645 - regression_loss: 281.7632 - val_loss: 206.4855 - val_regression_loss: 87.6183\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1257.8314 - regression_loss: 281.2874 - val_loss: 204.7063 - val_regression_loss: 86.7346\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1234.6121 - regression_loss: 279.0193 - val_loss: 202.6149 - val_regression_loss: 85.6334\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1270.0387 - regression_loss: 277.3894 - val_loss: 200.3416 - val_regression_loss: 84.4206\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1235.5480 - regression_loss: 276.1302 - val_loss: 198.1810 - val_regression_loss: 83.2749\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1255.3712 - regression_loss: 275.2623 - val_loss: 195.7793 - val_regression_loss: 82.0994\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1225.8909 - regression_loss: 274.3118 - val_loss: 192.9177 - val_regression_loss: 80.6898\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1249.4960 - regression_loss: 273.5647 - val_loss: 190.6301 - val_regression_loss: 79.5346\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1240.0826 - regression_loss: 272.4849 - val_loss: 188.7868 - val_regression_loss: 78.6030\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1221.9077 - regression_loss: 271.9279 - val_loss: 187.0984 - val_regression_loss: 77.7716\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1218.6154 - regression_loss: 271.1758 - val_loss: 185.8111 - val_regression_loss: 77.1313\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1228.1040 - regression_loss: 270.6688 - val_loss: 184.9559 - val_regression_loss: 76.6875\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1225.5220 - regression_loss: 269.7325 - val_loss: 183.8794 - val_regression_loss: 76.0789\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1215.1298 - regression_loss: 268.8326 - val_loss: 182.7916 - val_regression_loss: 75.4882\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1208.7986 - regression_loss: 268.1966 - val_loss: 181.4521 - val_regression_loss: 74.8355\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1188.3970 - regression_loss: 267.5668 - val_loss: 181.1897 - val_regression_loss: 74.7337\n",
            "***************************** elapsed_time is:  6.256590127944946\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 218ms/step - loss: 31835.1543 - regression_loss: 8500.7725 - val_loss: 2523.8303 - val_regression_loss: 1234.4297\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 22406.5586 - regression_loss: 6011.1001 - val_loss: 1661.4624 - val_regression_loss: 809.5310\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 14874.8809 - regression_loss: 3958.3391 - val_loss: 980.4637 - val_regression_loss: 476.0704\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 8810.9355 - regression_loss: 2335.9749 - val_loss: 893.9358 - val_regression_loss: 438.9430\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 7527.7925 - regression_loss: 2014.4883 - val_loss: 759.8162 - val_regression_loss: 370.6649\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 6488.5630 - regression_loss: 1708.5179 - val_loss: 426.0829 - val_regression_loss: 198.0950\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 3905.5950 - regression_loss: 1003.7895 - val_loss: 342.8306 - val_regression_loss: 150.3482\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 3337.9771 - regression_loss: 833.4010 - val_loss: 471.8009 - val_regression_loss: 211.2624\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 4273.7705 - regression_loss: 1102.3619 - val_loss: 505.2914 - val_regression_loss: 227.7827\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 4653.9829 - regression_loss: 1154.2250 - val_loss: 406.7736 - val_regression_loss: 180.4817\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 3772.9407 - regression_loss: 920.5073 - val_loss: 315.7230 - val_regression_loss: 137.7718\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2830.9624 - regression_loss: 697.0029 - val_loss: 308.1429 - val_regression_loss: 136.7770\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 2676.2629 - regression_loss: 659.0042 - val_loss: 341.1754 - val_regression_loss: 155.4323\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 2852.4734 - regression_loss: 713.9254 - val_loss: 342.5437 - val_regression_loss: 157.1944\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 2725.0608 - regression_loss: 712.8906 - val_loss: 315.6696 - val_regression_loss: 143.8240\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2691.4043 - regression_loss: 663.0258 - val_loss: 290.0003 - val_regression_loss: 130.3994\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2472.0046 - regression_loss: 624.9949 - val_loss: 268.6417 - val_regression_loss: 118.9056\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 2329.3708 - regression_loss: 594.6912 - val_loss: 244.4489 - val_regression_loss: 105.9788\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2253.0989 - regression_loss: 546.6159 - val_loss: 228.3052 - val_regression_loss: 97.1320\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 2088.2715 - regression_loss: 503.5934 - val_loss: 231.7399 - val_regression_loss: 98.1563\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2042.3981 - regression_loss: 495.2631 - val_loss: 242.1709 - val_regression_loss: 103.0487\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 2056.0889 - regression_loss: 496.8839 - val_loss: 244.0734 - val_regression_loss: 104.1581\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2039.1178 - regression_loss: 477.5166 - val_loss: 243.4245 - val_regression_loss: 104.2990\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1838.0121 - regression_loss: 454.8543 - val_loss: 248.6019 - val_regression_loss: 107.4446\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1880.7096 - regression_loss: 445.0802 - val_loss: 255.4942 - val_regression_loss: 111.4854\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1876.6622 - regression_loss: 438.3195 - val_loss: 260.8481 - val_regression_loss: 114.6811\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 1822.4769 - regression_loss: 430.0052 - val_loss: 263.6149 - val_regression_loss: 116.3707\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1812.5067 - regression_loss: 424.7692 - val_loss: 260.3930 - val_regression_loss: 114.7784\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1752.5161 - regression_loss: 416.3329 - val_loss: 247.6648 - val_regression_loss: 108.1548\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1699.8066 - regression_loss: 401.1002 - val_loss: 235.2785 - val_regression_loss: 101.5632\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1665.4619 - regression_loss: 389.9359 - val_loss: 227.7082 - val_regression_loss: 97.4642\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1658.4098 - regression_loss: 382.5475 - val_loss: 224.5246 - val_regression_loss: 95.7385\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 1630.6898 - regression_loss: 373.0005 - val_loss: 224.7786 - val_regression_loss: 95.8874\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1605.0248 - regression_loss: 365.3553 - val_loss: 226.0371 - val_regression_loss: 96.6867\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1543.2380 - regression_loss: 359.0813 - val_loss: 224.4599 - val_regression_loss: 96.0522\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1536.4844 - regression_loss: 352.1438 - val_loss: 222.4636 - val_regression_loss: 95.2211\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1513.1422 - regression_loss: 346.2408 - val_loss: 220.5637 - val_regression_loss: 94.3755\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 1501.9907 - regression_loss: 340.9763 - val_loss: 219.0395 - val_regression_loss: 93.6756\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1481.5508 - regression_loss: 335.2834 - val_loss: 217.8888 - val_regression_loss: 93.1170\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1431.0643 - regression_loss: 329.1104 - val_loss: 215.4225 - val_regression_loss: 91.8199\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 1403.8942 - regression_loss: 323.2603 - val_loss: 211.8134 - val_regression_loss: 89.9092\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1399.9420 - regression_loss: 317.5797 - val_loss: 208.5175 - val_regression_loss: 88.2072\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1348.7861 - regression_loss: 312.6689 - val_loss: 206.7963 - val_regression_loss: 87.3635\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1344.7572 - regression_loss: 308.6380 - val_loss: 207.7133 - val_regression_loss: 87.9453\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1326.8192 - regression_loss: 304.9074 - val_loss: 209.1100 - val_regression_loss: 88.7443\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1336.3428 - regression_loss: 301.9755 - val_loss: 209.7728 - val_regression_loss: 89.1461\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1296.0990 - regression_loss: 299.1328 - val_loss: 209.7866 - val_regression_loss: 89.1724\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1314.7528 - regression_loss: 296.5997 - val_loss: 210.4279 - val_regression_loss: 89.4583\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1301.8892 - regression_loss: 293.7115 - val_loss: 211.3448 - val_regression_loss: 89.9151\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 1289.9978 - regression_loss: 291.2437 - val_loss: 211.3222 - val_regression_loss: 89.8592\n",
            "***************************** elapsed_time is:  6.2251362800598145\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 176ms/step - loss: 48450.4727 - regression_loss: 13363.2549 - val_loss: 4855.7266 - val_regression_loss: 2413.2129\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 38413.9141 - regression_loss: 10469.2129 - val_loss: 3569.1072 - val_regression_loss: 1767.4213\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 28900.7480 - regression_loss: 7728.0063 - val_loss: 2202.0637 - val_regression_loss: 1079.8396\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 18627.0312 - regression_loss: 4907.4902 - val_loss: 1230.6152 - val_regression_loss: 588.7338\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 11779.0449 - regression_loss: 3077.4736 - val_loss: 1027.5652 - val_regression_loss: 484.3648\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 11138.6201 - regression_loss: 2879.3218 - val_loss: 853.2685 - val_regression_loss: 400.8383\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 9561.0557 - regression_loss: 2473.2788 - val_loss: 667.1058 - val_regression_loss: 313.7635\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 7245.5454 - regression_loss: 1910.9469 - val_loss: 759.4046 - val_regression_loss: 364.8152\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 6942.9150 - regression_loss: 1858.0208 - val_loss: 968.8311 - val_regression_loss: 471.7404\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 8117.1406 - regression_loss: 2113.6951 - val_loss: 924.8904 - val_regression_loss: 449.4951\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 7439.8057 - regression_loss: 1976.5403 - val_loss: 684.5424 - val_regression_loss: 327.6324\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 5929.4927 - regression_loss: 1543.0139 - val_loss: 489.7091 - val_regression_loss: 227.9947\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 4869.5098 - regression_loss: 1268.1831 - val_loss: 449.4645 - val_regression_loss: 205.9230\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 5134.2686 - regression_loss: 1318.5529 - val_loss: 462.4944 - val_regression_loss: 211.4221\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 5347.4146 - regression_loss: 1402.1738 - val_loss: 431.8970 - val_regression_loss: 196.2721\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 5026.4541 - regression_loss: 1300.4728 - val_loss: 407.8636 - val_regression_loss: 185.2103\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 4087.8625 - regression_loss: 1176.3483 - val_loss: 429.1106 - val_regression_loss: 197.0853\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 4464.8027 - regression_loss: 1127.1807 - val_loss: 440.7843 - val_regression_loss: 203.9437\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 4296.4160 - regression_loss: 1094.7644 - val_loss: 409.8149 - val_regression_loss: 189.1108\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 3941.9385 - regression_loss: 1007.0902 - val_loss: 370.6714 - val_regression_loss: 169.8079\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 3692.7781 - regression_loss: 929.5170 - val_loss: 361.6927 - val_regression_loss: 165.2934\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 3554.8423 - regression_loss: 910.3495 - val_loss: 361.8596 - val_regression_loss: 165.1743\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3455.8730 - regression_loss: 886.4628 - val_loss: 347.4307 - val_regression_loss: 157.7144\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 3256.8850 - regression_loss: 814.8141 - val_loss: 336.5546 - val_regression_loss: 152.0369\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3017.6050 - regression_loss: 753.4577 - val_loss: 331.6035 - val_regression_loss: 149.2590\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2894.6404 - regression_loss: 726.1558 - val_loss: 315.0879 - val_regression_loss: 140.5484\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 2811.3386 - regression_loss: 684.4494 - val_loss: 293.8161 - val_regression_loss: 129.4332\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2557.4844 - regression_loss: 637.1594 - val_loss: 285.3336 - val_regression_loss: 124.8794\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 2505.7886 - regression_loss: 610.3519 - val_loss: 282.5279 - val_regression_loss: 123.5360\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 2394.3008 - regression_loss: 583.1821 - val_loss: 274.6371 - val_regression_loss: 120.0372\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 2078.4917 - regression_loss: 545.2153 - val_loss: 275.3200 - val_regression_loss: 120.7679\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 2177.8359 - regression_loss: 529.1038 - val_loss: 281.7009 - val_regression_loss: 123.9351\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 2101.7908 - regression_loss: 505.3867 - val_loss: 293.8691 - val_regression_loss: 129.6964\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 2023.3093 - regression_loss: 483.5692 - val_loss: 302.0740 - val_regression_loss: 133.4893\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1940.1621 - regression_loss: 468.3610 - val_loss: 290.9621 - val_regression_loss: 127.8114\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1884.9241 - regression_loss: 447.2484 - val_loss: 276.8422 - val_regression_loss: 120.6475\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 1819.2190 - regression_loss: 433.3368 - val_loss: 270.9352 - val_regression_loss: 117.4644\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1815.6021 - regression_loss: 421.4618 - val_loss: 277.9760 - val_regression_loss: 120.5892\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1733.7992 - regression_loss: 407.8954 - val_loss: 284.3333 - val_regression_loss: 123.5430\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1702.8120 - regression_loss: 399.7478 - val_loss: 274.4710 - val_regression_loss: 118.7585\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1652.7263 - regression_loss: 390.4713 - val_loss: 266.9060 - val_regression_loss: 115.1702\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1674.9470 - regression_loss: 385.0332 - val_loss: 268.2198 - val_regression_loss: 115.8275\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1607.2966 - regression_loss: 377.3701 - val_loss: 271.1490 - val_regression_loss: 117.2397\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1598.8572 - regression_loss: 371.7848 - val_loss: 262.7913 - val_regression_loss: 113.1538\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1598.3855 - regression_loss: 365.7101 - val_loss: 251.9511 - val_regression_loss: 107.8770\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1571.9463 - regression_loss: 360.4371 - val_loss: 249.2712 - val_regression_loss: 106.5102\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1547.3563 - regression_loss: 354.1645 - val_loss: 251.2230 - val_regression_loss: 107.3601\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1526.8828 - regression_loss: 348.0125 - val_loss: 245.4157 - val_regression_loss: 104.4810\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1509.0027 - regression_loss: 342.3511 - val_loss: 234.2478 - val_regression_loss: 99.0344\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 1460.5364 - regression_loss: 338.3165 - val_loss: 225.8711 - val_regression_loss: 94.9413\n",
            "***************************** elapsed_time is:  6.16556453704834\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 171ms/step - loss: 61015.9062 - regression_loss: 16525.6289 - val_loss: 6293.2983 - val_regression_loss: 3130.0481\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 49262.7656 - regression_loss: 13282.8770 - val_loss: 4995.2236 - val_regression_loss: 2480.2241\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 37306.6328 - regression_loss: 10114.4736 - val_loss: 3514.9021 - val_regression_loss: 1738.6639\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 24385.2012 - regression_loss: 6505.0137 - val_loss: 2276.0178 - val_regression_loss: 1117.3169\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 13134.4805 - regression_loss: 3428.9441 - val_loss: 2130.8220 - val_regression_loss: 1043.1392\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 10313.9902 - regression_loss: 2735.6755 - val_loss: 2507.8306 - val_regression_loss: 1232.9510\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 13219.6719 - regression_loss: 3508.2700 - val_loss: 2098.7581 - val_regression_loss: 1031.2557\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 10871.2861 - regression_loss: 2869.2224 - val_loss: 1494.2098 - val_regression_loss: 730.9858\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 7096.9668 - regression_loss: 1880.3994 - val_loss: 1268.2748 - val_regression_loss: 618.9915\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 6283.7266 - regression_loss: 1647.9332 - val_loss: 1302.8374 - val_regression_loss: 636.3551\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 6954.2441 - regression_loss: 1860.4619 - val_loss: 1277.5906 - val_regression_loss: 623.0320\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 6966.8037 - regression_loss: 1826.6298 - val_loss: 1130.5624 - val_regression_loss: 548.3050\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 5495.9297 - regression_loss: 1440.2910 - val_loss: 1004.7333 - val_regression_loss: 484.0185\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 3824.6946 - regression_loss: 1066.1890 - val_loss: 1009.5748 - val_regression_loss: 485.3493\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 3879.1267 - regression_loss: 996.5254 - val_loss: 1069.7603 - val_regression_loss: 515.0547\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 4329.4351 - regression_loss: 1094.4259 - val_loss: 1024.9093 - val_regression_loss: 493.0510\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 4045.7622 - regression_loss: 1030.7982 - val_loss: 898.6243 - val_regression_loss: 430.8942\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 3322.7212 - regression_loss: 821.4515 - val_loss: 838.3426 - val_regression_loss: 401.9632\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3057.1289 - regression_loss: 756.2514 - val_loss: 853.2416 - val_regression_loss: 410.4405\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 3070.9556 - regression_loss: 822.8605 - val_loss: 837.9742 - val_regression_loss: 403.3578\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3171.3018 - regression_loss: 798.0071 - val_loss: 793.1354 - val_regression_loss: 380.9911\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2791.5066 - regression_loss: 699.6148 - val_loss: 785.4109 - val_regression_loss: 376.8328\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 2687.2627 - regression_loss: 670.2373 - val_loss: 792.3184 - val_regression_loss: 379.7531\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2724.3652 - regression_loss: 673.0495 - val_loss: 772.0143 - val_regression_loss: 368.9803\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 2517.7585 - regression_loss: 623.7355 - val_loss: 741.0045 - val_regression_loss: 352.9890\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 2309.8438 - regression_loss: 567.8961 - val_loss: 724.0317 - val_regression_loss: 344.1865\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2170.7585 - regression_loss: 556.8087 - val_loss: 710.1719 - val_regression_loss: 336.9854\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2203.1355 - regression_loss: 554.4727 - val_loss: 688.2130 - val_regression_loss: 325.6930\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 2133.5908 - regression_loss: 525.0967 - val_loss: 671.8289 - val_regression_loss: 317.2685\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2095.7280 - regression_loss: 499.1641 - val_loss: 665.8189 - val_regression_loss: 314.2119\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2073.1677 - regression_loss: 490.8903 - val_loss: 656.3411 - val_regression_loss: 309.6826\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 2034.2765 - regression_loss: 480.5300 - val_loss: 640.4103 - val_regression_loss: 302.0695\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1938.3744 - regression_loss: 462.1498 - val_loss: 627.6439 - val_regression_loss: 295.9786\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1872.7677 - regression_loss: 453.3913 - val_loss: 616.9277 - val_regression_loss: 290.7070\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1775.4767 - regression_loss: 446.1190 - val_loss: 604.0013 - val_regression_loss: 284.0957\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1824.0913 - regression_loss: 432.6082 - val_loss: 592.4780 - val_regression_loss: 278.0405\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1793.2885 - regression_loss: 419.7487 - val_loss: 584.4635 - val_regression_loss: 273.8297\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1764.6234 - regression_loss: 413.8589 - val_loss: 575.5208 - val_regression_loss: 269.4056\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1577.6766 - regression_loss: 404.6429 - val_loss: 567.4478 - val_regression_loss: 265.5722\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1713.0138 - regression_loss: 396.3303 - val_loss: 560.7198 - val_regression_loss: 262.3872\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1648.2338 - regression_loss: 390.5058 - val_loss: 552.2883 - val_regression_loss: 258.2339\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1541.8359 - regression_loss: 382.6169 - val_loss: 543.3136 - val_regression_loss: 253.7434\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1602.6216 - regression_loss: 375.4061 - val_loss: 533.7480 - val_regression_loss: 248.9818\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1588.6663 - regression_loss: 370.2924 - val_loss: 522.6444 - val_regression_loss: 243.4897\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1573.2111 - regression_loss: 364.2537 - val_loss: 511.8332 - val_regression_loss: 238.1929\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1544.5874 - regression_loss: 358.4408 - val_loss: 502.4376 - val_regression_loss: 233.5330\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 1468.3138 - regression_loss: 353.2389 - val_loss: 495.3887 - val_regression_loss: 229.9718\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1483.3707 - regression_loss: 347.3295 - val_loss: 488.5316 - val_regression_loss: 226.4953\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1498.4338 - regression_loss: 341.9884 - val_loss: 481.2683 - val_regression_loss: 222.8672\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1465.9335 - regression_loss: 337.5037 - val_loss: 473.9711 - val_regression_loss: 219.3482\n",
            "***************************** elapsed_time is:  5.964436292648315\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 205ms/step - loss: 102670.2188 - regression_loss: 27960.8789 - val_loss: 10606.3994 - val_regression_loss: 5296.5718\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 89937.4922 - regression_loss: 24180.1289 - val_loss: 8825.9326 - val_regression_loss: 4404.6011\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 73648.5312 - regression_loss: 20114.9785 - val_loss: 6527.4360 - val_regression_loss: 3251.5063\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 55777.6367 - regression_loss: 14976.5215 - val_loss: 4069.3296 - val_regression_loss: 2014.8577\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 35187.1836 - regression_loss: 9489.8301 - val_loss: 2408.4568 - val_regression_loss: 1170.7683\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 22855.2930 - regression_loss: 6008.4038 - val_loss: 2471.9846 - val_regression_loss: 1186.5240\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 22934.7402 - regression_loss: 6291.2632 - val_loss: 2597.1926 - val_regression_loss: 1248.8794\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 26293.8262 - regression_loss: 6936.3052 - val_loss: 1940.3536 - val_regression_loss: 931.5453\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 21663.5430 - regression_loss: 5730.9741 - val_loss: 1436.1067 - val_regression_loss: 690.9798\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 16189.9873 - regression_loss: 4582.6484 - val_loss: 1519.2338 - val_regression_loss: 740.2447\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 16546.6250 - regression_loss: 4486.1699 - val_loss: 1782.3206 - val_regression_loss: 875.2833\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 17308.8438 - regression_loss: 4777.0073 - val_loss: 1795.8597 - val_regression_loss: 882.4265\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 17724.7031 - regression_loss: 4665.0576 - val_loss: 1545.2743 - val_regression_loss: 755.5610\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 15403.5293 - regression_loss: 4122.2002 - val_loss: 1240.8361 - val_regression_loss: 600.8344\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 13552.0928 - regression_loss: 3586.0781 - val_loss: 1063.2225 - val_regression_loss: 509.5287\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 12537.7744 - regression_loss: 3371.4016 - val_loss: 1014.4380 - val_regression_loss: 483.6013\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 13091.9033 - regression_loss: 3400.4893 - val_loss: 969.9280 - val_regression_loss: 461.3965\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 12219.1123 - regression_loss: 3312.4067 - val_loss: 885.0068 - val_regression_loss: 420.5916\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 11385.8887 - regression_loss: 2972.8091 - val_loss: 861.0757 - val_regression_loss: 411.1929\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 10157.6152 - regression_loss: 2646.2808 - val_loss: 947.9758 - val_regression_loss: 457.1304\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 9757.6396 - regression_loss: 2541.0994 - val_loss: 1005.1450 - val_regression_loss: 487.1380\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 9447.8955 - regression_loss: 2505.9749 - val_loss: 916.3712 - val_regression_loss: 442.8022\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 8122.2773 - regression_loss: 2295.2285 - val_loss: 752.6917 - val_regression_loss: 359.9525\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 7847.3081 - regression_loss: 2059.0911 - val_loss: 652.5465 - val_regression_loss: 308.6848\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 7580.5488 - regression_loss: 1951.6189 - val_loss: 594.7495 - val_regression_loss: 279.4499\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 7161.5796 - regression_loss: 1853.1378 - val_loss: 539.7240 - val_regression_loss: 252.6492\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 5988.7354 - regression_loss: 1678.5503 - val_loss: 532.2538 - val_regression_loss: 250.0197\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 5761.0005 - regression_loss: 1576.1711 - val_loss: 527.7288 - val_regression_loss: 248.0912\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 5895.1636 - regression_loss: 1524.0707 - val_loss: 475.2805 - val_regression_loss: 221.1522\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 5327.4507 - regression_loss: 1388.9409 - val_loss: 443.9817 - val_regression_loss: 204.4685\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 5078.0049 - regression_loss: 1303.9581 - val_loss: 433.1991 - val_regression_loss: 198.7742\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 4916.0771 - regression_loss: 1243.7733 - val_loss: 395.8463 - val_regression_loss: 180.8698\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 4537.4062 - regression_loss: 1138.9196 - val_loss: 368.1253 - val_regression_loss: 167.8860\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 4196.2163 - regression_loss: 1072.7163 - val_loss: 339.8564 - val_regression_loss: 153.7306\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 3898.7112 - regression_loss: 999.0300 - val_loss: 315.8694 - val_regression_loss: 141.0075\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3394.7512 - regression_loss: 909.9012 - val_loss: 309.4650 - val_regression_loss: 137.1305\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 3288.3440 - regression_loss: 857.0708 - val_loss: 296.5713 - val_regression_loss: 130.4692\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 3170.6152 - regression_loss: 796.0329 - val_loss: 285.4826 - val_regression_loss: 124.8907\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3037.0874 - regression_loss: 747.1368 - val_loss: 284.9696 - val_regression_loss: 124.3809\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2852.4626 - regression_loss: 708.9619 - val_loss: 293.6346 - val_regression_loss: 128.2097\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 2588.5513 - regression_loss: 670.2545 - val_loss: 305.6319 - val_regression_loss: 133.8400\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2604.4858 - regression_loss: 644.5621 - val_loss: 298.0123 - val_regression_loss: 130.1948\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2491.2822 - regression_loss: 618.7031 - val_loss: 289.4949 - val_regression_loss: 126.1668\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 2444.6528 - regression_loss: 602.0568 - val_loss: 290.9290 - val_regression_loss: 126.8842\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2256.3047 - regression_loss: 585.9362 - val_loss: 294.1267 - val_regression_loss: 128.4108\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 2334.3301 - regression_loss: 572.2098 - val_loss: 306.1075 - val_regression_loss: 134.1352\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2263.1782 - regression_loss: 559.5459 - val_loss: 297.7612 - val_regression_loss: 130.0799\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 2252.6655 - regression_loss: 545.0610 - val_loss: 294.8844 - val_regression_loss: 128.6888\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2195.3674 - regression_loss: 533.5844 - val_loss: 296.7207 - val_regression_loss: 129.5348\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2143.9302 - regression_loss: 520.2799 - val_loss: 294.8875 - val_regression_loss: 128.6625\n",
            "***************************** elapsed_time is:  6.0964086055755615\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 179ms/step - loss: 28325.0371 - regression_loss: 7673.3467 - val_loss: 2595.7756 - val_regression_loss: 1278.5979\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 20203.0957 - regression_loss: 5352.0610 - val_loss: 1691.5454 - val_regression_loss: 824.4687\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 13133.6865 - regression_loss: 3432.1633 - val_loss: 994.4854 - val_regression_loss: 473.3331\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 7852.5151 - regression_loss: 2052.6221 - val_loss: 808.6190 - val_regression_loss: 379.1782\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 6857.2705 - regression_loss: 1776.1482 - val_loss: 587.8915 - val_regression_loss: 271.5106\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 4765.7754 - regression_loss: 1207.3282 - val_loss: 430.4204 - val_regression_loss: 196.7116\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2832.0874 - regression_loss: 695.1188 - val_loss: 567.0797 - val_regression_loss: 268.2700\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 3359.3586 - regression_loss: 846.2281 - val_loss: 722.5628 - val_regression_loss: 347.4044\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 4347.2637 - regression_loss: 1107.3947 - val_loss: 663.6646 - val_regression_loss: 317.7924\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3847.1150 - regression_loss: 988.0109 - val_loss: 510.1662 - val_regression_loss: 240.0822\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2848.5491 - regression_loss: 718.7651 - val_loss: 401.2646 - val_regression_loss: 184.3740\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 2285.7454 - regression_loss: 559.5442 - val_loss: 367.2851 - val_regression_loss: 166.2092\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 2314.0552 - regression_loss: 565.7448 - val_loss: 360.3075 - val_regression_loss: 161.9106\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 2495.9097 - regression_loss: 608.5674 - val_loss: 346.0493 - val_regression_loss: 154.5267\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2536.5208 - regression_loss: 604.2061 - val_loss: 334.7943 - val_regression_loss: 149.1376\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 2426.8262 - regression_loss: 572.0674 - val_loss: 332.0213 - val_regression_loss: 148.2857\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2227.7256 - regression_loss: 537.8759 - val_loss: 329.1423 - val_regression_loss: 147.5101\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2095.5586 - regression_loss: 494.3362 - val_loss: 325.5955 - val_regression_loss: 146.4210\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1970.1152 - regression_loss: 463.4640 - val_loss: 329.2525 - val_regression_loss: 148.8475\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1964.9481 - regression_loss: 464.7509 - val_loss: 333.3493 - val_regression_loss: 151.2601\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1969.8945 - regression_loss: 474.3613 - val_loss: 327.8889 - val_regression_loss: 148.5882\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1948.0625 - regression_loss: 460.1543 - val_loss: 319.1108 - val_regression_loss: 144.0180\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1882.9131 - regression_loss: 440.1020 - val_loss: 309.4475 - val_regression_loss: 138.8867\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 1706.7242 - regression_loss: 424.5082 - val_loss: 295.2021 - val_regression_loss: 131.4481\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1683.1853 - regression_loss: 408.4175 - val_loss: 284.2795 - val_regression_loss: 125.7099\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1724.4836 - regression_loss: 400.8631 - val_loss: 278.8357 - val_regression_loss: 122.8390\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1640.5614 - regression_loss: 399.1005 - val_loss: 278.5654 - val_regression_loss: 122.7178\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1659.9705 - regression_loss: 392.0840 - val_loss: 278.9114 - val_regression_loss: 123.0472\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 1613.4243 - regression_loss: 383.9992 - val_loss: 282.8954 - val_regression_loss: 125.2548\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1598.3119 - regression_loss: 380.5628 - val_loss: 280.4865 - val_regression_loss: 124.2367\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1529.8269 - regression_loss: 375.5502 - val_loss: 272.5927 - val_regression_loss: 120.3930\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1617.5602 - regression_loss: 372.2358 - val_loss: 266.5789 - val_regression_loss: 117.4340\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1554.1047 - regression_loss: 369.7504 - val_loss: 266.9279 - val_regression_loss: 117.6308\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1575.2831 - regression_loss: 363.7925 - val_loss: 266.2330 - val_regression_loss: 117.2393\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1547.8790 - regression_loss: 359.9664 - val_loss: 257.4577 - val_regression_loss: 112.7415\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1519.2534 - regression_loss: 355.0241 - val_loss: 247.3856 - val_regression_loss: 107.5852\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1514.3823 - regression_loss: 350.9450 - val_loss: 242.4479 - val_regression_loss: 105.0684\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1522.2163 - regression_loss: 347.7035 - val_loss: 240.8036 - val_regression_loss: 104.2665\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1500.4557 - regression_loss: 343.9379 - val_loss: 239.7301 - val_regression_loss: 103.7987\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 1496.7299 - regression_loss: 341.1021 - val_loss: 234.8921 - val_regression_loss: 101.4207\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1464.3585 - regression_loss: 337.9860 - val_loss: 231.2104 - val_regression_loss: 99.6129\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1479.4663 - regression_loss: 335.1820 - val_loss: 227.8604 - val_regression_loss: 97.9534\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1425.7953 - regression_loss: 332.6521 - val_loss: 225.4327 - val_regression_loss: 96.7368\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1428.3304 - regression_loss: 329.9438 - val_loss: 217.6658 - val_regression_loss: 92.8080\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1410.6409 - regression_loss: 326.9087 - val_loss: 213.6075 - val_regression_loss: 90.7331\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1424.9756 - regression_loss: 323.5750 - val_loss: 214.2206 - val_regression_loss: 91.0218\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1394.5326 - regression_loss: 320.9666 - val_loss: 210.9576 - val_regression_loss: 89.3644\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1393.0588 - regression_loss: 319.0775 - val_loss: 202.5204 - val_regression_loss: 85.1080\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1361.9702 - regression_loss: 315.7155 - val_loss: 201.3923 - val_regression_loss: 84.5661\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 1352.4314 - regression_loss: 313.3467 - val_loss: 201.3292 - val_regression_loss: 84.5399\n",
            "***************************** elapsed_time is:  6.3262176513671875\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 223ms/step - loss: 18268.8945 - regression_loss: 4963.9189 - val_loss: 1448.3026 - val_regression_loss: 698.5082\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 12233.6465 - regression_loss: 3227.7847 - val_loss: 838.0989 - val_regression_loss: 394.0469\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 6856.6655 - regression_loss: 1770.3352 - val_loss: 452.8370 - val_regression_loss: 202.3799\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3699.1262 - regression_loss: 897.5069 - val_loss: 440.9401 - val_regression_loss: 197.1845\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3726.2712 - regression_loss: 908.1626 - val_loss: 366.3532 - val_regression_loss: 159.9572\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 3232.0793 - regression_loss: 798.1326 - val_loss: 274.0626 - val_regression_loss: 113.9706\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 2611.1602 - regression_loss: 617.8741 - val_loss: 293.3878 - val_regression_loss: 124.3288\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2735.4341 - regression_loss: 648.9443 - val_loss: 297.1022 - val_regression_loss: 127.4527\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2658.4636 - regression_loss: 642.3295 - val_loss: 249.7672 - val_regression_loss: 105.1788\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2211.1826 - regression_loss: 526.1160 - val_loss: 209.1565 - val_regression_loss: 86.1375\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1809.1394 - regression_loss: 424.7965 - val_loss: 216.0368 - val_regression_loss: 90.5641\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1846.8230 - regression_loss: 434.2570 - val_loss: 242.2919 - val_regression_loss: 104.2910\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2023.7831 - regression_loss: 484.1612 - val_loss: 233.8759 - val_regression_loss: 100.2829\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1950.4504 - regression_loss: 462.7899 - val_loss: 200.7184 - val_regression_loss: 83.6420\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1691.5404 - regression_loss: 395.8968 - val_loss: 183.0473 - val_regression_loss: 74.6443\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1553.6951 - regression_loss: 364.1262 - val_loss: 186.5335 - val_regression_loss: 76.2422\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 1620.2075 - regression_loss: 377.4771 - val_loss: 190.0369 - val_regression_loss: 77.9170\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1638.2456 - regression_loss: 385.7934 - val_loss: 184.0902 - val_regression_loss: 74.9484\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1621.3856 - regression_loss: 371.7347 - val_loss: 176.5963 - val_regression_loss: 71.3122\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1539.7764 - regression_loss: 349.4520 - val_loss: 176.4488 - val_regression_loss: 71.4355\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1484.7859 - regression_loss: 341.0634 - val_loss: 179.7530 - val_regression_loss: 73.3273\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1470.9315 - regression_loss: 338.5580 - val_loss: 182.1183 - val_regression_loss: 74.7110\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1444.5457 - regression_loss: 332.9329 - val_loss: 184.0161 - val_regression_loss: 75.7753\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 1439.6119 - regression_loss: 328.6920 - val_loss: 184.6451 - val_regression_loss: 76.0958\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1446.0367 - regression_loss: 325.4713 - val_loss: 181.5002 - val_regression_loss: 74.4314\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1374.3241 - regression_loss: 317.0021 - val_loss: 177.2250 - val_regression_loss: 72.1531\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1367.2614 - regression_loss: 308.6334 - val_loss: 175.2523 - val_regression_loss: 71.0260\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1344.6849 - regression_loss: 305.4492 - val_loss: 175.5370 - val_regression_loss: 71.0914\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1368.1290 - regression_loss: 304.2031 - val_loss: 175.0007 - val_regression_loss: 70.8413\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1326.3290 - regression_loss: 299.5933 - val_loss: 174.3107 - val_regression_loss: 70.5756\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 1323.8528 - regression_loss: 294.3862 - val_loss: 173.7319 - val_regression_loss: 70.3753\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1289.4103 - regression_loss: 292.1743 - val_loss: 172.8061 - val_regression_loss: 69.9741\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1306.9592 - regression_loss: 290.4436 - val_loss: 171.3879 - val_regression_loss: 69.2710\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1263.8640 - regression_loss: 288.0107 - val_loss: 170.0726 - val_regression_loss: 68.5523\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 1272.8335 - regression_loss: 285.0616 - val_loss: 168.9697 - val_regression_loss: 67.9074\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1240.0739 - regression_loss: 281.6147 - val_loss: 168.5717 - val_regression_loss: 67.6100\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1272.5684 - regression_loss: 279.4922 - val_loss: 168.1620 - val_regression_loss: 67.3462\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1249.2814 - regression_loss: 277.9863 - val_loss: 166.7811 - val_regression_loss: 66.6524\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1216.2930 - regression_loss: 275.9821 - val_loss: 165.8863 - val_regression_loss: 66.2220\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1256.3826 - regression_loss: 274.2708 - val_loss: 165.5919 - val_regression_loss: 66.0841\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1233.0204 - regression_loss: 273.0183 - val_loss: 165.1330 - val_regression_loss: 65.8446\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1233.2173 - regression_loss: 271.8752 - val_loss: 165.3496 - val_regression_loss: 65.9138\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1208.7458 - regression_loss: 270.6561 - val_loss: 165.5849 - val_regression_loss: 65.9801\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 1210.7657 - regression_loss: 268.9179 - val_loss: 165.4283 - val_regression_loss: 65.8645\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1190.6461 - regression_loss: 267.7305 - val_loss: 165.0081 - val_regression_loss: 65.6218\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1200.5848 - regression_loss: 266.7118 - val_loss: 163.8252 - val_regression_loss: 65.0157\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1191.8932 - regression_loss: 265.6187 - val_loss: 164.6261 - val_regression_loss: 65.4011\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 1169.4553 - regression_loss: 264.9651 - val_loss: 166.5898 - val_regression_loss: 66.3822\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 1187.3076 - regression_loss: 263.9963 - val_loss: 167.1491 - val_regression_loss: 66.6400\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1178.8044 - regression_loss: 263.1684 - val_loss: 165.3669 - val_regression_loss: 65.7119\n",
            "***************************** elapsed_time is:  6.029137849807739\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 193ms/step - loss: 3175909.7500 - regression_loss: 871055.6250 - val_loss: 350507.5938 - val_regression_loss: 175141.3906\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3099778.7500 - regression_loss: 843909.8125 - val_loss: 335759.0938 - val_regression_loss: 167791.6875\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3014482.2500 - regression_loss: 810700.0625 - val_loss: 314613.8438 - val_regression_loss: 157248.1719\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 2819414.5000 - regression_loss: 762877.3750 - val_loss: 285581.2188 - val_regression_loss: 142767.3438\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2533471.7500 - regression_loss: 696853.3750 - val_loss: 248539.6406 - val_regression_loss: 124295.5938\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2243495.2500 - regression_loss: 613095.8125 - val_loss: 204862.0000 - val_regression_loss: 102543.7656\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1816583.2500 - regression_loss: 513122.0312 - val_loss: 156764.6406 - val_regression_loss: 78680.6016\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1487338.2500 - regression_loss: 405112.9688 - val_loss: 107976.4844 - val_regression_loss: 54617.3047\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1083658.8750 - regression_loss: 294672.5938 - val_loss: 65534.9219 - val_regression_loss: 33686.7109\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 709964.1250 - regression_loss: 196048.9219 - val_loss: 41522.0078 - val_regression_loss: 21667.7969\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 468867.6875 - regression_loss: 135199.5938 - val_loss: 48170.6836 - val_regression_loss: 24758.8809\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 491136.8750 - regression_loss: 138550.5156 - val_loss: 64030.1445 - val_regression_loss: 32373.3105\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 594771.5625 - regression_loss: 162882.6719 - val_loss: 56730.2695 - val_regression_loss: 28420.5664\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 530885.2500 - regression_loss: 144118.4844 - val_loss: 38223.3477 - val_regression_loss: 18942.6641\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 399085.2812 - regression_loss: 107071.1328 - val_loss: 24707.8613 - val_regression_loss: 12080.8350\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 302906.5625 - regression_loss: 83961.3438 - val_loss: 20099.5527 - val_regression_loss: 9777.6328\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 295924.6562 - regression_loss: 79408.8594 - val_loss: 20915.6543 - val_regression_loss: 10244.6758\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 315464.1875 - regression_loss: 84274.5469 - val_loss: 22840.1797 - val_regression_loss: 11279.2930\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 317929.6875 - regression_loss: 89544.7266 - val_loss: 23315.5273 - val_regression_loss: 11577.2988\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 334654.6250 - regression_loss: 90118.4609 - val_loss: 21711.8438 - val_regression_loss: 10811.3232\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 304589.7812 - regression_loss: 84758.0234 - val_loss: 19021.5078 - val_regression_loss: 9479.7920\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 267955.0312 - regression_loss: 75768.7188 - val_loss: 16788.7441 - val_regression_loss: 8361.9902\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 247343.7188 - regression_loss: 66867.1641 - val_loss: 16521.1152 - val_regression_loss: 8221.8496\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 225897.2500 - regression_loss: 60926.8203 - val_loss: 18652.9512 - val_regression_loss: 9288.9414\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 217843.4688 - regression_loss: 59228.2812 - val_loss: 21868.5586 - val_regression_loss: 10916.9443\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 221806.5156 - regression_loss: 60250.7188 - val_loss: 24000.1543 - val_regression_loss: 12021.6934\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 212540.1250 - regression_loss: 60175.6914 - val_loss: 23723.2324 - val_regression_loss: 11933.3828\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 213180.1094 - regression_loss: 58103.8867 - val_loss: 22160.9902 - val_regression_loss: 11196.1572\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 203326.2656 - regression_loss: 55548.6016 - val_loss: 20366.9336 - val_regression_loss: 10325.1270\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 193390.2656 - regression_loss: 53721.6445 - val_loss: 18922.0879 - val_regression_loss: 9603.5967\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 189342.1562 - regression_loss: 52617.0859 - val_loss: 17685.9492 - val_regression_loss: 8962.6035\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 182890.2656 - regression_loss: 51102.8945 - val_loss: 16669.6484 - val_regression_loss: 8410.9375\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 177370.5000 - regression_loss: 48852.6328 - val_loss: 16103.5869 - val_regression_loss: 8069.8599\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 157666.9219 - regression_loss: 46371.6797 - val_loss: 15897.0576 - val_regression_loss: 7909.1011\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 159488.6094 - regression_loss: 44440.9336 - val_loss: 16038.1748 - val_regression_loss: 7940.1270\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 159408.4844 - regression_loss: 43230.4922 - val_loss: 16041.4248 - val_regression_loss: 7932.5508\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 153869.1406 - regression_loss: 42041.7148 - val_loss: 15685.9814 - val_regression_loss: 7775.5474\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 152471.8281 - regression_loss: 40586.4141 - val_loss: 15013.3447 - val_regression_loss: 7481.3906\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 142613.7500 - regression_loss: 39029.0977 - val_loss: 14354.7969 - val_regression_loss: 7200.4214\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 132088.2969 - regression_loss: 37698.8438 - val_loss: 13867.4531 - val_regression_loss: 6996.7500\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 136968.3281 - regression_loss: 36661.0703 - val_loss: 13652.5049 - val_regression_loss: 6910.3140\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 130887.6406 - regression_loss: 35514.8008 - val_loss: 13599.1270 - val_regression_loss: 6887.4888\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 121159.2500 - regression_loss: 34157.3086 - val_loss: 13612.3848 - val_regression_loss: 6884.3267\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 120955.8047 - regression_loss: 32866.0586 - val_loss: 13759.6396 - val_regression_loss: 6941.6597\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 117480.9531 - regression_loss: 31637.5117 - val_loss: 13730.9365 - val_regression_loss: 6911.7422\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 109508.3984 - regression_loss: 30453.2715 - val_loss: 13316.7100 - val_regression_loss: 6697.5625\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 105546.2734 - regression_loss: 29304.8105 - val_loss: 12712.1914 - val_regression_loss: 6396.0498\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 97536.7969 - regression_loss: 28160.8965 - val_loss: 12186.1582 - val_regression_loss: 6134.0830\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 100847.4844 - regression_loss: 27182.3047 - val_loss: 11858.8438 - val_regression_loss: 5973.0586\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 94823.0781 - regression_loss: 26235.6230 - val_loss: 11746.1025 - val_regression_loss: 5919.5273\n",
            "***************************** elapsed_time is:  5.884876728057861\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 218ms/step - loss: 24730.2734 - regression_loss: 6644.5537 - val_loss: 1757.2177 - val_regression_loss: 854.4725\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 17919.9961 - regression_loss: 4764.2729 - val_loss: 1112.7902 - val_regression_loss: 535.1917\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 11785.4561 - regression_loss: 3090.9141 - val_loss: 606.5490 - val_regression_loss: 285.3185\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 6459.8027 - regression_loss: 1673.7227 - val_loss: 613.3674 - val_regression_loss: 291.5388\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 5470.9204 - regression_loss: 1415.8572 - val_loss: 532.2084 - val_regression_loss: 250.6733\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 4459.1685 - regression_loss: 1137.6935 - val_loss: 304.4305 - val_regression_loss: 134.6969\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2709.7134 - regression_loss: 665.8088 - val_loss: 259.5493 - val_regression_loss: 110.3468\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2710.9080 - regression_loss: 662.0446 - val_loss: 319.6686 - val_regression_loss: 139.8107\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3391.5068 - regression_loss: 841.6219 - val_loss: 286.3698 - val_regression_loss: 123.6943\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 3061.5635 - regression_loss: 744.2589 - val_loss: 201.9907 - val_regression_loss: 82.5085\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2126.0088 - regression_loss: 502.3729 - val_loss: 187.5128 - val_regression_loss: 76.3170\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1682.8622 - regression_loss: 399.8505 - val_loss: 246.1171 - val_regression_loss: 106.4230\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2037.5983 - regression_loss: 487.6216 - val_loss: 264.8877 - val_regression_loss: 116.1899\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 2160.7969 - regression_loss: 525.0444 - val_loss: 224.9367 - val_regression_loss: 96.1432\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1936.9243 - regression_loss: 463.4545 - val_loss: 195.2885 - val_regression_loss: 80.9490\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1822.1951 - regression_loss: 427.5499 - val_loss: 186.1160 - val_regression_loss: 75.8822\n",
            "Epoch 17/50\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 1827.5200 - regression_loss: 791.6523\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "2/2 [==============================] - 0s 218ms/step - loss: 1746.4424 - regression_loss: 413.2975 - val_loss: 180.4205 - val_regression_loss: 72.5448\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 1622.5780 - regression_loss: 381.0902 - val_loss: 181.4639 - val_regression_loss: 72.8267\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1575.0925 - regression_loss: 361.9983 - val_loss: 187.8319 - val_regression_loss: 75.8119\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1548.2047 - regression_loss: 352.2997 - val_loss: 197.3370 - val_regression_loss: 80.4392\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 1554.7273 - regression_loss: 352.8621 - val_loss: 203.2710 - val_regression_loss: 83.3891\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1522.5344 - regression_loss: 354.3113 - val_loss: 201.0454 - val_regression_loss: 82.3757\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 1513.3210 - regression_loss: 348.0198 - val_loss: 192.2902 - val_regression_loss: 78.1593\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1430.3215 - regression_loss: 337.0482 - val_loss: 182.0578 - val_regression_loss: 73.2272\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1416.6877 - regression_loss: 328.2844 - val_loss: 175.1013 - val_regression_loss: 69.9142\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1407.9772 - regression_loss: 324.0640 - val_loss: 172.2895 - val_regression_loss: 68.6282\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1405.3522 - regression_loss: 323.6711 - val_loss: 172.1015 - val_regression_loss: 68.6105\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1423.8163 - regression_loss: 322.0256 - val_loss: 173.9726 - val_regression_loss: 69.5787\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1395.0162 - regression_loss: 317.9400 - val_loss: 177.5456 - val_regression_loss: 71.3668\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1379.3812 - regression_loss: 313.3086 - val_loss: 181.3505 - val_regression_loss: 73.2308\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1377.5829 - regression_loss: 310.4605 - val_loss: 184.2675 - val_regression_loss: 74.6251\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1376.0541 - regression_loss: 307.7183 - val_loss: 184.6850 - val_regression_loss: 74.7579\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1347.8708 - regression_loss: 304.5417 - val_loss: 183.3590 - val_regression_loss: 74.0244\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1349.8326 - regression_loss: 301.3665 - val_loss: 181.7565 - val_regression_loss: 73.1719\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1351.3959 - regression_loss: 298.9654 - val_loss: 180.4771 - val_regression_loss: 72.5085\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1324.5327 - regression_loss: 297.5021 - val_loss: 180.4226 - val_regression_loss: 72.4926\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1308.7462 - regression_loss: 295.6669 - val_loss: 180.6340 - val_regression_loss: 72.6306\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1323.9066 - regression_loss: 293.5708 - val_loss: 181.4758 - val_regression_loss: 73.0994\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1289.0026 - regression_loss: 291.5995 - val_loss: 182.5154 - val_regression_loss: 73.6767\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1301.8987 - regression_loss: 290.3239 - val_loss: 183.2762 - val_regression_loss: 74.1058\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1275.6163 - regression_loss: 289.1406 - val_loss: 183.3372 - val_regression_loss: 74.1641\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1273.8337 - regression_loss: 287.8843 - val_loss: 182.6445 - val_regression_loss: 73.8196\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1289.1617 - regression_loss: 286.2209 - val_loss: 181.7893 - val_regression_loss: 73.3739\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1290.7728 - regression_loss: 284.6566 - val_loss: 181.6631 - val_regression_loss: 73.2817\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1273.2212 - regression_loss: 283.1968 - val_loss: 182.5724 - val_regression_loss: 73.7100\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1266.6311 - regression_loss: 281.6425 - val_loss: 183.6981 - val_regression_loss: 74.2469\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 1274.5939 - regression_loss: 280.1890 - val_loss: 185.1225 - val_regression_loss: 74.9374\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1270.4120 - regression_loss: 279.0139 - val_loss: 186.3623 - val_regression_loss: 75.5409\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1232.7804 - regression_loss: 277.6959 - val_loss: 186.1270 - val_regression_loss: 75.4111\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1254.5143 - regression_loss: 276.5359 - val_loss: 185.3676 - val_regression_loss: 75.0228\n",
            "***************************** elapsed_time is:  6.37166953086853\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 180ms/step - loss: 8216.7402 - regression_loss: 2176.1201 - val_loss: 603.0849 - val_regression_loss: 280.8537\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 5151.6392 - regression_loss: 1314.9331 - val_loss: 410.8073 - val_regression_loss: 188.3450\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3468.5959 - regression_loss: 870.9608 - val_loss: 263.5152 - val_regression_loss: 114.3646\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 2102.6006 - regression_loss: 499.5241 - val_loss: 201.2468 - val_regression_loss: 80.4145\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1462.3730 - regression_loss: 322.9955 - val_loss: 287.3998 - val_regression_loss: 120.9744\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 2135.3909 - regression_loss: 500.7124 - val_loss: 265.6104 - val_regression_loss: 110.8003\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1987.8745 - regression_loss: 453.6852 - val_loss: 196.4919 - val_regression_loss: 78.4019\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1479.3961 - regression_loss: 321.8385 - val_loss: 168.5389 - val_regression_loss: 66.5564\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 1296.4653 - regression_loss: 288.1911 - val_loss: 171.6237 - val_regression_loss: 69.4939\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1410.5844 - regression_loss: 325.2181 - val_loss: 174.8484 - val_regression_loss: 71.6945\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1461.5013 - regression_loss: 349.8148 - val_loss: 168.3529 - val_regression_loss: 68.3911\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1445.8818 - regression_loss: 335.2123 - val_loss: 158.8352 - val_regression_loss: 63.0440\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1317.2626 - regression_loss: 298.4227 - val_loss: 157.4093 - val_regression_loss: 61.1862\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1209.3129 - regression_loss: 271.4003 - val_loss: 167.6847 - val_regression_loss: 64.8279\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1239.3248 - regression_loss: 268.6069 - val_loss: 176.9818 - val_regression_loss: 68.4254\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1261.2551 - regression_loss: 276.5293 - val_loss: 174.9348 - val_regression_loss: 67.4077\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1264.8296 - regression_loss: 274.1028 - val_loss: 164.2889 - val_regression_loss: 62.8752\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1236.8136 - regression_loss: 265.6775 - val_loss: 154.4859 - val_regression_loss: 58.9440\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 1201.0348 - regression_loss: 261.0318 - val_loss: 150.9748 - val_regression_loss: 57.9215\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1189.7565 - regression_loss: 264.2057 - val_loss: 152.3868 - val_regression_loss: 59.0346\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1201.9160 - regression_loss: 269.5818 - val_loss: 153.4807 - val_regression_loss: 59.6678\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1208.0419 - regression_loss: 269.2870 - val_loss: 153.1972 - val_regression_loss: 59.3247\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1182.5197 - regression_loss: 263.6896 - val_loss: 153.9219 - val_regression_loss: 59.3301\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1185.8887 - regression_loss: 258.4875 - val_loss: 157.1598 - val_regression_loss: 60.5561\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1166.1628 - regression_loss: 256.1034 - val_loss: 159.9938 - val_regression_loss: 61.7004\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1173.7036 - regression_loss: 256.1199 - val_loss: 161.2908 - val_regression_loss: 62.3066\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1146.6423 - regression_loss: 256.0934 - val_loss: 158.9888 - val_regression_loss: 61.3544\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1180.3331 - regression_loss: 255.0067 - val_loss: 156.1107 - val_regression_loss: 60.2172\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1169.4062 - regression_loss: 254.6505 - val_loss: 154.5761 - val_regression_loss: 59.7312\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1159.5961 - regression_loss: 255.3677 - val_loss: 155.1448 - val_regression_loss: 60.1826\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1136.5303 - regression_loss: 255.9484 - val_loss: 155.6207 - val_regression_loss: 60.4747\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1153.1001 - regression_loss: 255.7798 - val_loss: 155.7093 - val_regression_loss: 60.4607\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 1171.6860 - regression_loss: 254.3964 - val_loss: 154.9168 - val_regression_loss: 59.9403\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1136.9187 - regression_loss: 253.3081 - val_loss: 153.9975 - val_regression_loss: 59.3849\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1165.4384 - regression_loss: 252.8614 - val_loss: 156.4485 - val_regression_loss: 60.5861\n",
            "Epoch 36/50\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 1203.2782 - regression_loss: 481.6871\n",
            "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 1150.1456 - regression_loss: 251.9925 - val_loss: 156.8817 - val_regression_loss: 60.8838\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 1151.7850 - regression_loss: 251.8506 - val_loss: 155.8013 - val_regression_loss: 60.4089\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1159.5527 - regression_loss: 251.5647 - val_loss: 154.4662 - val_regression_loss: 59.8195\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1139.8472 - regression_loss: 251.5952 - val_loss: 153.7151 - val_regression_loss: 59.5255\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1150.2240 - regression_loss: 251.6460 - val_loss: 154.0516 - val_regression_loss: 59.7417\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1135.8199 - regression_loss: 251.5994 - val_loss: 154.9398 - val_regression_loss: 60.2190\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1141.3510 - regression_loss: 251.4900 - val_loss: 154.8352 - val_regression_loss: 60.1793\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1145.9945 - regression_loss: 251.2888 - val_loss: 154.7132 - val_regression_loss: 60.1013\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1150.6541 - regression_loss: 250.8660 - val_loss: 154.0550 - val_regression_loss: 59.7671\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1149.0677 - regression_loss: 250.5908 - val_loss: 153.4697 - val_regression_loss: 59.4606\n",
            "Epoch 46/50\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 1189.0708 - regression_loss: 477.4152\n",
            "Epoch 00046: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1136.6378 - regression_loss: 250.2817 - val_loss: 153.7570 - val_regression_loss: 59.6051\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1137.0118 - regression_loss: 250.0116 - val_loss: 154.0058 - val_regression_loss: 59.7401\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1138.5240 - regression_loss: 249.9023 - val_loss: 154.0213 - val_regression_loss: 59.7661\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1126.5863 - regression_loss: 249.8602 - val_loss: 153.9774 - val_regression_loss: 59.7655\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1105.8273 - regression_loss: 249.7936 - val_loss: 154.0004 - val_regression_loss: 59.7987\n",
            "***************************** elapsed_time is:  6.091400861740112\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 195ms/step - loss: 274556.8750 - regression_loss: 74461.9375 - val_loss: 30198.0566 - val_regression_loss: 15053.2637\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 241599.5000 - regression_loss: 66411.5938 - val_loss: 26423.7383 - val_regression_loss: 13168.0625\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 217834.3125 - regression_loss: 58265.1406 - val_loss: 21463.0918 - val_regression_loss: 10690.3438\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 176216.7969 - regression_loss: 47468.8438 - val_loss: 15421.5654 - val_regression_loss: 7673.5342\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 125678.6406 - regression_loss: 34409.2578 - val_loss: 9332.5166 - val_regression_loss: 4634.7451\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 81167.7188 - regression_loss: 21696.8926 - val_loss: 5427.7202 - val_regression_loss: 2689.4844\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 52350.4062 - regression_loss: 14078.2354 - val_loss: 5833.4834 - val_regression_loss: 2898.6167\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 57880.3906 - regression_loss: 15589.2285 - val_loss: 6608.0723 - val_regression_loss: 3287.2671\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 62894.6953 - regression_loss: 17188.2090 - val_loss: 5261.4272 - val_regression_loss: 2612.4485\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 50939.6055 - regression_loss: 14088.0195 - val_loss: 3786.3511 - val_regression_loss: 1872.7224\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 36897.9414 - regression_loss: 10540.4102 - val_loss: 3522.4614 - val_regression_loss: 1739.0907\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 35103.0859 - regression_loss: 9755.7168 - val_loss: 3969.0437 - val_regression_loss: 1961.6782\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 38271.6328 - regression_loss: 10627.7441 - val_loss: 4236.6021 - val_regression_loss: 2095.7864\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 41062.2070 - regression_loss: 11130.2158 - val_loss: 3970.3608 - val_regression_loss: 1963.7898\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 39886.7734 - regression_loss: 10508.2881 - val_loss: 3374.6333 - val_regression_loss: 1667.4609\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 33517.7891 - regression_loss: 9101.2920 - val_loss: 2872.9990 - val_regression_loss: 1418.1847\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 30133.2207 - regression_loss: 7939.5527 - val_loss: 2755.6316 - val_regression_loss: 1360.7327\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 27682.1758 - regression_loss: 7472.8516 - val_loss: 2945.0962 - val_regression_loss: 1456.1709\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 26860.6367 - regression_loss: 7650.1504 - val_loss: 3035.4102 - val_regression_loss: 1501.4944\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 27927.1230 - regression_loss: 7665.1436 - val_loss: 2825.2419 - val_regression_loss: 1396.1620\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 26609.9844 - regression_loss: 7114.6060 - val_loss: 2507.5315 - val_regression_loss: 1236.7948\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 23942.5859 - regression_loss: 6478.9160 - val_loss: 2311.6631 - val_regression_loss: 1138.2109\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 22797.0078 - regression_loss: 6134.6357 - val_loss: 2237.6670 - val_regression_loss: 1100.6396\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 22295.5352 - regression_loss: 6051.8413 - val_loss: 2153.8928 - val_regression_loss: 1058.3652\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 22224.4043 - regression_loss: 5839.1323 - val_loss: 2025.0465 - val_regression_loss: 993.8450\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 20454.9941 - regression_loss: 5401.7310 - val_loss: 1943.6837 - val_regression_loss: 953.2664\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 17055.1055 - regression_loss: 4961.0239 - val_loss: 1939.9995 - val_regression_loss: 951.6389\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 17827.6348 - regression_loss: 4719.1616 - val_loss: 1908.2238 - val_regression_loss: 935.9926\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 17047.4922 - regression_loss: 4490.2124 - val_loss: 1735.7537 - val_regression_loss: 849.8991\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 15733.2754 - regression_loss: 4131.7300 - val_loss: 1515.1644 - val_regression_loss: 739.5809\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 14333.6611 - regression_loss: 3780.8975 - val_loss: 1358.3767 - val_regression_loss: 661.0877\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 13070.1270 - regression_loss: 3561.0151 - val_loss: 1257.6814 - val_regression_loss: 610.7572\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 12771.9658 - regression_loss: 3371.1531 - val_loss: 1190.6295 - val_regression_loss: 577.4914\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 11868.2207 - regression_loss: 3096.9233 - val_loss: 1174.3521 - val_regression_loss: 569.6533\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 10886.7441 - regression_loss: 2877.8945 - val_loss: 1151.1600 - val_regression_loss: 558.1796\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 10271.5234 - regression_loss: 2720.2397 - val_loss: 1055.9478 - val_regression_loss: 510.4371\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 9739.2500 - regression_loss: 2533.8042 - val_loss: 940.6580 - val_regression_loss: 452.5473\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 9109.5107 - regression_loss: 2373.6865 - val_loss: 870.2693 - val_regression_loss: 417.2507\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 8665.1016 - regression_loss: 2257.3730 - val_loss: 844.5946 - val_regression_loss: 404.6093\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 8130.4497 - regression_loss: 2109.2737 - val_loss: 850.2081 - val_regression_loss: 407.7215\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 7557.8071 - regression_loss: 1980.4749 - val_loss: 821.2443 - val_regression_loss: 393.3880\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 7168.9004 - regression_loss: 1861.9808 - val_loss: 757.4216 - val_regression_loss: 361.4779\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 6758.7139 - regression_loss: 1739.2377 - val_loss: 701.0153 - val_regression_loss: 333.2620\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 6322.5664 - regression_loss: 1632.0022 - val_loss: 678.5518 - val_regression_loss: 322.1379\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 5972.8877 - regression_loss: 1525.0171 - val_loss: 685.5506 - val_regression_loss: 325.8101\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 5528.8828 - regression_loss: 1426.4801 - val_loss: 678.1899 - val_regression_loss: 322.2267\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 5089.8789 - regression_loss: 1339.9652 - val_loss: 629.4218 - val_regression_loss: 297.8028\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 4359.3882 - regression_loss: 1260.6484 - val_loss: 595.9346 - val_regression_loss: 281.0485\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 4645.9824 - regression_loss: 1202.8889 - val_loss: 612.1557 - val_regression_loss: 289.3205\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 4486.2412 - regression_loss: 1145.8147 - val_loss: 627.8967 - val_regression_loss: 297.3123\n",
            "***************************** elapsed_time is:  6.06724214553833\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 216ms/step - loss: 6969.6396 - regression_loss: 1772.6267 - val_loss: 624.6959 - val_regression_loss: 298.2042\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 4258.9834 - regression_loss: 1117.3643 - val_loss: 475.2057 - val_regression_loss: 228.2466\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 3051.0398 - regression_loss: 804.0117 - val_loss: 301.0225 - val_regression_loss: 138.5911\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1915.8870 - regression_loss: 461.7521 - val_loss: 192.2529 - val_regression_loss: 77.6504\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 1369.8436 - regression_loss: 290.7580 - val_loss: 214.6445 - val_regression_loss: 84.2849\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1980.7780 - regression_loss: 426.5163 - val_loss: 192.0798 - val_regression_loss: 74.8270\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1768.2209 - regression_loss: 385.4976 - val_loss: 167.0956 - val_regression_loss: 66.3463\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1401.6923 - regression_loss: 299.6047 - val_loss: 177.9839 - val_regression_loss: 75.1324\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1290.5002 - regression_loss: 287.5234 - val_loss: 206.6356 - val_regression_loss: 91.4828\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 1393.5582 - regression_loss: 333.9961 - val_loss: 215.7820 - val_regression_loss: 97.1470\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1442.6606 - regression_loss: 353.5512 - val_loss: 199.0682 - val_regression_loss: 89.0323\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1398.7291 - regression_loss: 329.0571 - val_loss: 174.9295 - val_regression_loss: 75.7274\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1256.2065 - regression_loss: 291.2720 - val_loss: 157.5484 - val_regression_loss: 64.6931\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1228.5642 - regression_loss: 269.3925 - val_loss: 152.8948 - val_regression_loss: 60.0650\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1289.2501 - regression_loss: 269.6977 - val_loss: 153.5298 - val_regression_loss: 59.5438\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1311.0647 - regression_loss: 274.5317 - val_loss: 152.0856 - val_regression_loss: 59.6105\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1259.8319 - regression_loss: 268.1932 - val_loss: 153.8131 - val_regression_loss: 61.9081\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1213.2939 - regression_loss: 265.6768 - val_loss: 159.4693 - val_regression_loss: 65.8918\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1212.1266 - regression_loss: 270.8597 - val_loss: 164.4520 - val_regression_loss: 68.8590\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1235.4464 - regression_loss: 277.8900 - val_loss: 164.9501 - val_regression_loss: 68.9658\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1223.3707 - regression_loss: 278.6552 - val_loss: 160.7727 - val_regression_loss: 66.3751\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 1218.0850 - regression_loss: 272.9421 - val_loss: 155.0359 - val_regression_loss: 62.7848\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1206.7937 - regression_loss: 265.7267 - val_loss: 151.2374 - val_regression_loss: 60.1966\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1180.3208 - regression_loss: 263.1016 - val_loss: 149.8508 - val_regression_loss: 58.9268\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1198.8417 - regression_loss: 262.0417 - val_loss: 149.8068 - val_regression_loss: 58.8183\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1189.0032 - regression_loss: 261.8504 - val_loss: 150.7401 - val_regression_loss: 59.6931\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1159.0090 - regression_loss: 261.2428 - val_loss: 152.5076 - val_regression_loss: 61.0728\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1165.0251 - regression_loss: 262.8061 - val_loss: 154.3112 - val_regression_loss: 62.4197\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1201.8658 - regression_loss: 265.0923 - val_loss: 154.8200 - val_regression_loss: 62.8133\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1180.6731 - regression_loss: 265.8191 - val_loss: 153.7320 - val_regression_loss: 62.1304\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1197.4086 - regression_loss: 264.4014 - val_loss: 151.9131 - val_regression_loss: 60.9060\n",
            "Epoch 32/50\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 1224.5601 - regression_loss: 495.8382\n",
            "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1170.8807 - regression_loss: 262.1984 - val_loss: 150.7293 - val_regression_loss: 59.9557\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1155.6095 - regression_loss: 260.6335 - val_loss: 150.5438 - val_regression_loss: 59.7397\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1184.8662 - regression_loss: 260.1609 - val_loss: 150.7998 - val_regression_loss: 59.8536\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1172.8171 - regression_loss: 260.1104 - val_loss: 151.2778 - val_regression_loss: 60.1328\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1183.8124 - regression_loss: 260.2154 - val_loss: 151.8782 - val_regression_loss: 60.4941\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1184.0731 - regression_loss: 260.3914 - val_loss: 152.3292 - val_regression_loss: 60.8039\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1155.1134 - regression_loss: 260.6346 - val_loss: 152.6843 - val_regression_loss: 61.0305\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1176.8977 - regression_loss: 260.8346 - val_loss: 152.9006 - val_regression_loss: 61.1369\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1151.8585 - regression_loss: 260.8336 - val_loss: 152.9515 - val_regression_loss: 61.1317\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1155.2653 - regression_loss: 260.6757 - val_loss: 152.7138 - val_regression_loss: 60.9506\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1172.8105 - regression_loss: 260.2509 - val_loss: 152.7001 - val_regression_loss: 60.8647\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 1174.3895 - regression_loss: 259.9349 - val_loss: 152.8039 - val_regression_loss: 60.8715\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1165.8076 - regression_loss: 259.6492 - val_loss: 152.4699 - val_regression_loss: 60.6209\n",
            "Epoch 45/50\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 1206.7712 - regression_loss: 491.3043\n",
            "Epoch 00045: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1154.1178 - regression_loss: 259.3203 - val_loss: 152.1845 - val_regression_loss: 60.3939\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1170.1943 - regression_loss: 258.7324 - val_loss: 152.1651 - val_regression_loss: 60.3864\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1152.0524 - regression_loss: 258.6997 - val_loss: 152.2829 - val_regression_loss: 60.4667\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 1179.2727 - regression_loss: 258.7312 - val_loss: 152.5110 - val_regression_loss: 60.6261\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1160.6320 - regression_loss: 258.8535 - val_loss: 152.6639 - val_regression_loss: 60.7323\n",
            "Epoch 50/50\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 1214.9065 - regression_loss: 494.8958\n",
            "Epoch 00050: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1161.3224 - regression_loss: 258.9416 - val_loss: 152.7263 - val_regression_loss: 60.7620\n",
            "***************************** elapsed_time is:  6.010124444961548\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 168ms/step - loss: 40554.9570 - regression_loss: 11057.9697 - val_loss: 3504.7219 - val_regression_loss: 1747.5479\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 29753.2793 - regression_loss: 8169.6260 - val_loss: 2368.7405 - val_regression_loss: 1171.7975\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 20061.4121 - regression_loss: 5456.8774 - val_loss: 1312.5002 - val_regression_loss: 628.6937\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 11396.8008 - regression_loss: 2961.8801 - val_loss: 960.5405 - val_regression_loss: 428.0871\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 8128.8687 - regression_loss: 1995.3502 - val_loss: 987.4432 - val_regression_loss: 432.4603\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 7912.9424 - regression_loss: 1924.1611 - val_loss: 596.6033 - val_regression_loss: 250.5529\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 4918.3545 - regression_loss: 1113.1866 - val_loss: 330.6244 - val_regression_loss: 135.9105\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2735.5212 - regression_loss: 617.2862 - val_loss: 408.1299 - val_regression_loss: 188.0307\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 3464.3860 - regression_loss: 873.4026 - val_loss: 540.3740 - val_regression_loss: 260.0671\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 4443.2593 - regression_loss: 1190.3905 - val_loss: 510.4039 - val_regression_loss: 245.2594\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 4182.0884 - regression_loss: 1115.3033 - val_loss: 378.0310 - val_regression_loss: 176.1536\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3163.8787 - regression_loss: 814.1346 - val_loss: 268.4511 - val_regression_loss: 117.2575\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2282.0425 - regression_loss: 563.9095 - val_loss: 242.4255 - val_regression_loss: 100.4424\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2176.9905 - regression_loss: 514.5587 - val_loss: 264.8785 - val_regression_loss: 109.1643\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2435.3433 - regression_loss: 566.7449 - val_loss: 270.9899 - val_regression_loss: 111.4979\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2500.7925 - regression_loss: 582.4495 - val_loss: 252.7606 - val_regression_loss: 103.1003\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 2337.6992 - regression_loss: 537.9272 - val_loss: 239.0415 - val_regression_loss: 97.7289\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2152.7993 - regression_loss: 500.0970 - val_loss: 234.5822 - val_regression_loss: 97.2117\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2059.1201 - regression_loss: 477.3139 - val_loss: 228.5563 - val_regression_loss: 95.7404\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 1943.7802 - regression_loss: 455.9250 - val_loss: 222.0768 - val_regression_loss: 93.8126\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1833.6649 - regression_loss: 433.7635 - val_loss: 223.4336 - val_regression_loss: 95.4773\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1756.7296 - regression_loss: 433.4821 - val_loss: 228.5404 - val_regression_loss: 98.5909\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1837.4066 - regression_loss: 442.8730 - val_loss: 225.1907 - val_regression_loss: 96.9813\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1806.1112 - regression_loss: 431.7827 - val_loss: 213.0532 - val_regression_loss: 90.5592\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1711.5801 - regression_loss: 403.8093 - val_loss: 203.1745 - val_regression_loss: 85.0057\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1616.6350 - regression_loss: 383.0802 - val_loss: 198.5677 - val_regression_loss: 82.0116\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1646.0665 - regression_loss: 375.7610 - val_loss: 193.3276 - val_regression_loss: 78.7576\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1600.8586 - regression_loss: 367.0831 - val_loss: 188.3891 - val_regression_loss: 75.8547\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1547.5457 - regression_loss: 356.6592 - val_loss: 187.6607 - val_regression_loss: 75.3747\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1508.6168 - regression_loss: 351.3190 - val_loss: 187.9650 - val_regression_loss: 75.7477\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1553.4510 - regression_loss: 348.4480 - val_loss: 187.0384 - val_regression_loss: 75.8554\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1475.9144 - regression_loss: 340.9918 - val_loss: 187.5757 - val_regression_loss: 76.7359\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 1470.7268 - regression_loss: 338.9467 - val_loss: 188.8947 - val_regression_loss: 77.8882\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1457.4554 - regression_loss: 339.3336 - val_loss: 188.1740 - val_regression_loss: 77.7377\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1444.3821 - regression_loss: 335.8373 - val_loss: 185.4673 - val_regression_loss: 76.3511\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1442.5278 - regression_loss: 331.1285 - val_loss: 181.7981 - val_regression_loss: 74.3600\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 1446.9305 - regression_loss: 327.2777 - val_loss: 177.4699 - val_regression_loss: 72.0439\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1394.1205 - regression_loss: 322.3143 - val_loss: 173.6039 - val_regression_loss: 70.0285\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1421.0020 - regression_loss: 317.9567 - val_loss: 171.0461 - val_regression_loss: 68.7007\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1407.2239 - regression_loss: 314.6467 - val_loss: 170.3442 - val_regression_loss: 68.2971\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1381.9821 - regression_loss: 310.9944 - val_loss: 170.2706 - val_regression_loss: 68.2489\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1384.4552 - regression_loss: 308.2271 - val_loss: 170.4773 - val_regression_loss: 68.4065\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1356.9474 - regression_loss: 305.9930 - val_loss: 168.8638 - val_regression_loss: 67.7923\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1330.6304 - regression_loss: 303.7398 - val_loss: 166.6524 - val_regression_loss: 66.8986\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1345.5315 - regression_loss: 302.2259 - val_loss: 164.5746 - val_regression_loss: 65.9775\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1335.5511 - regression_loss: 300.6954 - val_loss: 163.5000 - val_regression_loss: 65.4811\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1305.0590 - regression_loss: 298.6155 - val_loss: 163.0031 - val_regression_loss: 65.2105\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1294.4606 - regression_loss: 296.3918 - val_loss: 161.9695 - val_regression_loss: 64.6547\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1327.4100 - regression_loss: 293.9388 - val_loss: 160.6256 - val_regression_loss: 63.9893\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1317.9590 - regression_loss: 291.7763 - val_loss: 158.4690 - val_regression_loss: 62.9656\n",
            "***************************** elapsed_time is:  6.483909606933594\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 167ms/step - loss: 13240.0078 - regression_loss: 3551.0066 - val_loss: 813.3333 - val_regression_loss: 384.0575\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 8681.8057 - regression_loss: 2242.8093 - val_loss: 487.6320 - val_regression_loss: 214.9186\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 5178.8135 - regression_loss: 1306.1802 - val_loss: 387.5783 - val_regression_loss: 159.2414\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3809.4502 - regression_loss: 873.6277 - val_loss: 278.7959 - val_regression_loss: 109.7649\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2391.2305 - regression_loss: 530.6868 - val_loss: 263.7058 - val_regression_loss: 112.5423\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2058.0820 - regression_loss: 478.4848 - val_loss: 368.1015 - val_regression_loss: 172.1328\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 2768.3350 - regression_loss: 718.5032 - val_loss: 309.4109 - val_regression_loss: 142.8735\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2362.0725 - regression_loss: 601.3044 - val_loss: 205.1701 - val_regression_loss: 87.7286\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1706.6034 - regression_loss: 400.2243 - val_loss: 172.2174 - val_regression_loss: 68.0450\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1553.2800 - regression_loss: 354.3114 - val_loss: 176.6116 - val_regression_loss: 68.3129\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1763.2563 - regression_loss: 388.3629 - val_loss: 172.0307 - val_regression_loss: 65.6824\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1787.4879 - regression_loss: 394.6071 - val_loss: 164.0989 - val_regression_loss: 62.5488\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1700.1594 - regression_loss: 379.5217 - val_loss: 157.7503 - val_regression_loss: 60.8643\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1578.6577 - regression_loss: 353.8521 - val_loss: 155.6429 - val_regression_loss: 61.6123\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1437.7087 - regression_loss: 330.8034 - val_loss: 165.5329 - val_regression_loss: 68.3610\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1446.4670 - regression_loss: 333.7899 - val_loss: 179.6411 - val_regression_loss: 76.6932\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1491.3835 - regression_loss: 352.3414 - val_loss: 180.5166 - val_regression_loss: 77.4352\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1475.8279 - regression_loss: 350.5223 - val_loss: 171.0311 - val_regression_loss: 72.0789\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1445.3960 - regression_loss: 332.0941 - val_loss: 163.4653 - val_regression_loss: 67.1978\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1392.6497 - regression_loss: 319.5933 - val_loss: 158.2970 - val_regression_loss: 63.4734\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 1391.1547 - regression_loss: 307.5956 - val_loss: 156.8315 - val_regression_loss: 61.9304\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1360.5610 - regression_loss: 302.4046 - val_loss: 158.3067 - val_regression_loss: 62.4059\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1360.0452 - regression_loss: 300.8983 - val_loss: 158.6275 - val_regression_loss: 62.7854\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1324.4160 - regression_loss: 299.2680 - val_loss: 159.2077 - val_regression_loss: 63.6575\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1333.3336 - regression_loss: 298.4349 - val_loss: 161.0210 - val_regression_loss: 65.1637\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1321.8395 - regression_loss: 298.8339 - val_loss: 163.4630 - val_regression_loss: 66.7760\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1314.8602 - regression_loss: 298.2571 - val_loss: 166.0627 - val_regression_loss: 68.1892\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1318.6943 - regression_loss: 298.4475 - val_loss: 166.3434 - val_regression_loss: 68.2784\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1291.2651 - regression_loss: 297.4160 - val_loss: 163.9466 - val_regression_loss: 66.9164\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1322.0087 - regression_loss: 293.2495 - val_loss: 161.2365 - val_regression_loss: 65.3340\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1299.6615 - regression_loss: 290.4292 - val_loss: 159.1562 - val_regression_loss: 64.0047\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1293.6741 - regression_loss: 288.2549 - val_loss: 157.9746 - val_regression_loss: 63.2193\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1284.7704 - regression_loss: 286.7407 - val_loss: 157.3762 - val_regression_loss: 62.9332\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 1272.2933 - regression_loss: 285.8306 - val_loss: 156.9403 - val_regression_loss: 62.9424\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1300.7225 - regression_loss: 284.9570 - val_loss: 157.9216 - val_regression_loss: 63.6577\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 1267.7362 - regression_loss: 284.9110 - val_loss: 159.2779 - val_regression_loss: 64.4427\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1258.3126 - regression_loss: 284.4167 - val_loss: 159.9162 - val_regression_loss: 64.8363\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1257.5645 - regression_loss: 283.8783 - val_loss: 159.4581 - val_regression_loss: 64.6363\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 1271.2045 - regression_loss: 282.7800 - val_loss: 158.8986 - val_regression_loss: 64.3878\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1238.3660 - regression_loss: 282.4471 - val_loss: 158.1628 - val_regression_loss: 63.9085\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1267.3060 - regression_loss: 280.9198 - val_loss: 157.9439 - val_regression_loss: 63.6218\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1246.2180 - regression_loss: 279.6116 - val_loss: 157.4347 - val_regression_loss: 63.2909\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1246.3105 - regression_loss: 278.6362 - val_loss: 157.4347 - val_regression_loss: 63.4435\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1259.0258 - regression_loss: 278.4117 - val_loss: 158.1520 - val_regression_loss: 63.9379\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1232.3975 - regression_loss: 278.5005 - val_loss: 158.7984 - val_regression_loss: 64.2583\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1244.1038 - regression_loss: 277.7712 - val_loss: 159.0958 - val_regression_loss: 64.3839\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1248.1271 - regression_loss: 277.3025 - val_loss: 158.9538 - val_regression_loss: 64.3419\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1232.6752 - regression_loss: 276.6520 - val_loss: 158.7840 - val_regression_loss: 64.2874\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1243.7504 - regression_loss: 276.3437 - val_loss: 158.4289 - val_regression_loss: 64.0395\n",
            "Epoch 50/50\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 1293.4965 - regression_loss: 529.5388\n",
            "Epoch 00050: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1236.1526 - regression_loss: 275.4709 - val_loss: 158.3399 - val_regression_loss: 63.8979\n",
            "***************************** elapsed_time is:  6.210883855819702\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 165ms/step - loss: 8762.9297 - regression_loss: 2268.9497 - val_loss: 610.4851 - val_regression_loss: 285.0875\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 5266.2402 - regression_loss: 1361.3029 - val_loss: 397.7454 - val_regression_loss: 181.8451\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 3242.9460 - regression_loss: 817.1763 - val_loss: 278.6587 - val_regression_loss: 123.2343\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 2101.6821 - regression_loss: 507.7265 - val_loss: 201.4895 - val_regression_loss: 82.5926\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1445.5128 - regression_loss: 335.5065 - val_loss: 276.1222 - val_regression_loss: 117.1208\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2101.0720 - regression_loss: 490.4228 - val_loss: 265.3359 - val_regression_loss: 112.0013\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2029.1268 - regression_loss: 463.3710 - val_loss: 197.2286 - val_regression_loss: 79.8897\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1455.5724 - regression_loss: 316.9284 - val_loss: 177.5120 - val_regression_loss: 72.0323\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1307.4813 - regression_loss: 293.7091 - val_loss: 189.6299 - val_regression_loss: 79.3677\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1432.6985 - regression_loss: 339.1501 - val_loss: 195.2433 - val_regression_loss: 82.5684\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1548.1403 - regression_loss: 361.3544 - val_loss: 188.1157 - val_regression_loss: 78.7746\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1493.4890 - regression_loss: 344.3979 - val_loss: 175.7407 - val_regression_loss: 72.0040\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1360.8727 - regression_loss: 306.7001 - val_loss: 168.5380 - val_regression_loss: 67.5379\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1267.7841 - regression_loss: 279.8081 - val_loss: 172.7469 - val_regression_loss: 68.6555\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1277.1663 - regression_loss: 279.4565 - val_loss: 179.7383 - val_regression_loss: 71.4924\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1326.1439 - regression_loss: 289.6693 - val_loss: 179.0847 - val_regression_loss: 71.1875\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 1292.8239 - regression_loss: 286.2225 - val_loss: 174.0392 - val_regression_loss: 69.1256\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1283.7246 - regression_loss: 277.4344 - val_loss: 169.9258 - val_regression_loss: 67.6561\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1249.3225 - regression_loss: 272.9415 - val_loss: 168.7012 - val_regression_loss: 67.5456\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1240.9310 - regression_loss: 275.7363 - val_loss: 169.8014 - val_regression_loss: 68.4274\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1223.7434 - regression_loss: 280.6571 - val_loss: 169.8369 - val_regression_loss: 68.4956\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 1254.7280 - regression_loss: 281.2323 - val_loss: 168.2254 - val_regression_loss: 67.4930\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1210.3063 - regression_loss: 275.3730 - val_loss: 167.7408 - val_regression_loss: 66.9493\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1207.3627 - regression_loss: 270.3450 - val_loss: 169.0814 - val_regression_loss: 67.3271\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1225.6548 - regression_loss: 268.7762 - val_loss: 171.3522 - val_regression_loss: 68.2586\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1221.7751 - regression_loss: 268.4942 - val_loss: 172.8082 - val_regression_loss: 68.9419\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1207.7273 - regression_loss: 268.4904 - val_loss: 172.4957 - val_regression_loss: 68.8838\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 1194.9862 - regression_loss: 267.5722 - val_loss: 171.3982 - val_regression_loss: 68.4982\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1206.1776 - regression_loss: 266.6386 - val_loss: 170.3426 - val_regression_loss: 68.1493\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1198.8849 - regression_loss: 267.1221 - val_loss: 169.4371 - val_regression_loss: 67.8362\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 1201.7291 - regression_loss: 267.4974 - val_loss: 169.1940 - val_regression_loss: 67.7332\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1213.8638 - regression_loss: 267.2323 - val_loss: 169.0837 - val_regression_loss: 67.5854\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1193.3176 - regression_loss: 266.0971 - val_loss: 169.6887 - val_regression_loss: 67.7294\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1180.0981 - regression_loss: 264.5252 - val_loss: 170.6258 - val_regression_loss: 68.0826\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1176.1044 - regression_loss: 264.0543 - val_loss: 171.6024 - val_regression_loss: 68.4903\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1205.4966 - regression_loss: 263.5985 - val_loss: 171.4497 - val_regression_loss: 68.4243\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1186.9371 - regression_loss: 263.2458 - val_loss: 170.7693 - val_regression_loss: 68.1085\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1185.0864 - regression_loss: 262.8382 - val_loss: 170.4250 - val_regression_loss: 67.9977\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1130.7618 - regression_loss: 262.8333 - val_loss: 170.6530 - val_regression_loss: 68.1306\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1144.2194 - regression_loss: 262.7401 - val_loss: 170.9567 - val_regression_loss: 68.2072\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1185.2126 - regression_loss: 261.9385 - val_loss: 171.5023 - val_regression_loss: 68.3851\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1177.0768 - regression_loss: 261.2352 - val_loss: 172.1901 - val_regression_loss: 68.6380\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1170.1494 - regression_loss: 260.6656 - val_loss: 173.0669 - val_regression_loss: 69.0011\n",
            "Epoch 44/50\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 1199.7980 - regression_loss: 485.0686\n",
            "Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1147.8646 - regression_loss: 260.4466 - val_loss: 173.2147 - val_regression_loss: 69.0732\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1158.0565 - regression_loss: 260.0871 - val_loss: 172.9929 - val_regression_loss: 68.9235\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1153.1433 - regression_loss: 259.6364 - val_loss: 172.8625 - val_regression_loss: 68.8448\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1144.7502 - regression_loss: 259.4508 - val_loss: 172.8709 - val_regression_loss: 68.8170\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1153.4858 - regression_loss: 259.3691 - val_loss: 172.9491 - val_regression_loss: 68.8385\n",
            "Epoch 49/50\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 1217.7948 - regression_loss: 496.7363\n",
            "Epoch 00049: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1163.7892 - regression_loss: 259.0609 - val_loss: 173.2396 - val_regression_loss: 68.9711\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1169.3579 - regression_loss: 258.8552 - val_loss: 173.4138 - val_regression_loss: 69.0464\n",
            "***************************** elapsed_time is:  5.8258960247039795\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 213ms/step - loss: 468068.4688 - regression_loss: 127091.5781 - val_loss: 45923.8398 - val_regression_loss: 22841.6152\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 430536.9062 - regression_loss: 117928.2891 - val_loss: 42030.9297 - val_regression_loss: 20901.8555\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 401587.4375 - regression_loss: 108322.0156 - val_loss: 36445.3594 - val_regression_loss: 18121.8672\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 351367.7500 - regression_loss: 94557.6562 - val_loss: 29040.1621 - val_regression_loss: 14438.5781\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 280629.7188 - regression_loss: 76255.3516 - val_loss: 20335.8086 - val_regression_loss: 10111.7461\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 202906.5625 - regression_loss: 54729.4531 - val_loss: 11617.1650 - val_regression_loss: 5781.2471\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 121528.7969 - regression_loss: 32632.9824 - val_loss: 5200.5415 - val_regression_loss: 2600.9482\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 55876.1641 - regression_loss: 15130.9873 - val_loss: 4424.8730 - val_regression_loss: 2232.6016\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 35715.9844 - regression_loss: 9822.1387 - val_loss: 9206.1689 - val_regression_loss: 4617.0425\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 62819.1641 - regression_loss: 17429.4844 - val_loss: 10464.0312 - val_regression_loss: 5217.6562\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 73522.0078 - regression_loss: 19736.6504 - val_loss: 7104.1987 - val_regression_loss: 3528.8179\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 51310.2031 - regression_loss: 13648.9307 - val_loss: 3722.5505 - val_regression_loss: 1842.1104\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 30038.7207 - regression_loss: 7970.8325 - val_loss: 2367.9417 - val_regression_loss: 1168.4841\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 24732.5918 - regression_loss: 6645.0005 - val_loss: 2455.0679 - val_regression_loss: 1213.0000\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 29709.6504 - regression_loss: 7866.4741 - val_loss: 2862.0984 - val_regression_loss: 1416.6652\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 32536.1523 - regression_loss: 9214.3867 - val_loss: 2982.0981 - val_regression_loss: 1477.4872\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 34646.9883 - regression_loss: 9398.5029 - val_loss: 2745.1462 - val_regression_loss: 1361.0459\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 30890.4707 - regression_loss: 8417.9824 - val_loss: 2397.0930 - val_regression_loss: 1190.0449\n",
            "Epoch 19/50\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 26479.4844 - regression_loss: 13089.8516\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 25320.2441 - regression_loss: 6934.4985 - val_loss: 2253.3381 - val_regression_loss: 1121.6644\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 21055.1641 - regression_loss: 5769.4575 - val_loss: 2316.0168 - val_regression_loss: 1154.6514\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 19595.0684 - regression_loss: 5470.3398 - val_loss: 2452.3528 - val_regression_loss: 1224.1987\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 20287.4004 - regression_loss: 5387.3125 - val_loss: 2617.2502 - val_regression_loss: 1307.6621\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 20283.4531 - regression_loss: 5422.4937 - val_loss: 2748.9534 - val_regression_loss: 1374.1207\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 20700.4238 - regression_loss: 5480.5562 - val_loss: 2798.4250 - val_regression_loss: 1399.0420\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 20311.2969 - regression_loss: 5476.8120 - val_loss: 2751.5066 - val_regression_loss: 1375.3768\n",
            "Epoch 26/50\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 20526.6816 - regression_loss: 10173.8154\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 19624.8965 - regression_loss: 5373.7617 - val_loss: 2631.3389 - val_regression_loss: 1314.8240\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 19494.3398 - regression_loss: 5209.5757 - val_loss: 2552.1592 - val_regression_loss: 1274.9045\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 18793.2383 - regression_loss: 5113.3193 - val_loss: 2462.0947 - val_regression_loss: 1229.4817\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 18647.2598 - regression_loss: 5019.6016 - val_loss: 2371.5264 - val_regression_loss: 1183.7662\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 18400.6074 - regression_loss: 4935.8296 - val_loss: 2286.6865 - val_regression_loss: 1140.9130\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 18177.3359 - regression_loss: 4871.5854 - val_loss: 2210.6697 - val_regression_loss: 1102.4822\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 18233.9805 - regression_loss: 4817.8901 - val_loss: 2145.5566 - val_regression_loss: 1069.5498\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 14823.4014 - regression_loss: 4778.2637 - val_loss: 2089.4609 - val_regression_loss: 1041.1670\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 17859.0723 - regression_loss: 4746.0137 - val_loss: 2043.1350 - val_regression_loss: 1017.7540\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 17458.8281 - regression_loss: 4713.5205 - val_loss: 2004.4532 - val_regression_loss: 998.2290\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 17656.3516 - regression_loss: 4679.6675 - val_loss: 1973.0564 - val_regression_loss: 982.4131\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 17318.5371 - regression_loss: 4642.1538 - val_loss: 1948.1344 - val_regression_loss: 969.8992\n",
            "Epoch 38/50\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 17306.8984 - regression_loss: 8536.7451\n",
            "Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 16564.1348 - regression_loss: 4601.2910 - val_loss: 1927.0834 - val_regression_loss: 959.3666\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 17038.9707 - regression_loss: 4558.9175 - val_loss: 1919.9648 - val_regression_loss: 955.8317\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 17083.3555 - regression_loss: 4536.8125 - val_loss: 1915.0222 - val_regression_loss: 953.4025\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 16951.5898 - regression_loss: 4513.9580 - val_loss: 1911.4088 - val_regression_loss: 951.6464\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 16032.2422 - regression_loss: 4489.7559 - val_loss: 1908.5770 - val_regression_loss: 950.2872\n",
            "Epoch 43/50\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 17264.8574 - regression_loss: 8514.9912\n",
            "Epoch 00043: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 16501.0898 - regression_loss: 4468.6245 - val_loss: 1906.1837 - val_regression_loss: 949.1526\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 16477.2910 - regression_loss: 4446.7168 - val_loss: 1905.1119 - val_regression_loss: 948.6476\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 16750.6719 - regression_loss: 4436.0776 - val_loss: 1904.1555 - val_regression_loss: 948.2015\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 16569.5605 - regression_loss: 4425.4014 - val_loss: 1903.0645 - val_regression_loss: 947.6873\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 13857.8447 - regression_loss: 4413.8691 - val_loss: 1901.4690 - val_regression_loss: 946.9165\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 16135.9297 - regression_loss: 4403.6924 - val_loss: 1899.7684 - val_regression_loss: 946.0924\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 15941.0801 - regression_loss: 4392.9863 - val_loss: 1897.8734 - val_regression_loss: 945.1711\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 15307.9385 - regression_loss: 4382.0815 - val_loss: 1895.1481 - val_regression_loss: 943.8263\n",
            "***************************** elapsed_time is:  6.104827165603638\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 163ms/step - loss: 34843.1680 - regression_loss: 9557.3457 - val_loss: 3494.5613 - val_regression_loss: 1743.9891\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 27274.4004 - regression_loss: 7417.8750 - val_loss: 2519.3325 - val_regression_loss: 1250.9556\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 19672.7754 - regression_loss: 5262.3350 - val_loss: 1434.3313 - val_regression_loss: 697.3718\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 10977.3320 - regression_loss: 2894.8936 - val_loss: 701.6135 - val_regression_loss: 309.7312\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 5688.2295 - regression_loss: 1350.4302 - val_loss: 740.3972 - val_regression_loss: 309.6956\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 6389.3916 - regression_loss: 1466.9666 - val_loss: 595.0732 - val_regression_loss: 246.9348\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 5390.9141 - regression_loss: 1237.9506 - val_loss: 354.7907 - val_regression_loss: 145.8804\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3409.5669 - regression_loss: 801.2251 - val_loss: 302.0990 - val_regression_loss: 133.7946\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 2779.9683 - regression_loss: 682.8252 - val_loss: 398.0337 - val_regression_loss: 188.5503\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 3260.2720 - regression_loss: 847.3534 - val_loss: 462.6957 - val_regression_loss: 222.0930\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 3508.3372 - regression_loss: 945.7905 - val_loss: 425.8848 - val_regression_loss: 201.9136\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 3259.7285 - regression_loss: 850.7233 - val_loss: 323.8456 - val_regression_loss: 147.7852\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 2550.3384 - regression_loss: 639.9604 - val_loss: 231.4120 - val_regression_loss: 98.1457\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1892.3488 - regression_loss: 457.9337 - val_loss: 203.3018 - val_regression_loss: 81.1741\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1859.2150 - regression_loss: 429.1361 - val_loss: 225.9136 - val_regression_loss: 90.6997\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2153.0425 - regression_loss: 495.4308 - val_loss: 237.6283 - val_regression_loss: 96.2791\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2243.4856 - regression_loss: 521.2037 - val_loss: 216.0248 - val_regression_loss: 86.4347\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2061.0010 - regression_loss: 465.3902 - val_loss: 193.2704 - val_regression_loss: 76.6363\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1756.2509 - regression_loss: 403.2554 - val_loss: 193.6667 - val_regression_loss: 78.5848\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1699.5984 - regression_loss: 396.0079 - val_loss: 206.2113 - val_regression_loss: 86.4489\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1782.1721 - regression_loss: 418.6657 - val_loss: 210.4455 - val_regression_loss: 89.8573\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1802.2958 - regression_loss: 428.7103 - val_loss: 202.5499 - val_regression_loss: 86.8087\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1705.6924 - regression_loss: 414.6653 - val_loss: 192.7614 - val_regression_loss: 82.2942\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1675.7028 - regression_loss: 400.7372 - val_loss: 185.9740 - val_regression_loss: 78.6256\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1638.3510 - regression_loss: 390.6770 - val_loss: 181.0484 - val_regression_loss: 75.2641\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1579.2921 - regression_loss: 381.6027 - val_loss: 176.6743 - val_regression_loss: 71.9022\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1630.6771 - regression_loss: 370.2999 - val_loss: 174.4567 - val_regression_loss: 69.6612\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1572.8007 - regression_loss: 359.6078 - val_loss: 176.3251 - val_regression_loss: 69.9732\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1583.4828 - regression_loss: 355.4318 - val_loss: 178.5029 - val_regression_loss: 71.1266\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1583.8448 - regression_loss: 355.1988 - val_loss: 177.9500 - val_regression_loss: 71.4939\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1560.9363 - regression_loss: 352.4824 - val_loss: 175.1778 - val_regression_loss: 71.1560\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 1520.3700 - regression_loss: 349.0667 - val_loss: 173.7726 - val_regression_loss: 71.4433\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1497.0942 - regression_loss: 349.9589 - val_loss: 173.6160 - val_regression_loss: 72.0000\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1527.9402 - regression_loss: 352.1805 - val_loss: 172.6603 - val_regression_loss: 71.5688\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1497.8641 - regression_loss: 349.2049 - val_loss: 171.3754 - val_regression_loss: 70.5300\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1483.0387 - regression_loss: 342.9968 - val_loss: 170.4704 - val_regression_loss: 69.4143\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1467.5398 - regression_loss: 336.4873 - val_loss: 169.6418 - val_regression_loss: 68.4093\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1490.2261 - regression_loss: 331.8117 - val_loss: 168.5011 - val_regression_loss: 67.5128\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1452.4642 - regression_loss: 328.0491 - val_loss: 166.8888 - val_regression_loss: 66.6872\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1415.1934 - regression_loss: 325.2506 - val_loss: 165.2846 - val_regression_loss: 66.0233\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1438.9619 - regression_loss: 323.6417 - val_loss: 163.9945 - val_regression_loss: 65.7559\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1398.0626 - regression_loss: 321.9919 - val_loss: 164.0196 - val_regression_loss: 66.0364\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1409.9791 - regression_loss: 320.3327 - val_loss: 164.3549 - val_regression_loss: 66.3330\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1396.4614 - regression_loss: 318.5054 - val_loss: 164.3327 - val_regression_loss: 66.2054\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1382.1548 - regression_loss: 315.8278 - val_loss: 164.0428 - val_regression_loss: 65.8791\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1380.0940 - regression_loss: 312.7251 - val_loss: 162.8883 - val_regression_loss: 65.0785\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1357.3673 - regression_loss: 309.5123 - val_loss: 161.8283 - val_regression_loss: 64.3498\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1354.3927 - regression_loss: 306.8615 - val_loss: 161.5045 - val_regression_loss: 64.1325\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1353.9027 - regression_loss: 304.7840 - val_loss: 162.3049 - val_regression_loss: 64.5653\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1345.8607 - regression_loss: 303.0498 - val_loss: 163.2575 - val_regression_loss: 65.1463\n",
            "***************************** elapsed_time is:  6.259319305419922\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 192ms/step - loss: 17015.7773 - regression_loss: 4517.2734 - val_loss: 1411.4980 - val_regression_loss: 681.0356\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 11289.5625 - regression_loss: 2989.4331 - val_loss: 871.1877 - val_regression_loss: 413.9199\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 6854.7534 - regression_loss: 1782.4626 - val_loss: 564.0036 - val_regression_loss: 263.9550\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 4422.1201 - regression_loss: 1124.3331 - val_loss: 416.6861 - val_regression_loss: 191.0623\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 3179.7888 - regression_loss: 785.8620 - val_loss: 270.2200 - val_regression_loss: 115.6344\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1956.4192 - regression_loss: 460.0943 - val_loss: 308.1902 - val_regression_loss: 131.9192\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2283.2708 - regression_loss: 545.6603 - val_loss: 369.1148 - val_regression_loss: 161.5685\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 2792.5811 - regression_loss: 660.9360 - val_loss: 306.0843 - val_regression_loss: 131.2098\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2286.4954 - regression_loss: 535.1716 - val_loss: 213.1144 - val_regression_loss: 86.6624\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1644.5487 - regression_loss: 377.4442 - val_loss: 166.8486 - val_regression_loss: 65.4809\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1445.3344 - regression_loss: 326.2282 - val_loss: 174.2243 - val_regression_loss: 70.6404\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1585.0327 - regression_loss: 381.1709 - val_loss: 188.5301 - val_regression_loss: 78.5078\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1786.2173 - regression_loss: 423.5835 - val_loss: 178.5898 - val_regression_loss: 73.5202\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1634.2297 - regression_loss: 391.3094 - val_loss: 164.1386 - val_regression_loss: 65.8322\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 1468.6667 - regression_loss: 339.5330 - val_loss: 162.8352 - val_regression_loss: 64.5357\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1343.2281 - regression_loss: 318.9876 - val_loss: 169.6267 - val_regression_loss: 67.2476\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1397.6886 - regression_loss: 318.0159 - val_loss: 175.6200 - val_regression_loss: 69.6653\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1422.2212 - regression_loss: 323.2558 - val_loss: 176.6298 - val_regression_loss: 69.8681\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1430.1012 - regression_loss: 325.3473 - val_loss: 169.8189 - val_regression_loss: 66.6013\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1422.9526 - regression_loss: 318.1429 - val_loss: 158.2055 - val_regression_loss: 61.3632\n",
            "Epoch 21/50\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 1426.5717 - regression_loss: 578.5799\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1363.4537 - regression_loss: 304.0765 - val_loss: 149.8846 - val_regression_loss: 57.8617\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1319.6049 - regression_loss: 299.0223 - val_loss: 147.8118 - val_regression_loss: 57.0895\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1329.3085 - regression_loss: 300.2929 - val_loss: 146.6675 - val_regression_loss: 56.7050\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1316.5328 - regression_loss: 301.6534 - val_loss: 145.9575 - val_regression_loss: 56.4431\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1352.6437 - regression_loss: 302.1363 - val_loss: 145.2491 - val_regression_loss: 56.0979\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1325.0708 - regression_loss: 300.8452 - val_loss: 144.5938 - val_regression_loss: 55.7108\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1302.5424 - regression_loss: 298.4162 - val_loss: 144.0890 - val_regression_loss: 55.3461\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1282.2825 - regression_loss: 295.8571 - val_loss: 143.8678 - val_regression_loss: 55.1012\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 1327.4675 - regression_loss: 293.2789 - val_loss: 143.6495 - val_regression_loss: 54.8746\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1312.6376 - regression_loss: 291.8547 - val_loss: 143.4694 - val_regression_loss: 54.6881\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1284.6606 - regression_loss: 290.6203 - val_loss: 143.1976 - val_regression_loss: 54.4950\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1280.5076 - regression_loss: 290.0112 - val_loss: 142.7186 - val_regression_loss: 54.2383\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1287.8495 - regression_loss: 289.3602 - val_loss: 142.0079 - val_regression_loss: 53.9200\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 1283.4922 - regression_loss: 288.6523 - val_loss: 141.1113 - val_regression_loss: 53.5357\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1309.2705 - regression_loss: 287.8932 - val_loss: 140.1705 - val_regression_loss: 53.1452\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1264.0029 - regression_loss: 286.8827 - val_loss: 139.4294 - val_regression_loss: 52.8393\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1240.6135 - regression_loss: 286.3048 - val_loss: 138.8172 - val_regression_loss: 52.6063\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1290.2815 - regression_loss: 285.8786 - val_loss: 138.2827 - val_regression_loss: 52.3927\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 1261.9675 - regression_loss: 285.3976 - val_loss: 137.8070 - val_regression_loss: 52.1814\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1262.9575 - regression_loss: 284.9538 - val_loss: 137.1317 - val_regression_loss: 51.8583\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1228.1445 - regression_loss: 284.1161 - val_loss: 136.4808 - val_regression_loss: 51.5183\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1269.2429 - regression_loss: 283.3005 - val_loss: 136.0089 - val_regression_loss: 51.2585\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1271.8260 - regression_loss: 282.3439 - val_loss: 135.4744 - val_regression_loss: 50.9620\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1281.6937 - regression_loss: 281.5443 - val_loss: 135.1071 - val_regression_loss: 50.7536\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 1273.7623 - regression_loss: 280.6786 - val_loss: 134.6848 - val_regression_loss: 50.5420\n",
            "Epoch 46/50\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 1296.9503 - regression_loss: 524.2147\n",
            "Epoch 00046: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1240.5822 - regression_loss: 280.0059 - val_loss: 134.1962 - val_regression_loss: 50.3111\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1253.6591 - regression_loss: 279.4069 - val_loss: 133.9630 - val_regression_loss: 50.2084\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 1268.2863 - regression_loss: 279.0733 - val_loss: 133.7041 - val_regression_loss: 50.0915\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1235.4978 - regression_loss: 278.7615 - val_loss: 133.4964 - val_regression_loss: 49.9979\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1262.3916 - regression_loss: 278.4631 - val_loss: 133.2595 - val_regression_loss: 49.8953\n",
            "***************************** elapsed_time is:  6.081318616867065\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 193ms/step - loss: 20557.0078 - regression_loss: 5559.2363 - val_loss: 1968.5260 - val_regression_loss: 972.0548\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 14752.3506 - regression_loss: 3905.3013 - val_loss: 1297.5128 - val_regression_loss: 627.2374\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 9741.0996 - regression_loss: 2519.3108 - val_loss: 744.1866 - val_regression_loss: 337.8274\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 6017.5059 - regression_loss: 1453.7407 - val_loss: 533.1657 - val_regression_loss: 224.4157\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 4638.3423 - regression_loss: 1084.2589 - val_loss: 317.8796 - val_regression_loss: 126.5502\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2897.8086 - regression_loss: 635.6120 - val_loss: 283.4332 - val_regression_loss: 125.5123\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 2182.4373 - regression_loss: 557.8483 - val_loss: 459.2549 - val_regression_loss: 224.7597\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 3285.6758 - regression_loss: 877.8525 - val_loss: 451.7715 - val_regression_loss: 221.7126\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 3229.4231 - regression_loss: 848.3882 - val_loss: 306.3773 - val_regression_loss: 144.0185\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2241.8550 - regression_loss: 568.6760 - val_loss: 200.0332 - val_regression_loss: 84.3830\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 1680.0751 - regression_loss: 396.0758 - val_loss: 178.9573 - val_regression_loss: 68.4668\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1794.7177 - regression_loss: 398.3465 - val_loss: 189.9271 - val_regression_loss: 71.2100\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1971.2885 - regression_loss: 435.8695 - val_loss: 191.1519 - val_regression_loss: 71.8928\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1940.2859 - regression_loss: 418.6153 - val_loss: 191.7050 - val_regression_loss: 74.1257\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1779.6116 - regression_loss: 386.6892 - val_loss: 195.0255 - val_regression_loss: 78.5072\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1619.6783 - regression_loss: 362.5927 - val_loss: 191.3444 - val_regression_loss: 79.3315\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1536.2351 - regression_loss: 347.8542 - val_loss: 184.6004 - val_regression_loss: 78.2851\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1524.9656 - regression_loss: 349.4980 - val_loss: 183.1746 - val_regression_loss: 79.1254\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1571.4458 - regression_loss: 370.1208 - val_loss: 181.7159 - val_regression_loss: 78.7920\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1549.3536 - regression_loss: 376.3226 - val_loss: 175.5386 - val_regression_loss: 74.8972\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1500.5763 - regression_loss: 352.4258 - val_loss: 171.1674 - val_regression_loss: 71.1377\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1432.6071 - regression_loss: 324.9731 - val_loss: 171.2264 - val_regression_loss: 69.4886\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1378.4371 - regression_loss: 314.0337 - val_loss: 168.4314 - val_regression_loss: 66.6752\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 1400.7394 - regression_loss: 308.3590 - val_loss: 162.4769 - val_regression_loss: 62.8981\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1391.3612 - regression_loss: 304.4037 - val_loss: 158.2493 - val_regression_loss: 60.7034\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1376.1217 - regression_loss: 302.5821 - val_loss: 157.6139 - val_regression_loss: 61.0739\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 1379.0045 - regression_loss: 300.5732 - val_loss: 160.5504 - val_regression_loss: 63.6702\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1349.3312 - regression_loss: 299.5497 - val_loss: 165.0894 - val_regression_loss: 66.9648\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1339.6208 - regression_loss: 304.4985 - val_loss: 166.8167 - val_regression_loss: 68.4262\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 1340.8956 - regression_loss: 308.0582 - val_loss: 164.1025 - val_regression_loss: 67.0571\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1307.3813 - regression_loss: 306.5776 - val_loss: 160.0211 - val_regression_loss: 64.4718\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1303.5472 - regression_loss: 301.5963 - val_loss: 158.7039 - val_regression_loss: 63.1748\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1293.9379 - regression_loss: 296.9844 - val_loss: 159.7656 - val_regression_loss: 63.1824\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1324.7264 - regression_loss: 292.8161 - val_loss: 161.5137 - val_regression_loss: 63.6490\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1324.4313 - regression_loss: 291.3680 - val_loss: 160.3934 - val_regression_loss: 62.7875\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1313.5369 - regression_loss: 289.2744 - val_loss: 157.9716 - val_regression_loss: 61.5906\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1284.1207 - regression_loss: 288.3902 - val_loss: 156.1010 - val_regression_loss: 60.9067\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1276.2294 - regression_loss: 288.4544 - val_loss: 156.2011 - val_regression_loss: 61.3481\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1294.6395 - regression_loss: 289.5857 - val_loss: 157.3630 - val_regression_loss: 62.2506\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1284.5787 - regression_loss: 290.9971 - val_loss: 156.5601 - val_regression_loss: 61.8596\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1299.7441 - regression_loss: 290.3494 - val_loss: 154.8904 - val_regression_loss: 60.6604\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1259.3224 - regression_loss: 287.8589 - val_loss: 154.0012 - val_regression_loss: 59.8520\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1294.8285 - regression_loss: 286.0140 - val_loss: 154.7240 - val_regression_loss: 60.0538\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1282.8763 - regression_loss: 284.3138 - val_loss: 154.1388 - val_regression_loss: 59.7062\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1254.9393 - regression_loss: 283.5954 - val_loss: 152.9167 - val_regression_loss: 59.0628\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1267.4883 - regression_loss: 282.7785 - val_loss: 151.9216 - val_regression_loss: 58.6983\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1288.1418 - regression_loss: 282.6141 - val_loss: 151.0623 - val_regression_loss: 58.3509\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1262.6324 - regression_loss: 282.4944 - val_loss: 150.7472 - val_regression_loss: 58.3059\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1257.5472 - regression_loss: 282.3730 - val_loss: 149.9568 - val_regression_loss: 57.8971\n",
            "Epoch 50/50\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 1321.8171 - regression_loss: 541.0420\n",
            "Epoch 00050: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1263.1050 - regression_loss: 281.7288 - val_loss: 149.8549 - val_regression_loss: 57.8261\n",
            "***************************** elapsed_time is:  6.137404203414917\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 1s 337ms/step - loss: 12790.8018 - regression_loss: 3378.3667 - val_loss: 1103.2864 - val_regression_loss: 528.8266\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 8740.8740 - regression_loss: 2279.0833 - val_loss: 749.7264 - val_regression_loss: 353.4507\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 5562.6650 - regression_loss: 1421.0944 - val_loss: 499.1590 - val_regression_loss: 229.6752\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 3404.1350 - regression_loss: 829.5353 - val_loss: 373.3457 - val_regression_loss: 167.6508\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2472.3889 - regression_loss: 590.9892 - val_loss: 248.6742 - val_regression_loss: 105.0098\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1921.2513 - regression_loss: 454.7061 - val_loss: 239.9356 - val_regression_loss: 100.0356\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 2274.3159 - regression_loss: 537.7982 - val_loss: 248.1808 - val_regression_loss: 104.4778\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2247.6516 - regression_loss: 540.7216 - val_loss: 206.0033 - val_regression_loss: 84.4234\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1711.4993 - regression_loss: 392.1198 - val_loss: 186.7505 - val_regression_loss: 75.9480\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 1398.7629 - regression_loss: 311.5143 - val_loss: 210.9013 - val_regression_loss: 88.9319\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1505.1920 - regression_loss: 348.8032 - val_loss: 234.5030 - val_regression_loss: 101.2248\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1641.9874 - regression_loss: 395.0364 - val_loss: 221.9015 - val_regression_loss: 95.0138\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 1578.4424 - regression_loss: 368.2809 - val_loss: 195.4903 - val_regression_loss: 81.6654\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 1404.4054 - regression_loss: 317.7680 - val_loss: 178.0351 - val_regression_loss: 72.6995\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1309.2920 - regression_loss: 296.4725 - val_loss: 170.2386 - val_regression_loss: 68.5424\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1327.2274 - regression_loss: 300.8531 - val_loss: 167.3922 - val_regression_loss: 66.9162\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 1373.8337 - regression_loss: 310.3339 - val_loss: 166.2559 - val_regression_loss: 66.3122\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 1412.3669 - regression_loss: 310.7148 - val_loss: 165.3061 - val_regression_loss: 66.0088\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1338.6315 - regression_loss: 296.5421 - val_loss: 167.6822 - val_regression_loss: 67.4614\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1270.8274 - regression_loss: 284.1475 - val_loss: 174.1462 - val_regression_loss: 70.9264\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1255.8551 - regression_loss: 284.0963 - val_loss: 180.4807 - val_regression_loss: 74.2329\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1283.5675 - regression_loss: 289.3779 - val_loss: 182.0242 - val_regression_loss: 75.0429\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1305.2666 - regression_loss: 290.6004 - val_loss: 178.0857 - val_regression_loss: 73.0259\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1267.8090 - regression_loss: 285.3100 - val_loss: 171.6746 - val_regression_loss: 69.7085\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1261.8004 - regression_loss: 278.1019 - val_loss: 166.2520 - val_regression_loss: 66.8668\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 1241.7460 - regression_loss: 274.7635 - val_loss: 163.2464 - val_regression_loss: 65.2605\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1247.6195 - regression_loss: 275.0404 - val_loss: 162.0436 - val_regression_loss: 64.6082\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1231.8240 - regression_loss: 275.2153 - val_loss: 161.9553 - val_regression_loss: 64.5818\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1256.5339 - regression_loss: 273.2361 - val_loss: 163.0874 - val_regression_loss: 65.2188\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1240.5100 - regression_loss: 270.9948 - val_loss: 165.3161 - val_regression_loss: 66.4153\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1209.2253 - regression_loss: 270.2166 - val_loss: 167.3957 - val_regression_loss: 67.5154\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1216.8190 - regression_loss: 270.6723 - val_loss: 168.1465 - val_regression_loss: 67.9106\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1211.2980 - regression_loss: 270.3044 - val_loss: 167.4804 - val_regression_loss: 67.5559\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1224.3323 - regression_loss: 268.8329 - val_loss: 165.4217 - val_regression_loss: 66.4865\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1193.0288 - regression_loss: 267.5034 - val_loss: 163.2394 - val_regression_loss: 65.3462\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1221.9595 - regression_loss: 266.4930 - val_loss: 161.9559 - val_regression_loss: 64.6812\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1208.2808 - regression_loss: 266.1517 - val_loss: 161.5055 - val_regression_loss: 64.4505\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1191.8770 - regression_loss: 265.6811 - val_loss: 162.1505 - val_regression_loss: 64.7891\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1194.7054 - regression_loss: 264.8049 - val_loss: 163.0105 - val_regression_loss: 65.2428\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1206.7682 - regression_loss: 264.3767 - val_loss: 163.5351 - val_regression_loss: 65.5213\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1206.8354 - regression_loss: 264.1143 - val_loss: 163.3724 - val_regression_loss: 65.4464\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1163.5969 - regression_loss: 263.8524 - val_loss: 162.3192 - val_regression_loss: 64.9000\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 1189.0508 - regression_loss: 262.9911 - val_loss: 161.5092 - val_regression_loss: 64.4754\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1191.9813 - regression_loss: 262.3997 - val_loss: 160.9009 - val_regression_loss: 64.1536\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1173.3508 - regression_loss: 262.0752 - val_loss: 160.4000 - val_regression_loss: 63.8929\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1176.6862 - regression_loss: 261.5815 - val_loss: 160.0862 - val_regression_loss: 63.7275\n",
            "Epoch 47/50\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 1229.8732 - regression_loss: 494.7762\n",
            "Epoch 00047: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 1175.9344 - regression_loss: 261.2504 - val_loss: 160.0004 - val_regression_loss: 63.6848\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1178.2812 - regression_loss: 260.8749 - val_loss: 160.0565 - val_regression_loss: 63.7118\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1176.1418 - regression_loss: 260.6996 - val_loss: 160.2661 - val_regression_loss: 63.8200\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 1179.9810 - regression_loss: 260.5884 - val_loss: 160.4624 - val_regression_loss: 63.9191\n",
            "***************************** elapsed_time is:  6.352810382843018\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 182ms/step - loss: 2087201.8750 - regression_loss: 585099.5000 - val_loss: 267213.7188 - val_regression_loss: 133575.3906\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 2024294.0000 - regression_loss: 561962.4375 - val_loss: 254569.7188 - val_regression_loss: 127258.1094\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1931276.5000 - regression_loss: 535755.8750 - val_loss: 236882.8594 - val_regression_loss: 118419.6484\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1836601.7500 - regression_loss: 499400.9375 - val_loss: 213272.5625 - val_regression_loss: 106619.2969\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1659882.7500 - regression_loss: 451229.8750 - val_loss: 184061.0938 - val_regression_loss: 92017.4688\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1432589.2500 - regression_loss: 391692.4688 - val_loss: 150307.1406 - val_regression_loss: 75142.6875\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1177276.0000 - regression_loss: 323713.6875 - val_loss: 113731.6797 - val_regression_loss: 56854.7812\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 861280.0000 - regression_loss: 250183.9219 - val_loss: 77293.3984 - val_regression_loss: 38631.8320\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 628655.5000 - regression_loss: 179424.5938 - val_loss: 47385.0859 - val_regression_loss: 23667.1562\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 467519.8750 - regression_loss: 125676.7188 - val_loss: 35764.8828 - val_regression_loss: 17839.3379\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 401494.0938 - regression_loss: 108438.8594 - val_loss: 45290.1211 - val_regression_loss: 22591.6074\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 465040.7188 - regression_loss: 132616.2500 - val_loss: 47481.8477 - val_regression_loss: 23693.8203\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 483152.7188 - regression_loss: 135897.1562 - val_loss: 38652.1914 - val_regression_loss: 19291.4180\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 423583.3750 - regression_loss: 114366.4297 - val_loss: 31034.1914 - val_regression_loss: 15492.7891\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 356179.5938 - regression_loss: 95863.9219 - val_loss: 29414.9512 - val_regression_loss: 14689.8711\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 333527.7188 - regression_loss: 89893.3984 - val_loss: 31498.3574 - val_regression_loss: 15735.1963\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 338628.0938 - regression_loss: 92322.2969 - val_loss: 33609.3359 - val_regression_loss: 16792.2012\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 352890.8438 - regression_loss: 95345.6016 - val_loss: 33711.6172 - val_regression_loss: 16843.4961\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 327449.7188 - regression_loss: 95090.5156 - val_loss: 31685.1055 - val_regression_loss: 15829.5977\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 335174.9062 - regression_loss: 91278.5938 - val_loss: 28296.6543 - val_regression_loss: 14134.4043\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 312482.5312 - regression_loss: 84916.7969 - val_loss: 25000.5195 - val_regression_loss: 12485.5654\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 284126.8750 - regression_loss: 78991.4922 - val_loss: 23020.8301 - val_regression_loss: 11495.5645\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 273540.7188 - regression_loss: 75546.3281 - val_loss: 22715.4766 - val_regression_loss: 11343.4277\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 276454.5000 - regression_loss: 74732.4531 - val_loss: 23120.4824 - val_regression_loss: 11546.3916\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 276140.6875 - regression_loss: 74570.9453 - val_loss: 22899.5234 - val_regression_loss: 11435.3281\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 251151.2031 - regression_loss: 72638.0156 - val_loss: 21699.2188 - val_regression_loss: 10833.2520\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 253964.6875 - regression_loss: 69002.5234 - val_loss: 20462.8320 - val_regression_loss: 10212.6875\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 243242.8438 - regression_loss: 65466.7148 - val_loss: 19836.7129 - val_regression_loss: 9898.3584\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 227961.4219 - regression_loss: 63320.5273 - val_loss: 19736.4668 - val_regression_loss: 9848.7861\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 230056.7812 - regression_loss: 61782.9062 - val_loss: 19767.2891 - val_regression_loss: 9866.9434\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 222180.1562 - regression_loss: 59906.1641 - val_loss: 19673.3770 - val_regression_loss: 9824.6816\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 214274.9062 - regression_loss: 57343.6406 - val_loss: 19559.5059 - val_regression_loss: 9773.7871\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 203384.7969 - regression_loss: 54578.5625 - val_loss: 19526.6348 - val_regression_loss: 9763.8555\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 187902.5625 - regression_loss: 51955.6367 - val_loss: 19376.9023 - val_regression_loss: 9693.8008\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 182703.5781 - regression_loss: 49878.7734 - val_loss: 18800.1641 - val_regression_loss: 9407.2510\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 177105.3125 - regression_loss: 47553.0352 - val_loss: 17709.2988 - val_regression_loss: 8859.1846\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 166859.1719 - regression_loss: 44874.2109 - val_loss: 16386.6309 - val_regression_loss: 8191.9629\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 139069.4219 - regression_loss: 42257.6836 - val_loss: 15201.2119 - val_regression_loss: 7593.1250\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 148048.1875 - regression_loss: 40011.6562 - val_loss: 14263.2822 - val_regression_loss: 7121.4062\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 124145.3281 - regression_loss: 37547.3750 - val_loss: 13476.9590 - val_regression_loss: 6729.0391\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 128452.5859 - regression_loss: 35083.2031 - val_loss: 12955.5586 - val_regression_loss: 6472.2568\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 123363.2734 - regression_loss: 32755.2930 - val_loss: 12570.5674 - val_regression_loss: 6284.5405\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 113955.1328 - regression_loss: 30531.1797 - val_loss: 11807.9844 - val_regression_loss: 5906.1094\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 104504.6094 - regression_loss: 28333.5605 - val_loss: 10701.2139 - val_regression_loss: 5353.8105\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 97004.1016 - regression_loss: 26309.4023 - val_loss: 9602.8643 - val_regression_loss: 4804.3281\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 92889.3125 - regression_loss: 24612.5566 - val_loss: 8991.8057 - val_regression_loss: 4498.0112\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 74552.2734 - regression_loss: 22725.6797 - val_loss: 8567.2959 - val_regression_loss: 4283.4438\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 78807.6875 - regression_loss: 21208.4199 - val_loss: 8389.2090 - val_regression_loss: 4191.0078\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 71366.6484 - regression_loss: 19841.9492 - val_loss: 7553.9575 - val_regression_loss: 3769.1143\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 69029.7656 - regression_loss: 18546.8145 - val_loss: 6862.7280 - val_regression_loss: 3419.8755\n",
            "***************************** elapsed_time is:  6.10779333114624\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 173ms/step - loss: 548265.8125 - regression_loss: 149581.6562 - val_loss: 57338.9922 - val_regression_loss: 28617.9199\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 504413.6250 - regression_loss: 137210.8906 - val_loss: 51768.2070 - val_regression_loss: 25836.2617\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 454421.7500 - regression_loss: 123972.1797 - val_loss: 44300.0664 - val_regression_loss: 22107.9375\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 396291.5312 - regression_loss: 106416.0000 - val_loss: 34946.0000 - val_regression_loss: 17438.0977\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 309588.0000 - regression_loss: 84373.4375 - val_loss: 24671.2188 - val_regression_loss: 12308.4775\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 223484.7500 - regression_loss: 60512.8242 - val_loss: 15376.5195 - val_regression_loss: 7668.4092\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 142741.4688 - regression_loss: 38648.6445 - val_loss: 9669.3535 - val_regression_loss: 4820.6162\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 90451.1562 - regression_loss: 25161.5605 - val_loss: 9719.4365 - val_regression_loss: 4848.9282\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 90450.0000 - regression_loss: 24769.1816 - val_loss: 12113.7822 - val_regression_loss: 6045.5278\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 111085.0781 - regression_loss: 29873.5430 - val_loss: 11023.2031 - val_regression_loss: 5496.8594\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 101126.0078 - regression_loss: 27625.1523 - val_loss: 7918.3584 - val_regression_loss: 3941.1589\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 77567.5234 - regression_loss: 21318.0195 - val_loss: 5553.3843 - val_regression_loss: 2756.7366\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 61030.7188 - regression_loss: 16616.4258 - val_loss: 4949.9150 - val_regression_loss: 2453.9067\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 58878.9258 - regression_loss: 15890.6768 - val_loss: 5556.9326 - val_regression_loss: 2756.6592\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 64635.8867 - regression_loss: 17446.8750 - val_loss: 6166.5688 - val_regression_loss: 3061.2209\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 68349.2812 - regression_loss: 18672.3828 - val_loss: 5932.8213 - val_regression_loss: 2944.8394\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 67540.5703 - regression_loss: 17941.6191 - val_loss: 5014.2642 - val_regression_loss: 2486.6584\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 54222.7969 - regression_loss: 15722.6250 - val_loss: 4150.6016 - val_regression_loss: 2056.1531\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 52298.6445 - regression_loss: 13846.9902 - val_loss: 3810.7554 - val_regression_loss: 1887.4686\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 48987.8164 - regression_loss: 13116.9336 - val_loss: 3931.9661 - val_regression_loss: 1949.0139\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 48708.8984 - regression_loss: 13371.4326 - val_loss: 4047.8030 - val_regression_loss: 2007.4492\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 47402.2812 - regression_loss: 13586.7383 - val_loss: 3845.9910 - val_regression_loss: 1906.6046\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 48761.4727 - regression_loss: 13048.1133 - val_loss: 3445.6531 - val_regression_loss: 1706.0811\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 44022.5625 - regression_loss: 12005.3896 - val_loss: 3169.2170 - val_regression_loss: 1567.2537\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 41099.6992 - regression_loss: 11192.9268 - val_loss: 3126.0027 - val_regression_loss: 1544.9955\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 40599.1641 - regression_loss: 10837.4014 - val_loss: 3148.9717 - val_regression_loss: 1556.0077\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 39811.2461 - regression_loss: 10659.5547 - val_loss: 3054.2278 - val_regression_loss: 1508.4453\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 35751.1992 - regression_loss: 10232.0488 - val_loss: 2818.4297 - val_regression_loss: 1390.6511\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 35642.5820 - regression_loss: 9606.6045 - val_loss: 2572.8127 - val_regression_loss: 1268.1549\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 33526.5391 - regression_loss: 8982.6846 - val_loss: 2437.3838 - val_regression_loss: 1200.7897\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 31323.4824 - regression_loss: 8561.8975 - val_loss: 2360.3630 - val_regression_loss: 1162.5251\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 30219.5625 - regression_loss: 8244.0908 - val_loss: 2247.3860 - val_regression_loss: 1106.1077\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 29063.3281 - regression_loss: 7776.7368 - val_loss: 2109.2354 - val_regression_loss: 1036.9254\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 27215.2070 - regression_loss: 7226.1138 - val_loss: 2020.3700 - val_regression_loss: 992.3104\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 25503.3086 - regression_loss: 6773.3306 - val_loss: 1969.2065 - val_regression_loss: 966.6382\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 24298.9609 - regression_loss: 6436.4824 - val_loss: 1876.6615 - val_regression_loss: 920.5049\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 22770.6348 - regression_loss: 6040.3428 - val_loss: 1757.4752 - val_regression_loss: 861.2380\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 20640.9316 - regression_loss: 5585.9219 - val_loss: 1673.0641 - val_regression_loss: 819.3907\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 19296.4082 - regression_loss: 5215.0869 - val_loss: 1619.0492 - val_regression_loss: 792.5762\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 18634.8574 - regression_loss: 4914.1084 - val_loss: 1547.4120 - val_regression_loss: 756.7241\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 15899.1006 - regression_loss: 4571.2607 - val_loss: 1461.1659 - val_regression_loss: 713.3521\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 15902.7725 - regression_loss: 4253.8862 - val_loss: 1404.4794 - val_regression_loss: 684.8405\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 14559.4355 - regression_loss: 4005.5884 - val_loss: 1350.3429 - val_regression_loss: 657.8207\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 13322.6777 - regression_loss: 3745.0918 - val_loss: 1315.1208 - val_regression_loss: 640.4287\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 13200.0703 - regression_loss: 3521.5171 - val_loss: 1308.8361 - val_regression_loss: 637.5195\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 12399.7266 - regression_loss: 3332.2744 - val_loss: 1265.0974 - val_regression_loss: 615.6870\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 11947.0098 - regression_loss: 3143.0383 - val_loss: 1188.9034 - val_regression_loss: 577.4713\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 11293.8818 - regression_loss: 2959.0325 - val_loss: 1135.3547 - val_regression_loss: 550.6776\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 10735.5742 - regression_loss: 2817.2065 - val_loss: 1103.6914 - val_regression_loss: 535.0212\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 10140.4707 - regression_loss: 2656.4998 - val_loss: 1095.2511 - val_regression_loss: 531.0143\n",
            "***************************** elapsed_time is:  5.944938659667969\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 231ms/step - loss: 257573.2500 - regression_loss: 70234.1328 - val_loss: 26197.3691 - val_regression_loss: 13076.2793\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 226938.1719 - regression_loss: 62012.3711 - val_loss: 22277.4551 - val_regression_loss: 11118.3232\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 193917.1719 - regression_loss: 52860.7422 - val_loss: 17190.7402 - val_regression_loss: 8576.4805\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 152724.0000 - regression_loss: 41024.4023 - val_loss: 11263.3076 - val_regression_loss: 5613.4512\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 100767.6719 - regression_loss: 27152.9941 - val_loss: 5699.4512 - val_regression_loss: 2831.0415\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 52506.5039 - regression_loss: 14077.5449 - val_loss: 2592.9084 - val_regression_loss: 1276.3008\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 24574.5605 - regression_loss: 6630.8135 - val_loss: 3722.8115 - val_regression_loss: 1840.5557\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 31413.8730 - regression_loss: 8561.5361 - val_loss: 4890.8862 - val_regression_loss: 2426.2510\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 40523.4492 - regression_loss: 10957.2832 - val_loss: 3760.8911 - val_regression_loss: 1863.1415\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 31539.2559 - regression_loss: 8593.5430 - val_loss: 2132.7402 - val_regression_loss: 1049.9320\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 19884.8125 - regression_loss: 5337.1392 - val_loss: 1325.3809 - val_regression_loss: 646.4982\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 14360.2490 - regression_loss: 3938.8928 - val_loss: 1363.7595 - val_regression_loss: 665.6876\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 15396.9004 - regression_loss: 4331.1865 - val_loss: 1676.9941 - val_regression_loss: 822.2026\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 19504.4395 - regression_loss: 5162.1338 - val_loss: 1816.6747 - val_regression_loss: 891.8849\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 20787.3652 - regression_loss: 5494.7959 - val_loss: 1685.4315 - val_regression_loss: 826.0693\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 18658.4590 - regression_loss: 5098.3242 - val_loss: 1405.0854 - val_regression_loss: 685.6786\n",
            "Epoch 17/50\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 16824.3809 - regression_loss: 8287.7109\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 16072.5293 - regression_loss: 4308.5308 - val_loss: 1167.2574 - val_regression_loss: 566.5377\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 13497.6816 - regression_loss: 3561.8501 - val_loss: 1113.8385 - val_regression_loss: 539.7179\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 12628.6445 - regression_loss: 3330.0020 - val_loss: 1111.8301 - val_regression_loss: 538.6112\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 11825.5137 - regression_loss: 3204.3977 - val_loss: 1149.9503 - val_regression_loss: 557.5779\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 11889.1367 - regression_loss: 3184.7905 - val_loss: 1201.9214 - val_regression_loss: 583.4782\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 12114.4023 - regression_loss: 3224.6628 - val_loss: 1241.2991 - val_regression_loss: 603.0917\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 11906.2246 - regression_loss: 3254.2786 - val_loss: 1248.8381 - val_regression_loss: 606.7975\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 11962.4424 - regression_loss: 3243.8689 - val_loss: 1222.4110 - val_regression_loss: 593.5392\n",
            "Epoch 25/50\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 12415.4434 - regression_loss: 6075.8359\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 11865.2598 - regression_loss: 3183.6963 - val_loss: 1174.3557 - val_regression_loss: 569.4860\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 11476.2578 - regression_loss: 3094.8188 - val_loss: 1146.6857 - val_regression_loss: 555.6478\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 11093.5986 - regression_loss: 3049.9583 - val_loss: 1117.4431 - val_regression_loss: 541.0341\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 11377.3242 - regression_loss: 3002.4294 - val_loss: 1090.5029 - val_regression_loss: 527.5774\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 11183.2734 - regression_loss: 2963.8101 - val_loss: 1066.9233 - val_regression_loss: 515.8076\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 11000.5068 - regression_loss: 2931.8652 - val_loss: 1047.2115 - val_regression_loss: 505.9766\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 10691.8125 - regression_loss: 2903.9717 - val_loss: 1031.2930 - val_regression_loss: 498.0454\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 10787.7930 - regression_loss: 2884.0671 - val_loss: 1017.9324 - val_regression_loss: 491.3945\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 10836.5381 - regression_loss: 2864.1978 - val_loss: 1006.8056 - val_regression_loss: 485.8605\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 10816.2627 - regression_loss: 2845.0576 - val_loss: 997.0488 - val_regression_loss: 481.0102\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 10310.8203 - regression_loss: 2824.5867 - val_loss: 988.2238 - val_regression_loss: 476.6233\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 10771.8281 - regression_loss: 2802.5828 - val_loss: 980.6066 - val_regression_loss: 472.8357\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 10417.4746 - regression_loss: 2778.3113 - val_loss: 974.0024 - val_regression_loss: 469.5486\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 10348.2412 - regression_loss: 2753.0027 - val_loss: 968.0876 - val_regression_loss: 466.6018\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 9937.9590 - regression_loss: 2726.7578 - val_loss: 963.0555 - val_regression_loss: 464.0892\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 10191.6064 - regression_loss: 2701.4143 - val_loss: 958.5203 - val_regression_loss: 461.8199\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 9969.7656 - regression_loss: 2675.8352 - val_loss: 954.1758 - val_regression_loss: 459.6413\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 10065.7539 - regression_loss: 2650.5808 - val_loss: 949.9322 - val_regression_loss: 457.5078\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 9739.0234 - regression_loss: 2624.6978 - val_loss: 945.5209 - val_regression_loss: 455.2865\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 9975.5518 - regression_loss: 2599.3667 - val_loss: 940.2923 - val_regression_loss: 452.6523\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 9674.9385 - regression_loss: 2572.3838 - val_loss: 934.3400 - val_regression_loss: 449.6569\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 9766.2949 - regression_loss: 2545.8684 - val_loss: 927.9941 - val_regression_loss: 446.4623\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 9600.4863 - regression_loss: 2518.4697 - val_loss: 921.2590 - val_regression_loss: 443.0741\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 9422.8848 - regression_loss: 2490.4312 - val_loss: 914.0411 - val_regression_loss: 439.4457\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 9285.5059 - regression_loss: 2462.3013 - val_loss: 906.1483 - val_regression_loss: 435.4828\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 9156.3057 - regression_loss: 2433.9653 - val_loss: 897.7841 - val_regression_loss: 431.2876\n",
            "***************************** elapsed_time is:  6.467276573181152\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 169ms/step - loss: 577127.4375 - regression_loss: 160058.1719 - val_loss: 51601.9180 - val_regression_loss: 25671.8379\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 529166.1875 - regression_loss: 149976.0625 - val_loss: 47061.0195 - val_regression_loss: 23405.3398\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 516784.1562 - regression_loss: 138213.5781 - val_loss: 40755.4805 - val_regression_loss: 20258.6348\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 443716.7188 - regression_loss: 121673.4453 - val_loss: 32682.0195 - val_regression_loss: 16232.4951\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 377571.1562 - regression_loss: 100952.5781 - val_loss: 23682.1445 - val_regression_loss: 11750.6221\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 290880.6875 - regression_loss: 77366.9688 - val_loss: 15545.6143 - val_regression_loss: 7712.2861\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 199656.7656 - regression_loss: 54820.4062 - val_loss: 11144.2842 - val_regression_loss: 5556.9248\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 148180.5469 - regression_loss: 40117.8711 - val_loss: 13379.3760 - val_regression_loss: 6726.7686\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 137520.2656 - regression_loss: 39528.0195 - val_loss: 16523.5352 - val_regression_loss: 8319.3955\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 161472.7969 - regression_loss: 43913.1367 - val_loss: 14141.9160 - val_regression_loss: 7113.8076\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 139949.8438 - regression_loss: 38583.3398 - val_loss: 9698.1719 - val_regression_loss: 4863.5464\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 111550.6250 - regression_loss: 30428.9688 - val_loss: 6877.6982 - val_regression_loss: 3426.1665\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 96666.3750 - regression_loss: 26349.8398 - val_loss: 6248.2329 - val_regression_loss: 3092.9709\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 96675.8984 - regression_loss: 26668.0762 - val_loss: 6445.6006 - val_regression_loss: 3183.4602\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 101568.0391 - regression_loss: 27867.7051 - val_loss: 6236.8540 - val_regression_loss: 3079.2512\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 98719.2578 - regression_loss: 27260.0176 - val_loss: 5506.3779 - val_regression_loss: 2719.6230\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 93877.1016 - regression_loss: 24745.0527 - val_loss: 4778.8311 - val_regression_loss: 2364.5935\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 77152.5469 - regression_loss: 21454.2402 - val_loss: 4615.9976 - val_regression_loss: 2292.5190\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 68131.1562 - regression_loss: 19432.2383 - val_loss: 5009.4575 - val_regression_loss: 2497.0103\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 69704.5547 - regression_loss: 18836.0625 - val_loss: 5326.5044 - val_regression_loss: 2659.9294\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 59048.7891 - regression_loss: 18450.4707 - val_loss: 4922.5322 - val_regression_loss: 2457.7544\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 59572.5898 - regression_loss: 17297.6348 - val_loss: 4215.6069 - val_regression_loss: 2100.8459\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 58698.5391 - regression_loss: 15795.5557 - val_loss: 3730.0374 - val_regression_loss: 1853.0028\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 55514.6055 - regression_loss: 14913.3984 - val_loss: 3547.2791 - val_regression_loss: 1756.9995\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 54672.5703 - regression_loss: 14558.7100 - val_loss: 3397.6343 - val_regression_loss: 1679.7366\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 51503.7031 - regression_loss: 13911.7305 - val_loss: 3173.8464 - val_regression_loss: 1567.7070\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 46134.0664 - regression_loss: 12781.6924 - val_loss: 3042.8906 - val_regression_loss: 1503.0273\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 42651.3711 - regression_loss: 11837.5527 - val_loss: 3003.3315 - val_regression_loss: 1483.5057\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 41889.3789 - regression_loss: 11289.7578 - val_loss: 2810.0210 - val_regression_loss: 1385.8745\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 38123.5664 - regression_loss: 10677.7676 - val_loss: 2420.3093 - val_regression_loss: 1189.1227\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 36936.1055 - regression_loss: 9918.0654 - val_loss: 2081.4312 - val_regression_loss: 1018.1129\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 32621.2109 - regression_loss: 9306.9180 - val_loss: 1877.6085 - val_regression_loss: 916.1527\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 33025.5078 - regression_loss: 8851.0645 - val_loss: 1772.6097 - val_regression_loss: 865.7419\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 31139.9824 - regression_loss: 8291.9336 - val_loss: 1777.0907 - val_regression_loss: 871.5337\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 27670.4023 - regression_loss: 7694.6138 - val_loss: 1841.1061 - val_regression_loss: 906.7051\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 26985.1836 - regression_loss: 7282.3833 - val_loss: 1826.8521 - val_regression_loss: 900.9863\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 25416.4219 - regression_loss: 6819.0312 - val_loss: 1735.0670 - val_regression_loss: 854.9825\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 24043.6914 - regression_loss: 6383.5337 - val_loss: 1650.1793 - val_regression_loss: 811.7534\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 22481.5312 - regression_loss: 6011.6880 - val_loss: 1614.9418 - val_regression_loss: 794.0156\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 21403.0703 - regression_loss: 5662.7417 - val_loss: 1646.4916 - val_regression_loss: 810.6285\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 19827.1875 - regression_loss: 5277.3940 - val_loss: 1685.0974 - val_regression_loss: 830.5627\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 18201.1191 - regression_loss: 4939.2422 - val_loss: 1604.1213 - val_regression_loss: 789.4472\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 17165.0039 - regression_loss: 4600.3164 - val_loss: 1475.5593 - val_regression_loss: 724.1714\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 16042.9219 - regression_loss: 4272.5386 - val_loss: 1383.7933 - val_regression_loss: 678.0286\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 13711.1748 - regression_loss: 3969.9517 - val_loss: 1327.2191 - val_regression_loss: 650.1363\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 13742.2227 - regression_loss: 3687.9871 - val_loss: 1342.4348 - val_regression_loss: 658.9962\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 12935.8262 - regression_loss: 3426.4114 - val_loss: 1322.9359 - val_regression_loss: 649.9799\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 11961.8486 - regression_loss: 3186.5483 - val_loss: 1217.4836 - val_regression_loss: 597.0171\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 11300.4492 - regression_loss: 2954.7026 - val_loss: 1126.9058 - val_regression_loss: 551.3470\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 10608.9717 - regression_loss: 2769.2939 - val_loss: 1109.2911 - val_regression_loss: 542.8184\n",
            "***************************** elapsed_time is:  6.402012825012207\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 198ms/step - loss: 1219985.5000 - regression_loss: 340037.0312 - val_loss: 83431.8203 - val_regression_loss: 41671.2461\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1166194.0000 - regression_loss: 323096.7188 - val_loss: 77139.1016 - val_regression_loss: 38529.0469\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1107539.3750 - regression_loss: 304850.6875 - val_loss: 68712.5078 - val_regression_loss: 34320.6836\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 1029683.7500 - regression_loss: 280468.9062 - val_loss: 57921.4609 - val_regression_loss: 28930.7695\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 918465.1875 - regression_loss: 248973.4531 - val_loss: 45326.8398 - val_regression_loss: 22639.2734\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 792916.8750 - regression_loss: 211671.3125 - val_loss: 32402.7793 - val_regression_loss: 16182.4160\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 639942.1250 - regression_loss: 170943.7344 - val_loss: 21646.2617 - val_regression_loss: 10807.8477\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 474436.0312 - regression_loss: 131407.7031 - val_loss: 16999.4629 - val_regression_loss: 8486.1396\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 380388.3125 - regression_loss: 102769.0625 - val_loss: 23233.0957 - val_regression_loss: 11602.4512\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 353544.3125 - regression_loss: 95479.6562 - val_loss: 35286.6914 - val_regression_loss: 17626.6309\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 385818.0312 - regression_loss: 104823.3828 - val_loss: 34931.1055 - val_regression_loss: 17445.7031\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 375105.1562 - regression_loss: 100764.5469 - val_loss: 25603.0918 - val_regression_loss: 12780.1416\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 313017.3125 - regression_loss: 85280.4141 - val_loss: 16583.9570 - val_regression_loss: 8270.7080\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 271075.3125 - regression_loss: 73963.7500 - val_loss: 11894.1318 - val_regression_loss: 5926.2041\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 267741.9375 - regression_loss: 71433.4922 - val_loss: 10930.5791 - val_regression_loss: 5444.5078\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 260427.9531 - regression_loss: 74011.1641 - val_loss: 11244.2832 - val_regression_loss: 5601.4360\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 279806.1250 - regression_loss: 74689.2344 - val_loss: 10933.6191 - val_regression_loss: 5446.4858\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 266690.7188 - regression_loss: 70826.3594 - val_loss: 10309.4131 - val_regression_loss: 5134.9717\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 242257.7344 - regression_loss: 64274.6016 - val_loss: 10619.5537 - val_regression_loss: 5290.6035\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 213718.5000 - regression_loss: 58670.7891 - val_loss: 12191.6182 - val_regression_loss: 6076.9971\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 193226.5156 - regression_loss: 56031.0430 - val_loss: 13433.3115 - val_regression_loss: 6697.9756\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 208105.0781 - regression_loss: 55313.2734 - val_loss: 12831.7998 - val_regression_loss: 6397.1587\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 196040.6250 - regression_loss: 52798.4219 - val_loss: 10464.3301 - val_regression_loss: 5213.2368\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 182966.0938 - regression_loss: 48597.8047 - val_loss: 7956.2822 - val_regression_loss: 3958.9421\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 165383.7969 - regression_loss: 44995.0859 - val_loss: 6422.0630 - val_regression_loss: 3191.5400\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 154553.3750 - regression_loss: 42857.2031 - val_loss: 5721.6489 - val_regression_loss: 2841.1328\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 153669.6562 - regression_loss: 41004.5508 - val_loss: 5441.7178 - val_regression_loss: 2701.1357\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 140055.1250 - regression_loss: 37903.0938 - val_loss: 5594.0088 - val_regression_loss: 2777.3721\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 130603.5469 - regression_loss: 34773.4219 - val_loss: 6312.6270 - val_regression_loss: 3136.7910\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 111175.0469 - regression_loss: 32282.4297 - val_loss: 6690.1562 - val_regression_loss: 3325.5715\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 113756.9062 - regression_loss: 30673.2871 - val_loss: 6237.0864 - val_regression_loss: 3098.9612\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 105665.2031 - regression_loss: 28397.3574 - val_loss: 5123.6841 - val_regression_loss: 2542.1326\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 83545.8125 - regression_loss: 26063.4727 - val_loss: 4268.3501 - val_regression_loss: 2114.4077\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 90097.5859 - regression_loss: 24402.2559 - val_loss: 4040.2578 - val_regression_loss: 2000.4851\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 84019.0625 - regression_loss: 22662.0938 - val_loss: 4194.9429 - val_regression_loss: 2078.1052\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 68294.1797 - regression_loss: 20528.7148 - val_loss: 4445.5640 - val_regression_loss: 2203.7366\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 71408.0234 - regression_loss: 19160.6992 - val_loss: 4628.3516 - val_regression_loss: 2295.4192\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 65097.8828 - regression_loss: 17909.9688 - val_loss: 4175.7729 - val_regression_loss: 2069.3713\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 52784.0078 - regression_loss: 16567.4883 - val_loss: 3297.7429 - val_regression_loss: 1630.5853\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 51692.1250 - regression_loss: 15320.8457 - val_loss: 2981.8018 - val_regression_loss: 1472.8519\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 54285.4180 - regression_loss: 14367.3809 - val_loss: 3187.3132 - val_regression_loss: 1575.8334\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 48304.6836 - regression_loss: 13132.5781 - val_loss: 3471.1274 - val_regression_loss: 1717.9323\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 45750.4727 - regression_loss: 12235.7432 - val_loss: 3254.7603 - val_regression_loss: 1609.9043\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 42561.3281 - regression_loss: 11296.0078 - val_loss: 2632.1072 - val_regression_loss: 1298.7047\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 39365.6055 - regression_loss: 10417.9014 - val_loss: 2336.8928 - val_regression_loss: 1151.2064\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 36660.8398 - regression_loss: 9740.6729 - val_loss: 2503.0481 - val_regression_loss: 1234.3782\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 33916.2812 - regression_loss: 8965.0928 - val_loss: 2663.7788 - val_regression_loss: 1314.8228\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 30246.6328 - regression_loss: 8409.4893 - val_loss: 2370.1116 - val_regression_loss: 1168.0533\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 28931.0723 - regression_loss: 7880.8311 - val_loss: 2084.8870 - val_regression_loss: 1025.4916\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 28178.6875 - regression_loss: 7449.2344 - val_loss: 1986.1428 - val_regression_loss: 976.1642\n",
            "***************************** elapsed_time is:  6.237905025482178\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 208ms/step - loss: 1897434.6250 - regression_loss: 515589.6562 - val_loss: 210717.2188 - val_regression_loss: 105395.4531\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1804297.0000 - regression_loss: 496689.5625 - val_loss: 201296.0625 - val_regression_loss: 100690.2656\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1733338.8750 - regression_loss: 475352.8750 - val_loss: 187587.7031 - val_regression_loss: 93842.2734\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 1599635.0000 - regression_loss: 444405.0312 - val_loss: 168468.2031 - val_regression_loss: 84290.5938\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1427468.3750 - regression_loss: 401239.3438 - val_loss: 143679.6875 - val_regression_loss: 71908.2031\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1263366.3750 - regression_loss: 346120.5312 - val_loss: 114104.7891 - val_regression_loss: 57140.8008\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 1011605.9375 - regression_loss: 279708.7812 - val_loss: 81610.8672 - val_regression_loss: 40929.3047\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 750430.1250 - regression_loss: 206548.9219 - val_loss: 49975.3008 - val_regression_loss: 25157.0527\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 493276.1875 - regression_loss: 134706.6406 - val_loss: 27464.3242 - val_regression_loss: 13794.5742\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 296893.6250 - regression_loss: 81497.2422 - val_loss: 28534.7246 - val_regression_loss: 13361.7363\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 268973.9062 - regression_loss: 71738.0156 - val_loss: 45081.6641 - val_regression_loss: 20591.5625\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 377040.1875 - regression_loss: 96659.6797 - val_loss: 43249.2734 - val_regression_loss: 20012.6484\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 358002.5312 - regression_loss: 92618.8672 - val_loss: 29701.9863 - val_regression_loss: 14114.2617\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 269744.2812 - regression_loss: 70571.1641 - val_loss: 19786.2637 - val_regression_loss: 9699.5850\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 206997.5469 - regression_loss: 56138.6406 - val_loss: 17235.0527 - val_regression_loss: 8589.1201\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 196420.4844 - regression_loss: 55645.8477 - val_loss: 18245.6641 - val_regression_loss: 9130.4736\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 214375.9688 - regression_loss: 60391.0703 - val_loss: 19173.0039 - val_regression_loss: 9597.9238\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 232668.4375 - regression_loss: 63413.2422 - val_loss: 18528.7148 - val_regression_loss: 9269.8174\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 231588.5625 - regression_loss: 61892.3359 - val_loss: 16589.8086 - val_regression_loss: 8288.8936\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 210113.7188 - regression_loss: 56438.9219 - val_loss: 14570.5430 - val_regression_loss: 7262.2715\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 182062.1250 - regression_loss: 49923.3477 - val_loss: 13766.8936 - val_regression_loss: 6837.3398\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 166756.4688 - regression_loss: 45415.8164 - val_loss: 14730.9385 - val_regression_loss: 7292.0669\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 161198.6094 - regression_loss: 44170.5430 - val_loss: 16469.1211 - val_regression_loss: 8137.6680\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 167776.9219 - regression_loss: 45287.6367 - val_loss: 17245.8711 - val_regression_loss: 8515.2754\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 167073.7969 - regression_loss: 45325.6523 - val_loss: 16181.6514 - val_regression_loss: 7989.8369\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 150908.5781 - regression_loss: 43157.3594 - val_loss: 14181.9727 - val_regression_loss: 7008.2559\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 147482.9219 - regression_loss: 39967.6055 - val_loss: 12643.6162 - val_regression_loss: 6258.3179\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 141375.8438 - regression_loss: 38099.6719 - val_loss: 11855.0107 - val_regression_loss: 5878.8628\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 136122.6562 - regression_loss: 37490.5508 - val_loss: 11416.2227 - val_regression_loss: 5667.8340\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 136288.9219 - regression_loss: 36917.1836 - val_loss: 11018.5723 - val_regression_loss: 5470.1387\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 131062.4922 - regression_loss: 35560.1094 - val_loss: 10643.6348 - val_regression_loss: 5277.3799\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 125108.0078 - regression_loss: 33649.7109 - val_loss: 10525.4385 - val_regression_loss: 5206.8950\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 112840.9375 - regression_loss: 31767.1113 - val_loss: 10676.4707 - val_regression_loss: 5268.0327\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 111957.9766 - regression_loss: 30450.9297 - val_loss: 10726.7334 - val_regression_loss: 5281.5649\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 110160.3281 - regression_loss: 29395.0938 - val_loss: 10351.0225 - val_regression_loss: 5090.2744\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 101903.0859 - regression_loss: 28055.7832 - val_loss: 9505.7637 - val_regression_loss: 4676.4932\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 95776.3906 - regression_loss: 26386.5820 - val_loss: 8665.0830 - val_regression_loss: 4269.9331\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 88465.6328 - regression_loss: 24908.8574 - val_loss: 7956.3018 - val_regression_loss: 3930.5310\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 86662.9141 - regression_loss: 23715.8086 - val_loss: 7423.9658 - val_regression_loss: 3672.3779\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 81828.0312 - regression_loss: 22468.5488 - val_loss: 6985.7769 - val_regression_loss: 3451.3511\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 75862.5703 - regression_loss: 21007.7148 - val_loss: 6620.0483 - val_regression_loss: 3254.8882\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 68399.7891 - regression_loss: 19435.9805 - val_loss: 6296.5884 - val_regression_loss: 3071.8723\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 66702.4688 - regression_loss: 18029.9746 - val_loss: 5892.2334 - val_regression_loss: 2854.2349\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 61488.5898 - regression_loss: 16683.5996 - val_loss: 5316.5210 - val_regression_loss: 2568.4668\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 51715.3281 - regression_loss: 15397.9883 - val_loss: 4615.5513 - val_regression_loss: 2242.2019\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 51420.7500 - regression_loss: 14264.6709 - val_loss: 4101.0049 - val_regression_loss: 1999.3420\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 46541.2070 - regression_loss: 13260.0996 - val_loss: 3748.4189 - val_regression_loss: 1819.2739\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 43708.2656 - regression_loss: 12243.8701 - val_loss: 3523.5667 - val_regression_loss: 1688.0739\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 40055.3359 - regression_loss: 11212.3877 - val_loss: 3339.3118 - val_regression_loss: 1579.0018\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 36136.1523 - regression_loss: 10334.5762 - val_loss: 3122.2827 - val_regression_loss: 1464.4718\n",
            "***************************** elapsed_time is:  6.431341171264648\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 167ms/step - loss: 57874.7539 - regression_loss: 15601.7100 - val_loss: 6054.4292 - val_regression_loss: 3002.5000\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 47201.9766 - regression_loss: 12602.4863 - val_loss: 4656.1333 - val_regression_loss: 2303.9590\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 35547.9727 - regression_loss: 9607.7646 - val_loss: 3048.7036 - val_regression_loss: 1500.8032\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 23579.4043 - regression_loss: 6288.4253 - val_loss: 1682.2749 - val_regression_loss: 818.1057\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 13672.6367 - regression_loss: 3624.5706 - val_loss: 1282.4989 - val_regression_loss: 618.7943\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 11522.5283 - regression_loss: 3074.0840 - val_loss: 1191.2072 - val_regression_loss: 573.9844\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 10897.8701 - regression_loss: 2903.1650 - val_loss: 901.2762 - val_regression_loss: 429.8665\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 8191.1675 - regression_loss: 2156.2339 - val_loss: 775.5397 - val_regression_loss: 367.5668\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 6593.5752 - regression_loss: 1707.0326 - val_loss: 893.3608 - val_regression_loss: 426.8466\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 6868.0923 - regression_loss: 1788.9584 - val_loss: 1019.4128 - val_regression_loss: 490.2421\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 7382.1724 - regression_loss: 1954.5438 - val_loss: 979.5123 - val_regression_loss: 470.7367\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 7016.2124 - regression_loss: 1851.4159 - val_loss: 805.1561 - val_regression_loss: 384.0437\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 5921.4648 - regression_loss: 1540.2009 - val_loss: 618.5782 - val_regression_loss: 291.1919\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 4863.9497 - regression_loss: 1229.1544 - val_loss: 525.1071 - val_regression_loss: 244.7689\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 4411.3853 - regression_loss: 1109.4669 - val_loss: 534.3632 - val_regression_loss: 249.5537\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 4599.6924 - regression_loss: 1186.4354 - val_loss: 557.2927 - val_regression_loss: 261.0542\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 4698.5518 - regression_loss: 1236.4667 - val_loss: 534.7002 - val_regression_loss: 249.7352\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 4407.2349 - regression_loss: 1147.0161 - val_loss: 503.4967 - val_regression_loss: 234.0875\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 4045.3328 - regression_loss: 1028.9613 - val_loss: 505.5780 - val_regression_loss: 235.0829\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 3821.4878 - regression_loss: 985.9705 - val_loss: 516.2304 - val_regression_loss: 240.3964\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 3905.2117 - regression_loss: 979.9176 - val_loss: 496.7216 - val_regression_loss: 230.6765\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3639.8406 - regression_loss: 935.4834 - val_loss: 459.3669 - val_regression_loss: 212.0599\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3466.8037 - regression_loss: 878.2255 - val_loss: 430.9818 - val_regression_loss: 197.9333\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 3267.7373 - regression_loss: 850.8166 - val_loss: 412.6321 - val_regression_loss: 188.8149\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3374.6345 - regression_loss: 838.9022 - val_loss: 391.0903 - val_regression_loss: 178.0911\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 3209.4851 - regression_loss: 800.2788 - val_loss: 372.2078 - val_regression_loss: 168.6778\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2979.0403 - regression_loss: 753.6179 - val_loss: 365.4271 - val_regression_loss: 165.3001\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 2940.8257 - regression_loss: 728.1423 - val_loss: 361.4048 - val_regression_loss: 163.3069\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 2836.1802 - regression_loss: 711.4458 - val_loss: 349.8831 - val_regression_loss: 157.5798\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2788.6875 - regression_loss: 682.8862 - val_loss: 337.7468 - val_regression_loss: 151.5551\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2681.8972 - regression_loss: 653.6420 - val_loss: 329.0261 - val_regression_loss: 147.2285\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2472.3657 - regression_loss: 635.0134 - val_loss: 316.7582 - val_regression_loss: 141.1110\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2506.4741 - regression_loss: 613.1793 - val_loss: 304.3934 - val_regression_loss: 134.9389\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2462.4641 - regression_loss: 592.9409 - val_loss: 294.6979 - val_regression_loss: 130.1162\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 2379.6917 - regression_loss: 578.3375 - val_loss: 282.4539 - val_regression_loss: 124.0417\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2150.2654 - regression_loss: 557.6323 - val_loss: 271.1388 - val_regression_loss: 118.4382\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2209.6870 - regression_loss: 540.6668 - val_loss: 261.8669 - val_regression_loss: 113.8499\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 2192.6663 - regression_loss: 526.9132 - val_loss: 252.8713 - val_regression_loss: 109.3807\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2082.0686 - regression_loss: 511.1746 - val_loss: 243.9299 - val_regression_loss: 104.9236\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2060.4106 - regression_loss: 498.5450 - val_loss: 237.3228 - val_regression_loss: 101.6392\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 2062.3906 - regression_loss: 485.8430 - val_loss: 231.2708 - val_regression_loss: 98.6377\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1927.2336 - regression_loss: 472.8572 - val_loss: 226.4697 - val_regression_loss: 96.2559\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1894.4988 - regression_loss: 462.8293 - val_loss: 223.0868 - val_regression_loss: 94.5741\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1890.0295 - regression_loss: 452.8354 - val_loss: 219.3087 - val_regression_loss: 92.6924\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1855.2908 - regression_loss: 443.8397 - val_loss: 214.5590 - val_regression_loss: 90.3279\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1822.8235 - regression_loss: 434.6531 - val_loss: 209.3441 - val_regression_loss: 87.7338\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1779.1614 - regression_loss: 425.4510 - val_loss: 203.7200 - val_regression_loss: 84.9300\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1680.7145 - regression_loss: 416.1393 - val_loss: 199.3119 - val_regression_loss: 82.7313\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 1761.3864 - regression_loss: 408.0929 - val_loss: 196.6511 - val_regression_loss: 81.4045\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1636.3987 - regression_loss: 398.8848 - val_loss: 194.6022 - val_regression_loss: 80.3812\n",
            "***************************** elapsed_time is:  6.32662558555603\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 195ms/step - loss: 7386.6323 - regression_loss: 1960.1674 - val_loss: 552.2861 - val_regression_loss: 256.6348\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 4612.5752 - regression_loss: 1176.8707 - val_loss: 384.5393 - val_regression_loss: 177.2073\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 3323.7498 - regression_loss: 833.1464 - val_loss: 252.2130 - val_regression_loss: 110.4963\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 2016.0044 - regression_loss: 493.3112 - val_loss: 189.3772 - val_regression_loss: 75.0344\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1381.3889 - regression_loss: 293.8627 - val_loss: 279.1454 - val_regression_loss: 115.7325\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1996.0179 - regression_loss: 451.6160 - val_loss: 257.7093 - val_regression_loss: 105.6703\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1844.9397 - regression_loss: 416.8149 - val_loss: 187.1621 - val_regression_loss: 73.4627\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1400.5454 - regression_loss: 298.8018 - val_loss: 163.2148 - val_regression_loss: 64.4621\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1279.4767 - regression_loss: 285.1786 - val_loss: 170.4122 - val_regression_loss: 69.9044\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1401.6104 - regression_loss: 325.4562 - val_loss: 175.7930 - val_regression_loss: 73.3531\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1444.8267 - regression_loss: 348.0247 - val_loss: 169.5551 - val_regression_loss: 70.2061\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1359.2745 - regression_loss: 329.8291 - val_loss: 157.4279 - val_regression_loss: 63.4181\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1287.5442 - regression_loss: 296.2695 - val_loss: 150.8635 - val_regression_loss: 58.7168\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1232.0787 - regression_loss: 270.8208 - val_loss: 155.6547 - val_regression_loss: 59.4744\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1267.2042 - regression_loss: 270.3342 - val_loss: 163.4223 - val_regression_loss: 62.3931\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1282.9142 - regression_loss: 274.7366 - val_loss: 164.7278 - val_regression_loss: 63.2703\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1275.4503 - regression_loss: 271.2955 - val_loss: 160.5545 - val_regression_loss: 62.1389\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 1221.5184 - regression_loss: 263.7766 - val_loss: 155.9866 - val_regression_loss: 60.9291\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1208.9628 - regression_loss: 263.1439 - val_loss: 154.8757 - val_regression_loss: 61.2205\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1193.8484 - regression_loss: 269.6043 - val_loss: 155.5579 - val_regression_loss: 61.9708\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 1188.0288 - regression_loss: 272.9980 - val_loss: 156.1324 - val_regression_loss: 62.1793\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 1190.9194 - regression_loss: 269.9401 - val_loss: 157.0645 - val_regression_loss: 62.2208\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1188.1636 - regression_loss: 263.2476 - val_loss: 157.0788 - val_regression_loss: 61.7014\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1181.7235 - regression_loss: 259.3650 - val_loss: 157.9992 - val_regression_loss: 61.6380\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1200.0164 - regression_loss: 257.2876 - val_loss: 159.4607 - val_regression_loss: 62.1600\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1185.5081 - regression_loss: 257.6525 - val_loss: 160.5116 - val_regression_loss: 62.7215\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 1163.4988 - regression_loss: 256.6377 - val_loss: 160.4546 - val_regression_loss: 62.9732\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1149.0641 - regression_loss: 256.6407 - val_loss: 158.8878 - val_regression_loss: 62.5659\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1173.1326 - regression_loss: 257.1233 - val_loss: 157.8742 - val_regression_loss: 62.2815\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1167.4685 - regression_loss: 258.3981 - val_loss: 157.5755 - val_regression_loss: 62.1304\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1165.6998 - regression_loss: 258.3857 - val_loss: 158.2397 - val_regression_loss: 62.2572\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1154.0538 - regression_loss: 256.2751 - val_loss: 158.8345 - val_regression_loss: 62.2759\n",
            "Epoch 33/50\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 1206.2334 - regression_loss: 485.2657\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1153.0465 - regression_loss: 254.6022 - val_loss: 159.2314 - val_regression_loss: 62.2273\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1159.8125 - regression_loss: 253.7144 - val_loss: 159.1324 - val_regression_loss: 62.1277\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1160.8632 - regression_loss: 253.5136 - val_loss: 158.9358 - val_regression_loss: 62.0453\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1139.1154 - regression_loss: 253.3664 - val_loss: 158.6366 - val_regression_loss: 61.9511\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1163.0713 - regression_loss: 253.2430 - val_loss: 158.4775 - val_regression_loss: 61.9626\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1152.2219 - regression_loss: 253.3645 - val_loss: 158.5092 - val_regression_loss: 62.0600\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1158.0112 - regression_loss: 253.5416 - val_loss: 158.3548 - val_regression_loss: 62.0399\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1149.9463 - regression_loss: 253.6785 - val_loss: 158.1097 - val_regression_loss: 61.9288\n",
            "Epoch 41/50\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 1202.4358 - regression_loss: 485.1222\n",
            "Epoch 00041: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1149.1639 - regression_loss: 253.5427 - val_loss: 158.1444 - val_regression_loss: 61.9232\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1158.8271 - regression_loss: 253.2678 - val_loss: 158.1902 - val_regression_loss: 61.9192\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1158.3953 - regression_loss: 253.0822 - val_loss: 158.2940 - val_regression_loss: 61.9374\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1155.9847 - regression_loss: 252.8540 - val_loss: 158.4482 - val_regression_loss: 61.9781\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 1138.2576 - regression_loss: 252.6529 - val_loss: 158.6633 - val_regression_loss: 62.0416\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1142.1350 - regression_loss: 252.4073 - val_loss: 158.8106 - val_regression_loss: 62.0865\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1141.0341 - regression_loss: 252.2085 - val_loss: 158.8663 - val_regression_loss: 62.0920\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 1140.5938 - regression_loss: 252.0602 - val_loss: 158.8649 - val_regression_loss: 62.0745\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1099.6393 - regression_loss: 251.9472 - val_loss: 158.7942 - val_regression_loss: 62.0372\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1140.9187 - regression_loss: 251.8111 - val_loss: 158.8416 - val_regression_loss: 62.0611\n",
            "***************************** elapsed_time is:  6.168573617935181\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 164ms/step - loss: 63996.1211 - regression_loss: 17232.0059 - val_loss: 7210.8691 - val_regression_loss: 3570.4951\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 51516.4297 - regression_loss: 13840.7217 - val_loss: 5798.4707 - val_regression_loss: 2869.2073\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 39699.6406 - regression_loss: 10824.2988 - val_loss: 4176.0640 - val_regression_loss: 2063.2146\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 27713.3594 - regression_loss: 7449.1499 - val_loss: 2591.6794 - val_regression_loss: 1276.4333\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 16510.2266 - regression_loss: 4366.2861 - val_loss: 1682.5028 - val_regression_loss: 826.7353\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 10546.5176 - regression_loss: 2836.0405 - val_loss: 1715.8022 - val_regression_loss: 845.2830\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 11940.8174 - regression_loss: 3283.8958 - val_loss: 1665.4944 - val_regression_loss: 817.6971\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 11913.7500 - regression_loss: 3151.4058 - val_loss: 1387.2017 - val_regression_loss: 674.9987\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 8873.0186 - regression_loss: 2336.4490 - val_loss: 1257.6866 - val_regression_loss: 607.6921\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 7115.6748 - regression_loss: 1860.8550 - val_loss: 1350.9692 - val_regression_loss: 653.1664\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 7463.4814 - regression_loss: 1970.5573 - val_loss: 1399.4437 - val_regression_loss: 677.6320\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 7667.8208 - regression_loss: 2093.1172 - val_loss: 1262.6389 - val_regression_loss: 610.5143\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 7099.7764 - regression_loss: 1874.3912 - val_loss: 1051.5160 - val_regression_loss: 506.7920\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 5873.3965 - regression_loss: 1497.7750 - val_loss: 934.7969 - val_regression_loss: 450.2852\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 4973.2129 - regression_loss: 1284.9603 - val_loss: 951.3594 - val_regression_loss: 459.9796\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 5113.5166 - regression_loss: 1323.2588 - val_loss: 996.4196 - val_regression_loss: 483.1984\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 5310.8965 - regression_loss: 1397.5411 - val_loss: 968.4535 - val_regression_loss: 469.1329\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 5075.0352 - regression_loss: 1313.1309 - val_loss: 884.9597 - val_regression_loss: 426.6584\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 4321.7695 - regression_loss: 1123.0453 - val_loss: 829.1540 - val_regression_loss: 397.7154\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 3954.6858 - regression_loss: 994.5477 - val_loss: 811.8572 - val_regression_loss: 388.1366\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 3814.7375 - regression_loss: 965.5649 - val_loss: 788.6791 - val_regression_loss: 376.0309\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3386.8750 - regression_loss: 928.4862 - val_loss: 738.5026 - val_regression_loss: 350.9316\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3293.8049 - regression_loss: 850.8008 - val_loss: 685.1967 - val_regression_loss: 324.6980\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 3155.4407 - regression_loss: 780.9805 - val_loss: 658.4089 - val_regression_loss: 311.9246\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3027.8381 - regression_loss: 749.5941 - val_loss: 643.6810 - val_regression_loss: 305.0475\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2893.9023 - regression_loss: 719.6581 - val_loss: 621.8066 - val_regression_loss: 294.2965\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2624.6497 - regression_loss: 650.7697 - val_loss: 608.0861 - val_regression_loss: 287.3671\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 2458.8232 - regression_loss: 598.2103 - val_loss: 605.3090 - val_regression_loss: 285.8428\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2418.4780 - regression_loss: 581.6286 - val_loss: 590.8820 - val_regression_loss: 278.6277\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 2256.9475 - regression_loss: 558.0623 - val_loss: 558.8713 - val_regression_loss: 262.7906\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2157.1902 - regression_loss: 517.0825 - val_loss: 529.3250 - val_regression_loss: 248.1144\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1978.1892 - regression_loss: 489.5856 - val_loss: 505.2091 - val_regression_loss: 235.9443\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2002.4902 - regression_loss: 472.4359 - val_loss: 481.9375 - val_regression_loss: 223.9815\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1905.7151 - regression_loss: 447.1889 - val_loss: 466.2880 - val_regression_loss: 215.7775\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1744.1521 - regression_loss: 425.5394 - val_loss: 455.0663 - val_regression_loss: 209.9945\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1797.8438 - regression_loss: 416.1433 - val_loss: 437.9353 - val_regression_loss: 201.5358\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1739.2030 - regression_loss: 400.8345 - val_loss: 421.2883 - val_regression_loss: 193.4510\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1687.0724 - regression_loss: 388.5718 - val_loss: 408.9982 - val_regression_loss: 187.5147\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1650.4723 - regression_loss: 382.9076 - val_loss: 399.8484 - val_regression_loss: 182.9972\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 1573.3009 - regression_loss: 373.8406 - val_loss: 391.8956 - val_regression_loss: 178.9702\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1566.1632 - regression_loss: 366.1783 - val_loss: 384.2511 - val_regression_loss: 175.0996\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1522.3203 - regression_loss: 360.1811 - val_loss: 373.8009 - val_regression_loss: 169.8509\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1536.7487 - regression_loss: 353.7245 - val_loss: 363.6112 - val_regression_loss: 164.7790\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1522.0397 - regression_loss: 348.1729 - val_loss: 355.5408 - val_regression_loss: 160.7502\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1513.5632 - regression_loss: 344.3590 - val_loss: 349.1826 - val_regression_loss: 157.5465\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1485.3534 - regression_loss: 339.6685 - val_loss: 344.9370 - val_regression_loss: 155.3761\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1497.7823 - regression_loss: 335.2431 - val_loss: 340.6703 - val_regression_loss: 153.2794\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1466.4767 - regression_loss: 331.8046 - val_loss: 334.4938 - val_regression_loss: 150.3269\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1442.9802 - regression_loss: 327.6452 - val_loss: 330.2664 - val_regression_loss: 148.3150\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1424.9950 - regression_loss: 324.6289 - val_loss: 327.2296 - val_regression_loss: 146.7757\n",
            "***************************** elapsed_time is:  6.155645370483398\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 188ms/step - loss: 96266.9219 - regression_loss: 26152.3555 - val_loss: 8956.4785 - val_regression_loss: 4459.3896\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 78652.0234 - regression_loss: 21328.1992 - val_loss: 6938.7402 - val_regression_loss: 3450.1460\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 60339.9336 - regression_loss: 16400.7500 - val_loss: 4585.1924 - val_regression_loss: 2272.4644\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 39309.0664 - regression_loss: 10680.8623 - val_loss: 2346.2544 - val_regression_loss: 1151.5663\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 20114.9199 - regression_loss: 5297.0635 - val_loss: 1188.9121 - val_regression_loss: 571.4415\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 8514.0400 - regression_loss: 2248.6135 - val_loss: 1715.2184 - val_regression_loss: 834.4513\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 12102.5479 - regression_loss: 3223.5059 - val_loss: 2052.7102 - val_regression_loss: 1005.3211\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 15824.8545 - regression_loss: 4164.5234 - val_loss: 1419.5530 - val_regression_loss: 690.8727\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 11394.4082 - regression_loss: 3042.3669 - val_loss: 690.6379 - val_regression_loss: 327.6894\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 6381.6177 - regression_loss: 1665.0985 - val_loss: 479.0905 - val_regression_loss: 222.4812\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 5246.9937 - regression_loss: 1379.0334 - val_loss: 618.1440 - val_regression_loss: 292.0608\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 6572.5684 - regression_loss: 1763.9877 - val_loss: 747.5863 - val_regression_loss: 356.5134\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 7861.1704 - regression_loss: 2049.4355 - val_loss: 721.4970 - val_regression_loss: 343.0446\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 7469.1748 - regression_loss: 1933.4518 - val_loss: 592.4430 - val_regression_loss: 278.0839\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 5883.1968 - regression_loss: 1553.0685 - val_loss: 477.2505 - val_regression_loss: 220.1581\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 4655.7842 - regression_loss: 1204.5780 - val_loss: 455.8120 - val_regression_loss: 209.3033\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 4093.3281 - regression_loss: 1079.7402 - val_loss: 520.4410 - val_regression_loss: 241.6660\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 4495.6743 - regression_loss: 1161.4185 - val_loss: 585.8010 - val_regression_loss: 274.4626\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 4862.2168 - regression_loss: 1266.6105 - val_loss: 580.4075 - val_regression_loss: 271.7899\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 4728.7310 - regression_loss: 1239.8348 - val_loss: 516.6330 - val_regression_loss: 239.8509\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 4237.1118 - regression_loss: 1113.4722 - val_loss: 450.5131 - val_regression_loss: 206.7390\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3782.9800 - regression_loss: 997.3657 - val_loss: 419.1900 - val_regression_loss: 191.0779\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3740.5339 - regression_loss: 958.9859 - val_loss: 413.5222 - val_regression_loss: 188.3138\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3773.0122 - regression_loss: 976.8777 - val_loss: 408.8540 - val_regression_loss: 186.0932\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 3797.3215 - regression_loss: 984.5383 - val_loss: 392.6104 - val_regression_loss: 178.0897\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3782.1794 - regression_loss: 952.9825 - val_loss: 371.9470 - val_regression_loss: 167.8572\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 3534.0515 - regression_loss: 896.4230 - val_loss: 361.1567 - val_regression_loss: 162.5185\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3357.5457 - regression_loss: 851.6023 - val_loss: 363.8309 - val_regression_loss: 163.8383\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3252.4050 - regression_loss: 826.9481 - val_loss: 371.4427 - val_regression_loss: 167.5277\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3177.2988 - regression_loss: 811.0180 - val_loss: 373.3732 - val_regression_loss: 168.3029\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3144.7168 - regression_loss: 784.8153 - val_loss: 367.5647 - val_regression_loss: 165.1959\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3012.4268 - regression_loss: 747.5258 - val_loss: 357.9183 - val_regression_loss: 160.2245\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2875.4546 - regression_loss: 713.6965 - val_loss: 349.2593 - val_regression_loss: 155.8365\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2744.6785 - regression_loss: 691.6519 - val_loss: 339.6917 - val_regression_loss: 151.0677\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2682.6958 - regression_loss: 672.3041 - val_loss: 328.0438 - val_regression_loss: 145.3204\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2592.1453 - regression_loss: 649.0953 - val_loss: 314.8771 - val_regression_loss: 138.8257\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2506.9243 - regression_loss: 623.1234 - val_loss: 305.2671 - val_regression_loss: 134.1021\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2422.2339 - regression_loss: 602.5156 - val_loss: 298.1410 - val_regression_loss: 130.5809\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2337.7036 - regression_loss: 587.0331 - val_loss: 293.6254 - val_regression_loss: 128.3216\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 2296.0044 - regression_loss: 573.1458 - val_loss: 288.7491 - val_regression_loss: 125.8396\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2281.0728 - regression_loss: 557.0854 - val_loss: 284.5387 - val_regression_loss: 123.6571\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2205.0605 - regression_loss: 541.4269 - val_loss: 281.6543 - val_regression_loss: 122.1376\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2196.9375 - regression_loss: 529.4809 - val_loss: 277.3246 - val_regression_loss: 119.9115\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2118.7141 - regression_loss: 516.2673 - val_loss: 272.8366 - val_regression_loss: 117.6179\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2076.5920 - regression_loss: 502.1781 - val_loss: 266.5029 - val_regression_loss: 114.4348\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2026.4258 - regression_loss: 488.8604 - val_loss: 261.4189 - val_regression_loss: 111.9186\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 2005.7703 - regression_loss: 476.9074 - val_loss: 256.3969 - val_regression_loss: 109.4653\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1949.7842 - regression_loss: 464.4427 - val_loss: 249.3174 - val_regression_loss: 105.9783\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1900.3300 - regression_loss: 453.5249 - val_loss: 242.0855 - val_regression_loss: 102.4029\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1846.2889 - regression_loss: 443.4883 - val_loss: 236.6698 - val_regression_loss: 99.6937\n",
            "***************************** elapsed_time is:  5.9240758419036865\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 167ms/step - loss: 9632.7637 - regression_loss: 2513.5320 - val_loss: 838.4355 - val_regression_loss: 396.4182\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 5956.4224 - regression_loss: 1516.4923 - val_loss: 543.0885 - val_regression_loss: 251.4360\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 4103.4082 - regression_loss: 1024.5284 - val_loss: 388.1508 - val_regression_loss: 175.3269\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 3034.5903 - regression_loss: 740.6697 - val_loss: 262.0659 - val_regression_loss: 111.7143\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1756.4779 - regression_loss: 404.6829 - val_loss: 300.0610 - val_regression_loss: 129.3057\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1751.0381 - regression_loss: 402.5250 - val_loss: 374.4608 - val_regression_loss: 166.0822\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 2247.2258 - regression_loss: 533.7082 - val_loss: 317.6985 - val_regression_loss: 138.6962\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1938.9635 - regression_loss: 446.4783 - val_loss: 242.9454 - val_regression_loss: 102.8842\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1526.0939 - regression_loss: 339.1048 - val_loss: 217.0665 - val_regression_loss: 91.2782\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1464.2218 - regression_loss: 332.6729 - val_loss: 220.5620 - val_regression_loss: 93.7668\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1538.3939 - regression_loss: 362.7576 - val_loss: 228.3292 - val_regression_loss: 97.9066\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1609.6388 - regression_loss: 375.8572 - val_loss: 225.5102 - val_regression_loss: 96.5255\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1508.4298 - regression_loss: 355.7206 - val_loss: 214.8647 - val_regression_loss: 91.0162\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1371.2738 - regression_loss: 319.2235 - val_loss: 207.0863 - val_regression_loss: 86.6525\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1301.3767 - regression_loss: 297.4442 - val_loss: 206.7833 - val_regression_loss: 85.8538\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1347.7273 - regression_loss: 298.6298 - val_loss: 207.7269 - val_regression_loss: 85.8850\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1356.0724 - regression_loss: 302.7348 - val_loss: 204.3402 - val_regression_loss: 84.2067\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 1345.9991 - regression_loss: 299.8003 - val_loss: 193.0939 - val_regression_loss: 78.9607\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1313.7742 - regression_loss: 290.4107 - val_loss: 181.6218 - val_regression_loss: 73.7571\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 1291.9550 - regression_loss: 284.0066 - val_loss: 177.9682 - val_regression_loss: 72.3824\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1279.5265 - regression_loss: 285.6367 - val_loss: 180.2797 - val_regression_loss: 73.7220\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1266.2461 - regression_loss: 288.1964 - val_loss: 182.4915 - val_regression_loss: 74.7680\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1292.2563 - regression_loss: 286.8215 - val_loss: 180.8594 - val_regression_loss: 73.7532\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1258.5502 - regression_loss: 281.3400 - val_loss: 177.0575 - val_regression_loss: 71.5994\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1245.5226 - regression_loss: 275.8256 - val_loss: 174.4774 - val_regression_loss: 70.0473\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1242.1670 - regression_loss: 274.4930 - val_loss: 175.0786 - val_regression_loss: 70.1578\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1222.3588 - regression_loss: 274.2073 - val_loss: 175.7983 - val_regression_loss: 70.4792\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1233.6816 - regression_loss: 272.6812 - val_loss: 176.3716 - val_regression_loss: 70.8638\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 1229.8035 - regression_loss: 270.5918 - val_loss: 176.0602 - val_regression_loss: 70.8654\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1222.5634 - regression_loss: 269.7799 - val_loss: 175.2670 - val_regression_loss: 70.6082\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1217.7733 - regression_loss: 270.1810 - val_loss: 173.9740 - val_regression_loss: 69.9949\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1207.7562 - regression_loss: 269.7710 - val_loss: 174.0754 - val_regression_loss: 69.9595\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1222.8051 - regression_loss: 267.9994 - val_loss: 174.7635 - val_regression_loss: 70.1896\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1202.4204 - regression_loss: 266.4103 - val_loss: 174.3542 - val_regression_loss: 69.9010\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1184.7788 - regression_loss: 265.0616 - val_loss: 174.9870 - val_regression_loss: 70.1744\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1183.3124 - regression_loss: 264.7619 - val_loss: 175.8636 - val_regression_loss: 70.5865\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1205.0255 - regression_loss: 263.2270 - val_loss: 175.1128 - val_regression_loss: 70.2375\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1210.7830 - regression_loss: 262.3478 - val_loss: 173.5553 - val_regression_loss: 69.5102\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1179.5192 - regression_loss: 262.1914 - val_loss: 172.0359 - val_regression_loss: 68.7870\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1186.7336 - regression_loss: 261.7034 - val_loss: 173.8488 - val_regression_loss: 69.6853\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1184.2838 - regression_loss: 261.1029 - val_loss: 176.4562 - val_regression_loss: 70.9230\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1155.9004 - regression_loss: 260.0987 - val_loss: 174.8705 - val_regression_loss: 70.0885\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1157.8750 - regression_loss: 259.2478 - val_loss: 174.1372 - val_regression_loss: 69.6868\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1174.8441 - regression_loss: 258.7362 - val_loss: 174.5302 - val_regression_loss: 69.8197\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1154.5557 - regression_loss: 257.8586 - val_loss: 175.4647 - val_regression_loss: 70.2599\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1157.5586 - regression_loss: 257.5494 - val_loss: 173.3298 - val_regression_loss: 69.2347\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1149.4408 - regression_loss: 256.8391 - val_loss: 172.6532 - val_regression_loss: 68.8909\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1156.5516 - regression_loss: 256.3730 - val_loss: 172.9442 - val_regression_loss: 69.0037\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1160.5045 - regression_loss: 255.6931 - val_loss: 173.7316 - val_regression_loss: 69.3673\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 1141.0759 - regression_loss: 255.4290 - val_loss: 175.1905 - val_regression_loss: 70.0513\n",
            "***************************** elapsed_time is:  6.3704986572265625\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 224ms/step - loss: 95112.0625 - regression_loss: 25940.2832 - val_loss: 9575.0137 - val_regression_loss: 4778.6475\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 77448.9844 - regression_loss: 21283.3242 - val_loss: 7384.7192 - val_regression_loss: 3680.7585\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 60849.1875 - regression_loss: 16376.9824 - val_loss: 4851.6797 - val_regression_loss: 2408.8455\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 39907.6562 - regression_loss: 10665.4482 - val_loss: 2649.1282 - val_regression_loss: 1297.5945\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 21204.1855 - regression_loss: 5707.6724 - val_loss: 2086.9807 - val_regression_loss: 1002.1533\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 16767.8945 - regression_loss: 4404.0122 - val_loss: 2614.9714 - val_regression_loss: 1259.0614\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 20313.7969 - regression_loss: 5353.7358 - val_loss: 2097.3147 - val_regression_loss: 1006.6712\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 16298.8086 - regression_loss: 4184.8438 - val_loss: 1351.7145 - val_regression_loss: 644.2720\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 10334.9609 - regression_loss: 2666.9507 - val_loss: 1108.8466 - val_regression_loss: 531.3527\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 8779.4707 - regression_loss: 2312.0645 - val_loss: 1255.6484 - val_regression_loss: 609.9118\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 10198.7764 - regression_loss: 2748.9294 - val_loss: 1376.1411 - val_regression_loss: 672.3364\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 11464.1914 - regression_loss: 3072.0310 - val_loss: 1263.7864 - val_regression_loss: 616.2801\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 10585.6260 - regression_loss: 2849.5540 - val_loss: 1006.6373 - val_regression_loss: 486.6258\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 8558.3623 - regression_loss: 2289.9194 - val_loss: 807.8877 - val_regression_loss: 385.6492\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 6889.9868 - regression_loss: 1852.4365 - val_loss: 786.3459 - val_regression_loss: 373.3164\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 6504.2056 - regression_loss: 1763.0964 - val_loss: 865.4955 - val_regression_loss: 411.8686\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 7351.2368 - regression_loss: 1886.6843 - val_loss: 878.2654 - val_regression_loss: 418.0094\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 7188.0044 - regression_loss: 1858.8673 - val_loss: 787.9777 - val_regression_loss: 373.3781\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 6420.7153 - regression_loss: 1644.4086 - val_loss: 687.9749 - val_regression_loss: 324.4182\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 5626.5171 - regression_loss: 1437.3301 - val_loss: 641.5223 - val_regression_loss: 302.3717\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 5052.7930 - regression_loss: 1370.4534 - val_loss: 614.3524 - val_regression_loss: 289.7977\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 5174.9790 - regression_loss: 1347.4447 - val_loss: 563.7370 - val_regression_loss: 265.1299\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 4679.7007 - regression_loss: 1255.1249 - val_loss: 503.1522 - val_regression_loss: 235.0744\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 4420.4204 - regression_loss: 1134.1360 - val_loss: 470.8071 - val_regression_loss: 218.7918\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 4111.4849 - regression_loss: 1058.7131 - val_loss: 467.0201 - val_regression_loss: 216.5453\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 4057.7397 - regression_loss: 1031.9065 - val_loss: 449.2001 - val_regression_loss: 207.2381\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3888.2485 - regression_loss: 977.4530 - val_loss: 409.1491 - val_regression_loss: 186.9513\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 3511.2981 - regression_loss: 892.9719 - val_loss: 382.0747 - val_regression_loss: 173.2993\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 3394.7122 - regression_loss: 853.8267 - val_loss: 373.4751 - val_regression_loss: 168.9073\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3222.9749 - regression_loss: 848.8807 - val_loss: 361.5866 - val_regression_loss: 162.7735\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3280.7468 - regression_loss: 820.8231 - val_loss: 352.1736 - val_regression_loss: 157.7728\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3128.8538 - regression_loss: 772.4377 - val_loss: 355.5523 - val_regression_loss: 159.3583\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2991.3484 - regression_loss: 750.4158 - val_loss: 352.8340 - val_regression_loss: 158.2060\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 2929.5693 - regression_loss: 735.0153 - val_loss: 333.9524 - val_regression_loss: 149.2366\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2682.8157 - regression_loss: 709.1616 - val_loss: 312.8738 - val_regression_loss: 139.1690\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 2815.8557 - regression_loss: 690.5801 - val_loss: 302.1257 - val_regression_loss: 133.8785\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2721.9502 - regression_loss: 673.5878 - val_loss: 296.1719 - val_regression_loss: 130.6476\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 2593.8447 - regression_loss: 642.8181 - val_loss: 295.7041 - val_regression_loss: 130.0689\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2549.6924 - regression_loss: 619.1418 - val_loss: 293.3382 - val_regression_loss: 128.7095\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2362.4087 - regression_loss: 603.8433 - val_loss: 278.3752 - val_regression_loss: 121.3263\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 2372.2844 - regression_loss: 584.0064 - val_loss: 263.8596 - val_regression_loss: 114.2709\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2326.3269 - regression_loss: 568.0626 - val_loss: 254.7254 - val_regression_loss: 109.9179\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2254.1423 - regression_loss: 554.1410 - val_loss: 254.2454 - val_regression_loss: 109.7490\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2221.4338 - regression_loss: 537.4727 - val_loss: 256.7793 - val_regression_loss: 111.0487\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 2109.6995 - regression_loss: 523.4489 - val_loss: 255.0835 - val_regression_loss: 110.2514\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 2136.4006 - regression_loss: 511.2213 - val_loss: 246.1376 - val_regression_loss: 105.8642\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 2057.1040 - regression_loss: 497.9758 - val_loss: 236.9090 - val_regression_loss: 101.2953\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1999.6001 - regression_loss: 485.5499 - val_loss: 232.7417 - val_regression_loss: 99.1555\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1966.0714 - regression_loss: 474.5379 - val_loss: 232.9604 - val_regression_loss: 99.1610\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1954.8322 - regression_loss: 462.0643 - val_loss: 230.2474 - val_regression_loss: 97.7904\n",
            "***************************** elapsed_time is:  6.242146968841553\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 173ms/step - loss: 818870.1875 - regression_loss: 223176.5000 - val_loss: 94629.3438 - val_regression_loss: 47149.9062\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 787229.0000 - regression_loss: 211273.0781 - val_loss: 88472.5234 - val_regression_loss: 44085.7852\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 728439.6250 - regression_loss: 197681.7812 - val_loss: 80068.1797 - val_regression_loss: 39903.3438\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 671543.5000 - regression_loss: 179548.2188 - val_loss: 69327.6406 - val_regression_loss: 34558.5469\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 569802.2500 - regression_loss: 156270.1562 - val_loss: 56862.7852 - val_regression_loss: 28356.5391\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 461183.0000 - regression_loss: 129535.6484 - val_loss: 43753.7227 - val_regression_loss: 21834.8359\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 382261.2188 - regression_loss: 102282.6484 - val_loss: 31535.2168 - val_regression_loss: 15757.2363\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 284086.7500 - regression_loss: 77020.5859 - val_loss: 22360.1133 - val_regression_loss: 11195.7441\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 216565.2812 - regression_loss: 59218.0781 - val_loss: 19037.0449 - val_regression_loss: 9544.8594\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 192143.6250 - regression_loss: 54225.2344 - val_loss: 21201.0996 - val_regression_loss: 10597.9697\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 223060.9844 - regression_loss: 60664.5586 - val_loss: 21788.8652 - val_regression_loss: 10843.3193\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 231062.4844 - regression_loss: 61845.8594 - val_loss: 18872.9082 - val_regression_loss: 9382.1094\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 202291.2344 - regression_loss: 54193.3477 - val_loss: 16302.9512 - val_regression_loss: 8114.7349\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 174643.2656 - regression_loss: 46669.8711 - val_loss: 15876.9219 - val_regression_loss: 7910.2773\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 160121.2656 - regression_loss: 43979.9336 - val_loss: 16665.3711 - val_regression_loss: 8304.1641\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 136187.6250 - regression_loss: 44267.4805 - val_loss: 17228.7949 - val_regression_loss: 8582.9932\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 142472.2969 - regression_loss: 44698.0195 - val_loss: 16796.9922 - val_regression_loss: 8366.0518\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 158163.7656 - regression_loss: 43483.4141 - val_loss: 15489.8398 - val_regression_loss: 7715.0703\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 152797.7500 - regression_loss: 40680.2422 - val_loss: 13933.7949 - val_regression_loss: 6944.0034\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 139547.0000 - regression_loss: 37205.6211 - val_loss: 12912.3877 - val_regression_loss: 6443.6343\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 130931.4531 - regression_loss: 34800.2266 - val_loss: 12717.0664 - val_regression_loss: 6356.9541\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 125697.4922 - regression_loss: 33972.3828 - val_loss: 12806.8135 - val_regression_loss: 6409.0708\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 122689.4688 - regression_loss: 33439.5859 - val_loss: 12337.9258 - val_regression_loss: 6174.7710\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 118002.0547 - regression_loss: 31604.0547 - val_loss: 11255.6377 - val_regression_loss: 5627.4414\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 103943.2656 - regression_loss: 28833.2168 - val_loss: 10157.7686 - val_regression_loss: 5069.6909\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 96270.9219 - regression_loss: 26514.0547 - val_loss: 9406.8877 - val_regression_loss: 4687.8550\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 92918.8203 - regression_loss: 25065.3242 - val_loss: 8785.9473 - val_regression_loss: 4376.0034\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 87532.6016 - regression_loss: 23548.5059 - val_loss: 8175.9912 - val_regression_loss: 4074.7886\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 81071.6719 - regression_loss: 21499.4492 - val_loss: 7798.5249 - val_regression_loss: 3893.2100\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 72653.5547 - regression_loss: 19362.3848 - val_loss: 7709.4478 - val_regression_loss: 3855.2463\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 65462.9961 - regression_loss: 17768.4238 - val_loss: 7362.0630 - val_regression_loss: 3683.9844\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 59293.9297 - regression_loss: 16230.4609 - val_loss: 6430.9863 - val_regression_loss: 3215.8765\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 54323.1133 - regression_loss: 14451.0439 - val_loss: 5391.0190 - val_regression_loss: 2691.0112\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 46606.8672 - regression_loss: 12969.1152 - val_loss: 4680.6182 - val_regression_loss: 2332.5430\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 44728.1055 - regression_loss: 11894.3926 - val_loss: 4299.1240 - val_regression_loss: 2142.9404\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 39713.4375 - regression_loss: 10666.9229 - val_loss: 4186.0879 - val_regression_loss: 2089.2295\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 36500.3203 - regression_loss: 9777.8613 - val_loss: 4070.4875 - val_regression_loss: 2032.2881\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 34514.5781 - regression_loss: 9109.4443 - val_loss: 3455.7556 - val_regression_loss: 1720.7429\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 30586.8750 - regression_loss: 8303.6738 - val_loss: 2863.8167 - val_regression_loss: 1419.1494\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 29432.5723 - regression_loss: 7722.5093 - val_loss: 2596.0029 - val_regression_loss: 1283.3929\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 27552.0605 - regression_loss: 7226.2773 - val_loss: 2555.5854 - val_regression_loss: 1265.5681\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 24941.0605 - regression_loss: 6669.3735 - val_loss: 2550.9265 - val_regression_loss: 1265.5779\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 24145.0605 - regression_loss: 6305.1045 - val_loss: 2336.5540 - val_regression_loss: 1158.3593\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 22034.0996 - regression_loss: 5877.8872 - val_loss: 2044.2728 - val_regression_loss: 1010.8191\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 20146.0684 - regression_loss: 5470.7041 - val_loss: 1868.1122 - val_regression_loss: 922.0432\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 12662.5859 - regression_loss: 5085.3569 - val_loss: 1817.6027 - val_regression_loss: 898.2408\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 18121.3848 - regression_loss: 4783.4585 - val_loss: 1950.0475 - val_regression_loss: 967.5854\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 17180.3203 - regression_loss: 4483.1255 - val_loss: 1825.4821 - val_regression_loss: 904.3330\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 15302.2764 - regression_loss: 4148.4009 - val_loss: 1617.7865 - val_regression_loss: 797.5820\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 14793.4473 - regression_loss: 3876.8342 - val_loss: 1588.7642 - val_regression_loss: 783.0337\n",
            "***************************** elapsed_time is:  6.111829996109009\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 163ms/step - loss: 41661.5273 - regression_loss: 11310.4219 - val_loss: 4348.3770 - val_regression_loss: 2151.9424\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 32915.1914 - regression_loss: 8870.2090 - val_loss: 3267.9270 - val_regression_loss: 1612.7512\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 24247.0195 - regression_loss: 6515.2642 - val_loss: 2044.4039 - val_regression_loss: 1001.9750\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 14639.3613 - regression_loss: 3895.0876 - val_loss: 1024.3298 - val_regression_loss: 492.9313\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 7051.2329 - regression_loss: 1843.7482 - val_loss: 806.7505 - val_regression_loss: 385.1298\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 5987.6851 - regression_loss: 1602.5264 - val_loss: 847.0621 - val_regression_loss: 406.0767\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 7170.7896 - regression_loss: 1888.0450 - val_loss: 604.5586 - val_regression_loss: 285.1608\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 5719.1733 - regression_loss: 1477.5608 - val_loss: 373.6570 - val_regression_loss: 169.7473\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 3778.5305 - regression_loss: 953.5873 - val_loss: 377.7520 - val_regression_loss: 171.7747\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 3410.8484 - regression_loss: 868.9747 - val_loss: 498.8386 - val_regression_loss: 232.3494\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 4171.9751 - regression_loss: 1043.7549 - val_loss: 544.8233 - val_regression_loss: 255.4329\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 4198.2983 - regression_loss: 1105.4839 - val_loss: 481.7139 - val_regression_loss: 223.9812\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 3828.9543 - regression_loss: 969.8375 - val_loss: 369.3358 - val_regression_loss: 167.8614\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 3034.4761 - regression_loss: 753.3681 - val_loss: 289.0043 - val_regression_loss: 127.6988\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2518.9094 - regression_loss: 623.0637 - val_loss: 277.6489 - val_regression_loss: 121.9736\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2571.7378 - regression_loss: 639.0935 - val_loss: 302.0949 - val_regression_loss: 134.1505\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 2876.6863 - regression_loss: 709.2092 - val_loss: 304.2175 - val_regression_loss: 135.2122\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 2803.8198 - regression_loss: 707.4910 - val_loss: 278.7752 - val_regression_loss: 122.5327\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 2552.8882 - regression_loss: 628.3060 - val_loss: 260.1843 - val_regression_loss: 113.2868\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2242.8550 - regression_loss: 562.5436 - val_loss: 264.4029 - val_regression_loss: 115.4220\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2268.9194 - regression_loss: 550.5292 - val_loss: 275.9674 - val_regression_loss: 121.2060\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2290.4260 - regression_loss: 562.6700 - val_loss: 273.1851 - val_regression_loss: 119.8060\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2241.0991 - regression_loss: 557.8833 - val_loss: 255.1867 - val_regression_loss: 110.7983\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 2220.0825 - regression_loss: 529.1937 - val_loss: 235.0779 - val_regression_loss: 100.7352\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2053.0215 - regression_loss: 497.5713 - val_loss: 224.6547 - val_regression_loss: 95.5094\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2042.5272 - regression_loss: 482.0924 - val_loss: 222.2826 - val_regression_loss: 94.3022\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1949.9460 - regression_loss: 472.1204 - val_loss: 221.6984 - val_regression_loss: 93.9788\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1927.4292 - regression_loss: 454.1481 - val_loss: 220.9357 - val_regression_loss: 93.5603\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1835.1567 - regression_loss: 433.4963 - val_loss: 222.3618 - val_regression_loss: 94.2320\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1740.2704 - regression_loss: 418.1649 - val_loss: 223.4201 - val_regression_loss: 94.7263\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1790.0414 - regression_loss: 410.8382 - val_loss: 219.1708 - val_regression_loss: 92.5911\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1663.0913 - regression_loss: 399.0999 - val_loss: 211.0847 - val_regression_loss: 88.5542\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1654.4053 - regression_loss: 385.7202 - val_loss: 204.2346 - val_regression_loss: 85.1401\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1659.1179 - regression_loss: 375.6165 - val_loss: 202.5165 - val_regression_loss: 84.2852\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1622.1582 - regression_loss: 371.9602 - val_loss: 204.1942 - val_regression_loss: 85.1209\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1584.1505 - regression_loss: 367.9423 - val_loss: 207.0435 - val_regression_loss: 86.5415\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1568.7830 - regression_loss: 361.8143 - val_loss: 211.0495 - val_regression_loss: 88.5482\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1565.2797 - regression_loss: 357.5446 - val_loss: 214.6398 - val_regression_loss: 90.3620\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1536.2594 - regression_loss: 355.1130 - val_loss: 215.9134 - val_regression_loss: 91.0315\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1521.8027 - regression_loss: 349.7424 - val_loss: 215.3329 - val_regression_loss: 90.7651\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1508.9963 - regression_loss: 344.6906 - val_loss: 213.9498 - val_regression_loss: 90.0913\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1499.3733 - regression_loss: 340.6389 - val_loss: 210.9934 - val_regression_loss: 88.6270\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1469.0231 - regression_loss: 335.5971 - val_loss: 207.0209 - val_regression_loss: 86.6577\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1424.9489 - regression_loss: 330.5242 - val_loss: 203.6486 - val_regression_loss: 84.9952\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1443.8042 - regression_loss: 326.5232 - val_loss: 202.0150 - val_regression_loss: 84.1924\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1415.8031 - regression_loss: 322.4124 - val_loss: 201.6098 - val_regression_loss: 83.9947\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1404.0135 - regression_loss: 318.5956 - val_loss: 201.0376 - val_regression_loss: 83.7120\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 1404.1302 - regression_loss: 315.1084 - val_loss: 200.2735 - val_regression_loss: 83.3286\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1368.4092 - regression_loss: 311.8224 - val_loss: 198.3640 - val_regression_loss: 82.3755\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1345.9047 - regression_loss: 308.5253 - val_loss: 195.2466 - val_regression_loss: 80.8208\n",
            "***************************** elapsed_time is:  6.521668195724487\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 189ms/step - loss: 74700.2969 - regression_loss: 20351.9551 - val_loss: 6831.3042 - val_regression_loss: 3398.5737\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 61244.9531 - regression_loss: 16576.2852 - val_loss: 5232.1074 - val_regression_loss: 2597.4497\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 46332.5195 - regression_loss: 12631.8770 - val_loss: 3428.6335 - val_regression_loss: 1693.1047\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 30289.8594 - regression_loss: 8122.5127 - val_loss: 1986.8329 - val_regression_loss: 968.4661\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 15993.8086 - regression_loss: 4262.2314 - val_loss: 1942.4434 - val_regression_loss: 942.4644\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 13036.2920 - regression_loss: 3407.8306 - val_loss: 2141.0083 - val_regression_loss: 1041.9686\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 13752.2910 - regression_loss: 3655.5325 - val_loss: 1341.7445 - val_regression_loss: 645.7120\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 8705.0859 - regression_loss: 2246.6887 - val_loss: 578.2811 - val_regression_loss: 268.0122\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 4247.3955 - regression_loss: 1101.3058 - val_loss: 407.8470 - val_regression_loss: 186.0359\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 4311.9263 - regression_loss: 1115.8275 - val_loss: 560.8773 - val_regression_loss: 264.4151\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 6082.1450 - regression_loss: 1615.7783 - val_loss: 632.6476 - val_regression_loss: 300.9011\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 6451.1372 - regression_loss: 1737.8593 - val_loss: 535.7471 - val_regression_loss: 252.1857\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 5438.7354 - regression_loss: 1400.5837 - val_loss: 394.4233 - val_regression_loss: 180.7158\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 3630.7556 - regression_loss: 927.8534 - val_loss: 350.3934 - val_regression_loss: 157.6088\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2756.1460 - regression_loss: 687.9999 - val_loss: 419.5068 - val_regression_loss: 191.0053\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 2881.8269 - regression_loss: 730.6763 - val_loss: 494.2754 - val_regression_loss: 227.4066\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3266.8123 - regression_loss: 817.0916 - val_loss: 488.6963 - val_regression_loss: 224.0766\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 3114.5952 - regression_loss: 773.6085 - val_loss: 436.7816 - val_regression_loss: 198.1147\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2709.5811 - regression_loss: 670.6332 - val_loss: 398.7349 - val_regression_loss: 179.4882\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2558.4414 - regression_loss: 623.6082 - val_loss: 370.1678 - val_regression_loss: 165.8065\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 2490.1453 - regression_loss: 610.4838 - val_loss: 331.4167 - val_regression_loss: 147.0640\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2366.5999 - regression_loss: 569.1128 - val_loss: 294.9534 - val_regression_loss: 129.4734\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2011.1138 - regression_loss: 501.1685 - val_loss: 285.8685 - val_regression_loss: 125.5093\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 2026.8885 - regression_loss: 482.1507 - val_loss: 296.4280 - val_regression_loss: 131.1834\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 2049.9822 - regression_loss: 492.4936 - val_loss: 297.3231 - val_regression_loss: 131.6929\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1985.0475 - regression_loss: 484.4997 - val_loss: 278.9436 - val_regression_loss: 122.2424\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1845.7441 - regression_loss: 444.7123 - val_loss: 259.6310 - val_regression_loss: 112.1713\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1698.2544 - regression_loss: 408.6523 - val_loss: 253.4375 - val_regression_loss: 108.6572\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1666.5101 - regression_loss: 399.2133 - val_loss: 253.1328 - val_regression_loss: 108.1595\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1679.0261 - regression_loss: 396.7072 - val_loss: 250.9525 - val_regression_loss: 106.8197\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1634.2815 - regression_loss: 383.2650 - val_loss: 248.7729 - val_regression_loss: 105.6001\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1556.6088 - regression_loss: 371.0057 - val_loss: 246.9285 - val_regression_loss: 104.7148\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1544.7734 - regression_loss: 365.0106 - val_loss: 240.5684 - val_regression_loss: 101.6825\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1571.6158 - regression_loss: 358.0866 - val_loss: 229.7533 - val_regression_loss: 96.4924\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1500.3726 - regression_loss: 350.7294 - val_loss: 219.7141 - val_regression_loss: 91.6771\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 1517.4313 - regression_loss: 348.0398 - val_loss: 212.7451 - val_regression_loss: 88.3037\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1478.2539 - regression_loss: 347.4917 - val_loss: 208.6709 - val_regression_loss: 86.2719\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1493.9182 - regression_loss: 342.5222 - val_loss: 207.0617 - val_regression_loss: 85.3653\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1459.8087 - regression_loss: 336.6298 - val_loss: 207.9710 - val_regression_loss: 85.6649\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1441.1875 - regression_loss: 331.9968 - val_loss: 207.3967 - val_regression_loss: 85.2412\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1429.8466 - regression_loss: 328.2570 - val_loss: 204.7176 - val_regression_loss: 83.8297\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1429.1920 - regression_loss: 324.4441 - val_loss: 200.9478 - val_regression_loss: 81.9265\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1402.1151 - regression_loss: 321.8461 - val_loss: 198.0124 - val_regression_loss: 80.4732\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1381.7803 - regression_loss: 319.1284 - val_loss: 194.9942 - val_regression_loss: 78.9943\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1383.1580 - regression_loss: 315.4292 - val_loss: 192.6258 - val_regression_loss: 77.8363\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1383.9404 - regression_loss: 312.4868 - val_loss: 189.9772 - val_regression_loss: 76.5535\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 1370.1863 - regression_loss: 310.3691 - val_loss: 186.7265 - val_regression_loss: 74.9759\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1351.7294 - regression_loss: 308.1830 - val_loss: 183.9015 - val_regression_loss: 73.5947\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1352.2473 - regression_loss: 305.9013 - val_loss: 182.1167 - val_regression_loss: 72.6942\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1334.9050 - regression_loss: 303.7534 - val_loss: 181.4351 - val_regression_loss: 72.3200\n",
            "***************************** elapsed_time is:  6.213181972503662\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 162ms/step - loss: 82268.7188 - regression_loss: 22179.9062 - val_loss: 7517.3906 - val_regression_loss: 3719.5898\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 65342.1172 - regression_loss: 17590.3105 - val_loss: 5501.8447 - val_regression_loss: 2718.2759\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 47055.7852 - regression_loss: 12759.1230 - val_loss: 3318.8176 - val_regression_loss: 1634.0918\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 28144.7559 - regression_loss: 7542.3926 - val_loss: 1602.1602 - val_regression_loss: 783.6412\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 12752.4951 - regression_loss: 3378.8391 - val_loss: 1372.7343 - val_regression_loss: 675.8091\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 9390.3594 - regression_loss: 2545.0757 - val_loss: 1899.8236 - val_regression_loss: 940.3226\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 13161.4473 - regression_loss: 3569.2080 - val_loss: 1557.1510 - val_regression_loss: 764.9540\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 10896.6045 - regression_loss: 2897.0569 - val_loss: 938.1535 - val_regression_loss: 450.9278\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 6605.3730 - regression_loss: 1712.6873 - val_loss: 668.9269 - val_regression_loss: 313.0450\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 4932.8525 - regression_loss: 1262.7211 - val_loss: 741.6072 - val_regression_loss: 347.7430\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 5869.9888 - regression_loss: 1537.2487 - val_loss: 840.1342 - val_regression_loss: 396.9471\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 6886.1138 - regression_loss: 1803.0371 - val_loss: 794.3986 - val_regression_loss: 375.2148\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 6631.7183 - regression_loss: 1700.4064 - val_loss: 644.6189 - val_regression_loss: 302.0906\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 5287.7866 - regression_loss: 1339.2550 - val_loss: 512.4827 - val_regression_loss: 237.8929\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 3866.2119 - regression_loss: 991.0284 - val_loss: 482.2683 - val_regression_loss: 224.3606\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 3456.4719 - regression_loss: 880.5903 - val_loss: 538.6635 - val_regression_loss: 253.6295\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 3803.7346 - regression_loss: 976.1196 - val_loss: 586.8499 - val_regression_loss: 278.2292\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 4177.9819 - regression_loss: 1073.7072 - val_loss: 557.6990 - val_regression_loss: 263.6610\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3938.4338 - regression_loss: 1027.4269 - val_loss: 477.3253 - val_regression_loss: 223.0721\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3383.8574 - regression_loss: 878.4153 - val_loss: 413.2959 - val_regression_loss: 190.3698\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3051.7478 - regression_loss: 772.0836 - val_loss: 390.2343 - val_regression_loss: 178.0141\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3022.0618 - regression_loss: 751.2395 - val_loss: 389.0202 - val_regression_loss: 176.6436\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2983.5198 - regression_loss: 763.3641 - val_loss: 382.8185 - val_regression_loss: 172.9670\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 2970.4990 - regression_loss: 751.3602 - val_loss: 363.7091 - val_regression_loss: 163.0985\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 2888.3899 - regression_loss: 704.1470 - val_loss: 344.7465 - val_regression_loss: 153.5866\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2706.1804 - regression_loss: 654.0899 - val_loss: 336.2914 - val_regression_loss: 149.6054\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 2534.9473 - regression_loss: 630.7594 - val_loss: 333.4532 - val_regression_loss: 148.6335\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2537.5925 - regression_loss: 620.4299 - val_loss: 325.9541 - val_regression_loss: 145.3613\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2476.4153 - regression_loss: 603.2083 - val_loss: 315.2472 - val_regression_loss: 140.3847\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2351.9058 - regression_loss: 575.2486 - val_loss: 309.6823 - val_regression_loss: 137.7563\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2270.9954 - regression_loss: 554.3938 - val_loss: 310.9522 - val_regression_loss: 138.3411\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2248.5020 - regression_loss: 546.0352 - val_loss: 313.0368 - val_regression_loss: 139.2552\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2091.3418 - regression_loss: 536.0950 - val_loss: 309.9762 - val_regression_loss: 137.6019\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 2152.2690 - regression_loss: 517.3261 - val_loss: 304.9147 - val_regression_loss: 134.9723\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2057.1992 - regression_loss: 498.0466 - val_loss: 301.9415 - val_regression_loss: 133.4140\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1976.9313 - regression_loss: 485.8925 - val_loss: 301.2325 - val_regression_loss: 132.9724\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1994.0962 - regression_loss: 478.0728 - val_loss: 299.9409 - val_regression_loss: 132.2537\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1950.9602 - regression_loss: 466.7524 - val_loss: 299.1134 - val_regression_loss: 131.8107\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1922.2998 - regression_loss: 453.9919 - val_loss: 299.7698 - val_regression_loss: 132.1631\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1869.2802 - regression_loss: 445.1421 - val_loss: 299.6073 - val_regression_loss: 132.2222\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1841.0479 - regression_loss: 437.2578 - val_loss: 297.9662 - val_regression_loss: 131.6030\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1847.1434 - regression_loss: 429.2006 - val_loss: 295.5915 - val_regression_loss: 130.6099\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1752.5244 - regression_loss: 421.7377 - val_loss: 293.7503 - val_regression_loss: 129.7880\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1771.2019 - regression_loss: 415.4501 - val_loss: 292.4214 - val_regression_loss: 129.1077\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1750.7469 - regression_loss: 408.3354 - val_loss: 291.5497 - val_regression_loss: 128.5657\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1720.0905 - regression_loss: 401.0201 - val_loss: 291.4629 - val_regression_loss: 128.4059\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1705.4874 - regression_loss: 394.9847 - val_loss: 290.4963 - val_regression_loss: 127.8827\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1661.3290 - regression_loss: 389.4702 - val_loss: 288.5861 - val_regression_loss: 126.9641\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1647.8752 - regression_loss: 383.9532 - val_loss: 286.6299 - val_regression_loss: 126.0619\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1622.2988 - regression_loss: 379.2248 - val_loss: 284.6745 - val_regression_loss: 125.1593\n",
            "***************************** elapsed_time is:  6.106198787689209\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 174ms/step - loss: 64960.4766 - regression_loss: 17847.1934 - val_loss: 6287.1226 - val_regression_loss: 3114.8926\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 53378.5977 - regression_loss: 14340.4170 - val_loss: 4712.8481 - val_regression_loss: 2330.4033\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 39648.9336 - regression_loss: 10706.8223 - val_loss: 2912.1057 - val_regression_loss: 1433.4775\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 24815.5938 - regression_loss: 6573.8501 - val_loss: 1473.4296 - val_regression_loss: 718.1038\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 12335.2178 - regression_loss: 3245.6147 - val_loss: 1375.5197 - val_regression_loss: 672.7537\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 10523.8730 - regression_loss: 2899.6958 - val_loss: 1361.0062 - val_regression_loss: 665.9279\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 10611.4502 - regression_loss: 2849.1736 - val_loss: 760.9768 - val_regression_loss: 364.1564\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 6467.7710 - regression_loss: 1684.1803 - val_loss: 448.7116 - val_regression_loss: 205.9812\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 4510.2051 - regression_loss: 1165.1387 - val_loss: 590.1237 - val_regression_loss: 275.3930\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 5642.9238 - regression_loss: 1509.4886 - val_loss: 763.1615 - val_regression_loss: 361.6635\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 7019.3350 - regression_loss: 1823.6426 - val_loss: 704.3302 - val_regression_loss: 332.8215\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 6256.9419 - regression_loss: 1612.3208 - val_loss: 510.7003 - val_regression_loss: 237.0077\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 4420.2026 - regression_loss: 1133.4596 - val_loss: 373.4340 - val_regression_loss: 169.4540\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 3265.8696 - regression_loss: 820.9871 - val_loss: 383.8636 - val_regression_loss: 175.5608\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 3211.5754 - regression_loss: 839.4464 - val_loss: 459.0573 - val_regression_loss: 213.7038\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 3848.0498 - regression_loss: 1003.8240 - val_loss: 468.9977 - val_regression_loss: 218.8279\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3868.3682 - regression_loss: 1019.1338 - val_loss: 408.4979 - val_regression_loss: 188.3940\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3419.6917 - regression_loss: 875.2797 - val_loss: 356.1767 - val_regression_loss: 161.8383\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 3002.8459 - regression_loss: 762.4844 - val_loss: 346.2245 - val_regression_loss: 156.3884\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2921.0540 - regression_loss: 729.1575 - val_loss: 351.1920 - val_regression_loss: 158.4410\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2785.2766 - regression_loss: 729.3436 - val_loss: 342.4280 - val_regression_loss: 153.7455\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2870.8535 - regression_loss: 705.4006 - val_loss: 323.1183 - val_regression_loss: 143.9311\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2647.2390 - regression_loss: 652.0771 - val_loss: 314.5985 - val_regression_loss: 139.6909\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 2479.1755 - regression_loss: 621.9033 - val_loss: 318.6598 - val_regression_loss: 141.8935\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 2466.6633 - regression_loss: 612.0883 - val_loss: 317.9078 - val_regression_loss: 141.7890\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2413.5212 - regression_loss: 590.0573 - val_loss: 306.1006 - val_regression_loss: 136.1110\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2306.6899 - regression_loss: 556.4595 - val_loss: 293.1976 - val_regression_loss: 129.7665\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 2151.7375 - regression_loss: 530.8748 - val_loss: 286.3928 - val_regression_loss: 126.3658\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2125.6111 - regression_loss: 525.2601 - val_loss: 279.3333 - val_regression_loss: 122.7857\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 2170.2395 - regression_loss: 519.2803 - val_loss: 268.7636 - val_regression_loss: 117.4359\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2086.2385 - regression_loss: 497.5885 - val_loss: 261.9876 - val_regression_loss: 113.9803\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 2001.2043 - regression_loss: 476.3705 - val_loss: 262.5721 - val_regression_loss: 114.1912\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 1940.9387 - regression_loss: 466.7542 - val_loss: 264.3344 - val_regression_loss: 114.9870\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1944.1750 - regression_loss: 461.4594 - val_loss: 262.4656 - val_regression_loss: 113.9907\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1907.5859 - regression_loss: 450.3106 - val_loss: 259.6375 - val_regression_loss: 112.5639\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1845.6652 - regression_loss: 440.4068 - val_loss: 257.8352 - val_regression_loss: 111.7006\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1848.7516 - regression_loss: 434.3515 - val_loss: 256.9278 - val_regression_loss: 111.3382\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1823.6758 - regression_loss: 428.3556 - val_loss: 256.8945 - val_regression_loss: 111.4391\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1779.0104 - regression_loss: 421.0836 - val_loss: 257.1179 - val_regression_loss: 111.6401\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1784.4867 - regression_loss: 415.3653 - val_loss: 254.7997 - val_regression_loss: 110.5067\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1748.7487 - regression_loss: 410.5355 - val_loss: 249.5571 - val_regression_loss: 107.8566\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1721.5231 - regression_loss: 404.2719 - val_loss: 244.4195 - val_regression_loss: 105.2386\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1682.1183 - regression_loss: 398.7427 - val_loss: 241.9185 - val_regression_loss: 103.9622\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1703.9236 - regression_loss: 394.6586 - val_loss: 242.8780 - val_regression_loss: 104.4545\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1676.9015 - regression_loss: 389.9926 - val_loss: 245.4340 - val_regression_loss: 105.7648\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1670.1979 - regression_loss: 385.7958 - val_loss: 247.8199 - val_regression_loss: 107.0168\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1656.7207 - regression_loss: 382.4673 - val_loss: 247.9238 - val_regression_loss: 107.1054\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1641.3224 - regression_loss: 379.1960 - val_loss: 244.5250 - val_regression_loss: 105.4190\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1624.1887 - regression_loss: 375.3990 - val_loss: 242.1604 - val_regression_loss: 104.2516\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1622.8540 - regression_loss: 372.2692 - val_loss: 240.2018 - val_regression_loss: 103.2786\n",
            "***************************** elapsed_time is:  6.488702058792114\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 170ms/step - loss: 582038.1250 - regression_loss: 157567.8281 - val_loss: 63851.8594 - val_regression_loss: 31911.7168\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 539348.9375 - regression_loss: 146179.7812 - val_loss: 58254.8945 - val_regression_loss: 29114.4238\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 488780.7812 - regression_loss: 133530.6250 - val_loss: 50538.7617 - val_regression_loss: 25256.6680\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 428364.5938 - regression_loss: 116252.2031 - val_loss: 40597.8164 - val_regression_loss: 20285.2930\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 353356.5312 - regression_loss: 94316.1250 - val_loss: 29385.4922 - val_regression_loss: 14676.4121\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 259611.1094 - regression_loss: 69573.7734 - val_loss: 18842.0391 - val_regression_loss: 9399.0156\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 171927.3438 - regression_loss: 46476.2539 - val_loss: 11976.1270 - val_regression_loss: 5956.4639\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 117802.5078 - regression_loss: 31938.3320 - val_loss: 11664.7451 - val_regression_loss: 5789.8779\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 115187.7891 - regression_loss: 31459.0117 - val_loss: 13181.9912 - val_regression_loss: 6546.0894\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 129501.6328 - regression_loss: 34428.2383 - val_loss: 10847.4121 - val_regression_loss: 5385.9272\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 105673.4297 - regression_loss: 29675.8008 - val_loss: 7715.8315 - val_regression_loss: 3829.1880\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 88471.8203 - regression_loss: 23901.5430 - val_loss: 6368.5283 - val_regression_loss: 3162.4666\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 77289.3594 - regression_loss: 22205.3340 - val_loss: 6781.4351 - val_regression_loss: 3373.1050\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 81129.6719 - regression_loss: 23276.2812 - val_loss: 7418.5562 - val_regression_loss: 3693.4944\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 89200.3438 - regression_loss: 24116.9414 - val_loss: 7357.9355 - val_regression_loss: 3663.4187\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 86785.1562 - regression_loss: 23427.8320 - val_loss: 6661.0737 - val_regression_loss: 3314.2397\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 80031.0781 - regression_loss: 21298.2188 - val_loss: 5850.3872 - val_regression_loss: 2907.6274\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 70361.0234 - regression_loss: 19063.6055 - val_loss: 5363.6943 - val_regression_loss: 2662.7815\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 65789.9531 - regression_loss: 17644.0000 - val_loss: 5359.7202 - val_regression_loss: 2659.3271\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 63521.4297 - regression_loss: 17229.1621 - val_loss: 5595.2681 - val_regression_loss: 2775.9692\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 63655.1211 - regression_loss: 17239.4453 - val_loss: 5600.2256 - val_regression_loss: 2777.9241\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 62342.5859 - regression_loss: 16769.0254 - val_loss: 5224.7925 - val_regression_loss: 2590.3928\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 57420.0430 - regression_loss: 15547.8291 - val_loss: 4760.6660 - val_regression_loss: 2359.0559\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 52849.3984 - regression_loss: 14229.2236 - val_loss: 4486.7334 - val_regression_loss: 2223.1035\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 48535.4297 - regression_loss: 13331.2656 - val_loss: 4393.1187 - val_regression_loss: 2177.2573\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 45099.3867 - regression_loss: 12753.6992 - val_loss: 4307.6841 - val_regression_loss: 2135.1770\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 43064.0117 - regression_loss: 12110.9385 - val_loss: 4183.8408 - val_regression_loss: 2073.5117\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 41257.5273 - regression_loss: 11262.7832 - val_loss: 4127.5259 - val_regression_loss: 2045.2651\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 39219.2383 - regression_loss: 10492.0234 - val_loss: 4150.6069 - val_regression_loss: 2056.5757\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 32306.9727 - regression_loss: 9863.4521 - val_loss: 4015.5474 - val_regression_loss: 1988.9182\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 34482.6055 - regression_loss: 9241.6680 - val_loss: 3709.7764 - val_regression_loss: 1836.0258\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 31357.8770 - regression_loss: 8473.1445 - val_loss: 3326.2268 - val_regression_loss: 1644.4182\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 28927.7949 - regression_loss: 7769.6279 - val_loss: 3033.2119 - val_regression_loss: 1498.0259\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 25449.5000 - regression_loss: 7234.2476 - val_loss: 2827.5117 - val_regression_loss: 1395.0771\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 25160.5332 - regression_loss: 6728.2988 - val_loss: 2719.8762 - val_regression_loss: 1340.9406\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 20879.1875 - regression_loss: 6141.3682 - val_loss: 2640.6902 - val_regression_loss: 1301.2115\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 19708.1836 - regression_loss: 5715.2319 - val_loss: 2524.0073 - val_regression_loss: 1242.9745\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 18437.0605 - regression_loss: 5334.2788 - val_loss: 2317.2339 - val_regression_loss: 1139.9995\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 18508.3516 - regression_loss: 4940.4453 - val_loss: 2148.0059 - val_regression_loss: 1055.7854\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 16956.4766 - regression_loss: 4603.4233 - val_loss: 2055.0532 - val_regression_loss: 1009.4263\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 16177.4023 - regression_loss: 4295.4683 - val_loss: 2012.4067 - val_regression_loss: 987.9651\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 15287.5830 - regression_loss: 4007.1589 - val_loss: 1993.8544 - val_regression_loss: 978.4208\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 14353.2979 - regression_loss: 3755.5747 - val_loss: 1915.7478 - val_regression_loss: 939.2674\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 13101.3584 - regression_loss: 3521.9597 - val_loss: 1815.7864 - val_regression_loss: 889.2510\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 12359.0068 - regression_loss: 3313.6680 - val_loss: 1719.3551 - val_regression_loss: 841.0203\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 12118.3936 - regression_loss: 3149.0164 - val_loss: 1678.2926 - val_regression_loss: 820.2273\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 11106.2451 - regression_loss: 2982.1638 - val_loss: 1641.0953 - val_regression_loss: 801.3978\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 10635.0742 - regression_loss: 2852.7485 - val_loss: 1600.8607 - val_regression_loss: 781.1216\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 10493.0264 - regression_loss: 2737.9714 - val_loss: 1525.2866 - val_regression_loss: 743.4014\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 10045.1240 - regression_loss: 2632.5251 - val_loss: 1465.9208 - val_regression_loss: 713.7368\n",
            "***************************** elapsed_time is:  6.165985107421875\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 191ms/step - loss: 134753.6562 - regression_loss: 36456.4531 - val_loss: 14602.1143 - val_regression_loss: 7293.4678\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 114618.2812 - regression_loss: 31040.2344 - val_loss: 12056.7500 - val_regression_loss: 6018.7637\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 94442.4609 - regression_loss: 25417.7422 - val_loss: 8867.0420 - val_regression_loss: 4420.3433\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 67032.7969 - regression_loss: 18397.4551 - val_loss: 5395.9482 - val_regression_loss: 2678.7490\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 42070.2109 - regression_loss: 11109.0127 - val_loss: 2802.8750 - val_regression_loss: 1372.8076\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 22444.6582 - regression_loss: 5958.3052 - val_loss: 2479.9368 - val_regression_loss: 1202.1801\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 22809.7070 - regression_loss: 6042.3130 - val_loss: 2539.0640 - val_regression_loss: 1232.8669\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 24303.5703 - regression_loss: 6567.7222 - val_loss: 1714.0298 - val_regression_loss: 828.0824\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 17480.9434 - regression_loss: 4719.1968 - val_loss: 1125.5454 - val_regression_loss: 542.0769\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 11820.9756 - regression_loss: 3112.1660 - val_loss: 1239.8333 - val_regression_loss: 605.0462\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 11250.9717 - regression_loss: 2984.9993 - val_loss: 1659.3110 - val_regression_loss: 817.6945\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 13265.0781 - regression_loss: 3631.9473 - val_loss: 1861.4952 - val_regression_loss: 919.3127\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 14565.1514 - regression_loss: 3911.7800 - val_loss: 1682.7942 - val_regression_loss: 828.9268\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 13283.6709 - regression_loss: 3517.7092 - val_loss: 1302.0121 - val_regression_loss: 636.5960\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 10496.5303 - regression_loss: 2786.6814 - val_loss: 971.6841 - val_regression_loss: 469.1075\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 8435.2979 - regression_loss: 2228.0615 - val_loss: 835.7991 - val_regression_loss: 398.9333\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 8045.2148 - regression_loss: 2118.7825 - val_loss: 852.2448 - val_regression_loss: 405.4086\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 8395.7246 - regression_loss: 2289.6450 - val_loss: 870.2545 - val_regression_loss: 413.4536\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 8962.9893 - regression_loss: 2349.5225 - val_loss: 820.4219 - val_regression_loss: 388.5072\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 8075.4087 - regression_loss: 2149.7419 - val_loss: 765.7636 - val_regression_loss: 361.9534\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 7077.6665 - regression_loss: 1873.2810 - val_loss: 770.8890 - val_regression_loss: 365.7504\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 6615.9048 - regression_loss: 1737.6940 - val_loss: 815.1527 - val_regression_loss: 389.2245\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 6583.3149 - regression_loss: 1716.7100 - val_loss: 825.8461 - val_regression_loss: 395.6783\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 6377.3901 - regression_loss: 1688.3801 - val_loss: 767.9693 - val_regression_loss: 367.4216\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 6173.2300 - regression_loss: 1588.8394 - val_loss: 673.2577 - val_regression_loss: 320.2523\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 5639.6753 - regression_loss: 1460.1810 - val_loss: 595.7390 - val_regression_loss: 281.1693\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 5342.7612 - regression_loss: 1366.8781 - val_loss: 557.4958 - val_regression_loss: 261.2974\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 5060.4375 - regression_loss: 1308.5000 - val_loss: 545.5278 - val_regression_loss: 254.3623\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 4702.0991 - regression_loss: 1222.5172 - val_loss: 557.2058 - val_regression_loss: 259.4183\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 4455.4795 - regression_loss: 1124.7510 - val_loss: 594.0199 - val_regression_loss: 277.4205\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 4039.1746 - regression_loss: 1058.6753 - val_loss: 633.8821 - val_regression_loss: 297.3357\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 4070.2063 - regression_loss: 1023.5385 - val_loss: 640.0141 - val_regression_loss: 300.5399\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3901.1152 - regression_loss: 978.9544 - val_loss: 611.9980 - val_regression_loss: 286.7304\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 3688.9580 - regression_loss: 919.5926 - val_loss: 578.6158 - val_regression_loss: 270.2536\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 3430.0569 - regression_loss: 873.0587 - val_loss: 562.4426 - val_regression_loss: 262.4290\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3242.5823 - regression_loss: 847.6946 - val_loss: 560.7220 - val_regression_loss: 261.8822\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3237.0823 - regression_loss: 816.8945 - val_loss: 569.9215 - val_regression_loss: 266.7593\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3168.2295 - regression_loss: 780.7101 - val_loss: 581.9103 - val_regression_loss: 272.8210\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 2992.3455 - regression_loss: 751.9201 - val_loss: 580.1862 - val_regression_loss: 271.7812\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2883.8362 - regression_loss: 721.9998 - val_loss: 564.0550 - val_regression_loss: 263.3974\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2822.5115 - regression_loss: 689.5370 - val_loss: 544.5473 - val_regression_loss: 253.3546\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2733.1321 - regression_loss: 663.5963 - val_loss: 529.2435 - val_regression_loss: 245.6449\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2641.3259 - regression_loss: 641.7635 - val_loss: 516.4413 - val_regression_loss: 239.4825\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 2521.6074 - regression_loss: 617.2253 - val_loss: 504.4523 - val_regression_loss: 233.7966\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2453.9014 - regression_loss: 597.5511 - val_loss: 489.1613 - val_regression_loss: 226.3425\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2390.0142 - regression_loss: 579.1026 - val_loss: 470.9544 - val_regression_loss: 217.3200\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2287.4316 - regression_loss: 560.4835 - val_loss: 454.2098 - val_regression_loss: 208.9633\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2244.7949 - regression_loss: 544.0081 - val_loss: 441.9737 - val_regression_loss: 202.8196\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2189.2141 - regression_loss: 526.7573 - val_loss: 433.1486 - val_regression_loss: 198.4494\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2141.0518 - regression_loss: 510.0398 - val_loss: 426.2627 - val_regression_loss: 195.0331\n",
            "***************************** elapsed_time is:  5.950837135314941\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 222ms/step - loss: 113744.2109 - regression_loss: 30732.7188 - val_loss: 12628.4961 - val_regression_loss: 6257.2056\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 97125.4219 - regression_loss: 26438.5781 - val_loss: 10628.9473 - val_regression_loss: 5263.6162\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 81298.2109 - regression_loss: 21928.6543 - val_loss: 7978.1050 - val_regression_loss: 3947.0544\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 59832.4609 - regression_loss: 16044.2178 - val_loss: 4902.7568 - val_regression_loss: 2420.3230\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 35465.0156 - regression_loss: 9392.3174 - val_loss: 2177.6072 - val_regression_loss: 1069.1895\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 14425.3447 - regression_loss: 3808.0061 - val_loss: 1005.1071 - val_regression_loss: 491.5909\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 6949.3271 - regression_loss: 1836.2380 - val_loss: 1787.2694 - val_regression_loss: 883.1633\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 15345.6729 - regression_loss: 4166.1328 - val_loss: 2142.6548 - val_regression_loss: 1055.7921\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 18812.7207 - regression_loss: 5045.4185 - val_loss: 1502.2134 - val_regression_loss: 733.6392\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 13181.5176 - regression_loss: 3440.7837 - val_loss: 912.7182 - val_regression_loss: 438.4088\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 7239.6577 - regression_loss: 1876.8262 - val_loss: 816.8766 - val_regression_loss: 389.5395\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 5520.9834 - regression_loss: 1465.4421 - val_loss: 1023.6514 - val_regression_loss: 491.8175\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 6693.3574 - regression_loss: 1785.5165 - val_loss: 1220.0189 - val_regression_loss: 589.4750\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 8101.4160 - regression_loss: 2162.5454 - val_loss: 1247.1938 - val_regression_loss: 603.4541\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 8471.2188 - regression_loss: 2227.0242 - val_loss: 1113.6677 - val_regression_loss: 537.8781\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 7631.1309 - regression_loss: 1982.3928 - val_loss: 914.1658 - val_regression_loss: 439.7095\n",
            "Epoch 17/50\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 6277.6279 - regression_loss: 2999.6497\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 6007.2856 - regression_loss: 1612.8397 - val_loss: 754.1292 - val_regression_loss: 361.1964\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 5255.1792 - regression_loss: 1350.9451 - val_loss: 709.3654 - val_regression_loss: 339.3987\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 4994.1899 - regression_loss: 1290.6232 - val_loss: 688.2750 - val_regression_loss: 329.2588\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 4977.7100 - regression_loss: 1280.0786 - val_loss: 683.1270 - val_regression_loss: 326.9066\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 4979.2007 - regression_loss: 1296.6141 - val_loss: 683.5515 - val_regression_loss: 327.1766\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 5094.6235 - regression_loss: 1312.5085 - val_loss: 681.1186 - val_regression_loss: 325.9025\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 5048.0483 - regression_loss: 1312.2645 - val_loss: 672.8658 - val_regression_loss: 321.6320\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 4817.6597 - regression_loss: 1290.0465 - val_loss: 661.0993 - val_regression_loss: 315.5627\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 4876.8765 - regression_loss: 1252.3519 - val_loss: 650.2797 - val_regression_loss: 309.9449\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 4728.3667 - regression_loss: 1212.1281 - val_loss: 643.6122 - val_regression_loss: 306.3999\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 4698.3193 - regression_loss: 1178.1602 - val_loss: 641.7568 - val_regression_loss: 305.2840\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 4534.4102 - regression_loss: 1155.8167 - val_loss: 642.5760 - val_regression_loss: 305.5440\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 4494.2466 - regression_loss: 1142.6707 - val_loss: 642.6664 - val_regression_loss: 305.5057\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 4284.2383 - regression_loss: 1133.7812 - val_loss: 639.6412 - val_regression_loss: 303.9711\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 4290.6567 - regression_loss: 1122.4130 - val_loss: 632.0944 - val_regression_loss: 300.2311\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 4370.5781 - regression_loss: 1106.8367 - val_loss: 620.8747 - val_regression_loss: 294.7173\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3959.1257 - regression_loss: 1085.2056 - val_loss: 607.8724 - val_regression_loss: 288.3428\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 4223.2861 - regression_loss: 1064.6486 - val_loss: 594.3400 - val_regression_loss: 281.7307\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 4016.5359 - regression_loss: 1042.3391 - val_loss: 582.2730 - val_regression_loss: 275.8269\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 4054.5410 - regression_loss: 1023.8029 - val_loss: 571.9682 - val_regression_loss: 270.7925\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 3986.4326 - regression_loss: 1006.7271 - val_loss: 562.9177 - val_regression_loss: 266.3514\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 3856.6812 - regression_loss: 989.3681 - val_loss: 554.4363 - val_regression_loss: 262.1356\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3651.2134 - regression_loss: 970.8292 - val_loss: 546.7562 - val_regression_loss: 258.2774\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3679.6797 - regression_loss: 951.4403 - val_loss: 539.4899 - val_regression_loss: 254.6059\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 3657.0291 - regression_loss: 930.3337 - val_loss: 533.3016 - val_regression_loss: 251.4606\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 3584.8755 - regression_loss: 909.0707 - val_loss: 527.4525 - val_regression_loss: 248.4693\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3533.9790 - regression_loss: 888.3820 - val_loss: 521.6894 - val_regression_loss: 245.5218\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 3457.7278 - regression_loss: 868.8711 - val_loss: 516.0219 - val_regression_loss: 242.6461\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 3310.9216 - regression_loss: 848.9878 - val_loss: 509.7032 - val_regression_loss: 239.4725\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3240.6575 - regression_loss: 828.8110 - val_loss: 502.2556 - val_regression_loss: 235.7604\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3197.5674 - regression_loss: 808.8860 - val_loss: 493.6759 - val_regression_loss: 231.5340\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 3125.0278 - regression_loss: 788.2754 - val_loss: 485.2931 - val_regression_loss: 227.3981\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3063.1211 - regression_loss: 768.9800 - val_loss: 476.4968 - val_regression_loss: 223.0573\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3012.5547 - regression_loss: 749.3270 - val_loss: 468.4297 - val_regression_loss: 219.0549\n",
            "***************************** elapsed_time is:  6.1617326736450195\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "32zLJ4yfLOj8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "93bb9965-cd4b-42f3-eed6-ef34f417db4b"
      },
      "source": [
        "df_dragonnet=pd.DataFrame([train_result, test_result])\n",
        "df_dragonnet"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pehe_score-mean</th>\n",
              "      <th>pehe_score-median</th>\n",
              "      <th>pehe_score-std</th>\n",
              "      <th>mean_absolute-mean</th>\n",
              "      <th>mean_absolute-median</th>\n",
              "      <th>mean_absolute-std</th>\n",
              "      <th>method</th>\n",
              "      <th>train</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.895922</td>\n",
              "      <td>1.147672</td>\n",
              "      <td>4.997336</td>\n",
              "      <td>0.837232</td>\n",
              "      <td>0.360654</td>\n",
              "      <td>1.772157</td>\n",
              "      <td>Dragonnet</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.989693</td>\n",
              "      <td>1.080577</td>\n",
              "      <td>4.865770</td>\n",
              "      <td>0.915148</td>\n",
              "      <td>0.361813</td>\n",
              "      <td>2.094658</td>\n",
              "      <td>Dragonnet</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   pehe_score-mean  pehe_score-median  ...     method  train\n",
              "0         2.895922           1.147672  ...  Dragonnet   True\n",
              "1         2.989693           1.080577  ...  Dragonnet  False\n",
              "\n",
              "[2 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imeaA1mMLOkD"
      },
      "source": [
        "### 1.7.1 Neura Network Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9IhdaTnLOkF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6928850-9198-4497-fc22-1d59390a4200"
      },
      "source": [
        "random_state = 1\n",
        "\n",
        "results_df = list()\n",
        "test_scores = list()\n",
        "train_scores = list()\n",
        "\n",
        "\n",
        "\n",
        "train, test = train_test_split(\n",
        "        replications[n], train_size=train_size, random_state=random_state\n",
        "    )\n",
        "\n",
        "# REPLACE this with the function you implemented and want to evaluate\n",
        "train_ite, test_ite = causal_forest(train, test, model)\n",
        "\n",
        "# Calculate the scores and append them to a dataframe\n",
        "train_scores.append(calc_scores(train[Col.ite], train_ite, metrics))\n",
        "test_scores.append(calc_scores(test[Col.ite], test_ite, metrics))\n",
        "\n",
        "# Summarize the scores and save them in a dataframe\n",
        "train_result, test_result = summarize_scores(train_scores), summarize_scores(test_scores)\n",
        "train_result.update({'method': 'Dragonnet', 'train': True})\n",
        "test_result.update({'method': 'Dragonnet', 'train': False})"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 173ms/step - loss: 14214.2520 - regression_loss: 3783.1951 - val_loss: 1408.0223 - val_regression_loss: 683.1639\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 9455.8633 - regression_loss: 2459.1714 - val_loss: 889.2617 - val_regression_loss: 419.6762\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 5815.4253 - regression_loss: 1473.8182 - val_loss: 575.0538 - val_regression_loss: 259.1024\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 4030.9312 - regression_loss: 975.0330 - val_loss: 376.2782 - val_regression_loss: 163.1413\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 2584.3389 - regression_loss: 601.3830 - val_loss: 342.3614 - val_regression_loss: 152.8701\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2182.5352 - regression_loss: 512.9050 - val_loss: 472.8154 - val_regression_loss: 222.7929\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2899.2288 - regression_loss: 739.6091 - val_loss: 415.1308 - val_regression_loss: 193.9024\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 2576.3821 - regression_loss: 639.7289 - val_loss: 287.4392 - val_regression_loss: 127.8276\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1817.2274 - regression_loss: 432.4073 - val_loss: 240.2055 - val_regression_loss: 101.8245\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1692.1818 - regression_loss: 397.4845 - val_loss: 249.1659 - val_regression_loss: 104.9153\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1864.1870 - regression_loss: 432.7738 - val_loss: 263.5613 - val_regression_loss: 111.8912\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1851.3910 - regression_loss: 436.0755 - val_loss: 277.5934 - val_regression_loss: 119.4627\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1848.4640 - regression_loss: 422.8118 - val_loss: 275.8039 - val_regression_loss: 119.4497\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1725.6997 - regression_loss: 395.6080 - val_loss: 258.5613 - val_regression_loss: 111.9240\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1595.8065 - regression_loss: 365.2817 - val_loss: 245.9306 - val_regression_loss: 106.8776\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1559.0162 - regression_loss: 360.5272 - val_loss: 247.1534 - val_regression_loss: 108.6043\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1611.3217 - regression_loss: 377.9745 - val_loss: 249.3414 - val_regression_loss: 110.2031\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1619.0714 - regression_loss: 378.2205 - val_loss: 248.8548 - val_regression_loss: 109.7390\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1535.7156 - regression_loss: 359.8220 - val_loss: 243.8078 - val_regression_loss: 106.5480\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1450.4359 - regression_loss: 344.6531 - val_loss: 231.8360 - val_regression_loss: 99.6996\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1429.6998 - regression_loss: 330.1404 - val_loss: 221.8461 - val_regression_loss: 93.9801\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1425.3820 - regression_loss: 327.5424 - val_loss: 220.6714 - val_regression_loss: 93.0487\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1462.6133 - regression_loss: 328.5075 - val_loss: 222.7206 - val_regression_loss: 94.2427\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1433.1210 - regression_loss: 322.9512 - val_loss: 228.0286 - val_regression_loss: 97.4194\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1411.3876 - regression_loss: 317.9465 - val_loss: 227.6066 - val_regression_loss: 97.6935\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1386.4213 - regression_loss: 315.6278 - val_loss: 223.5397 - val_regression_loss: 95.9819\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1369.4034 - regression_loss: 315.8157 - val_loss: 221.4593 - val_regression_loss: 95.0622\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1384.4563 - regression_loss: 316.5164 - val_loss: 221.7275 - val_regression_loss: 95.0745\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1351.6594 - regression_loss: 311.9870 - val_loss: 222.3485 - val_regression_loss: 95.1191\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1349.1636 - regression_loss: 307.1617 - val_loss: 222.4364 - val_regression_loss: 94.8565\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1331.2667 - regression_loss: 303.9774 - val_loss: 218.2167 - val_regression_loss: 92.4564\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1287.3167 - regression_loss: 301.3793 - val_loss: 213.7257 - val_regression_loss: 90.0875\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1343.8250 - regression_loss: 299.4598 - val_loss: 213.7087 - val_regression_loss: 90.2958\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1297.7905 - regression_loss: 296.8962 - val_loss: 216.6482 - val_regression_loss: 92.1048\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1313.5079 - regression_loss: 295.9919 - val_loss: 213.8217 - val_regression_loss: 90.8441\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1306.0613 - regression_loss: 293.9889 - val_loss: 210.4095 - val_regression_loss: 89.1454\n",
            "Epoch 37/50\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 1349.0763 - regression_loss: 559.3774\n",
            "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1289.3396 - regression_loss: 292.2746 - val_loss: 210.7675 - val_regression_loss: 89.2921\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1272.5336 - regression_loss: 289.2270 - val_loss: 210.3556 - val_regression_loss: 89.0184\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1276.2142 - regression_loss: 287.3371 - val_loss: 210.0499 - val_regression_loss: 88.7922\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1277.5774 - regression_loss: 285.6501 - val_loss: 209.4922 - val_regression_loss: 88.4261\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1248.6819 - regression_loss: 283.9499 - val_loss: 207.9680 - val_regression_loss: 87.5865\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1266.3052 - regression_loss: 282.3698 - val_loss: 206.3687 - val_regression_loss: 86.7326\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1260.4012 - regression_loss: 280.8408 - val_loss: 205.4600 - val_regression_loss: 86.2681\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1250.2217 - regression_loss: 279.5772 - val_loss: 205.6954 - val_regression_loss: 86.4492\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1230.8638 - regression_loss: 278.2202 - val_loss: 205.0034 - val_regression_loss: 86.1417\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1235.0546 - regression_loss: 276.8810 - val_loss: 203.8131 - val_regression_loss: 85.5859\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1239.5082 - regression_loss: 275.5760 - val_loss: 203.1270 - val_regression_loss: 85.2917\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1243.6985 - regression_loss: 274.3008 - val_loss: 202.0387 - val_regression_loss: 84.7602\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1193.0680 - regression_loss: 272.8361 - val_loss: 201.4217 - val_regression_loss: 84.4564\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1206.8752 - regression_loss: 271.3734 - val_loss: 201.4553 - val_regression_loss: 84.4533\n",
            "***************************** elapsed_time is:  5.5646140575408936\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvttPAc8LOkJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "outputId": "dfb9d658-8c35-4df2-9d56-e92ecfd78566"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "colors = (0,0,0)\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.scatter(test.np.ite, test_ite,  c=colors, alpha=0.5)\n",
        "plt.title('Scatter of treatment effects')\n",
        "plt.xlabel('real treatment effect')\n",
        "plt.ylabel('estimated treatment effect')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlcAAAG5CAYAAACjnRHrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzde3xcZ3X/++/SXbKk0cSxLdvKOLZ8S2wUJzHFSdtEJgmQlgKd0lJ+nHLSqE1pSimH6rQQoD+1QNpfKziUXlIuTQkHTnPaMoVwK5CAYiBJgxOMsOM4cS6eyPEljscj2R5JlvT8/phLRrIue6zZc5E+79drXtbsmdn7mUfOyyvrWXs95pwTAAAA8qOi2AMAAABYSAiuAAAA8ojgCgAAII8IrgAAAPKI4AoAACCPCK4AAADyiOAKQMGZ2UfN7ISZHS32WMrR1Pkzs181sxfM7LSZXVns8QGLHcEVUKLM7BfM7CEzi5vZSTP7kZm9ep7nvMXMfjjl2OfN7KPzG21OYwhJ+mNJlzvnWqd5vdPMBnwew/NmdqOf15jl2uf9DnL8/HTz1yvp3c65RufcT+Zxbmdm6y/08wCSqoo9AADnM7NmSV+X9PuS/k1SjaRflDRSzHFNx8yqnHNjOXwkJOll59zxAl5zIZlu/tZI2lek8QCYyjnHgwePEntI2i7p1Bzv+V1J+yUNSXpC0lWp4++X9EzW8V9NHb9M0rCkcUmnJZ2SdJukc5JGU8e+lnrvKklflvSSpOckvSfruj2S/kPSFyUNSvqdacYWkPSF1OcPSfqQkpnyGyUlJE2krvf5KZ9bMuX106mxnHfN1DX+WdIRSYclfVRSZeo87ZK+J+llSSckfUlSS+q1/zd1/kTq/H8i6VJJTtJvS3pBUkzSuyS9WlJ/aq7+fspYb03Nf0zStyWtyXrNpT7/dOqz/yDJpvsdzPC7nfa7TTN//5r600k6I+kZD7+/Skl3ZP0deUzSJZJ2ZZ3ntKS3SbpYySD/lKSTkn4gqaLY/33w4FHqj6IPgAcPHuc/JDWnAoN7JN0sKTjl9V9P/aP76tQ/2uvT/7inXluVCmbelvrHcmXqtVsk/XDKuT4v6aNZzytS/+D+mZIZs3WSnpX0+tTrPUoGZG9Jvbd+mvF/QdJXJTWlApenJHWlXuuUNDDLdz/v9emuKek/JX1ayYBsuaRHJf1e6v3rJd0kqVbSslTg8Mms8z0v6cas55emAot/klQn6XVKBkFfSZ17taTjkq5Pvf/Nkg4qGSxVKRk8PpR1PpcKSlqUzDS9JOkNM/0OppmD2b7bdPPjJK33+Pv7vyX9TNImJf/uXCFp6dTzpJ7/ZWpOqlOPX5Rkxf7vgwePUn9QcwWUIOfcoKRfUPIfu89KesnM7jOzFam3/I6kv3bO/dglHXTOHUp99t+dcy865yacc/+/ktmTn8vh8q+WtMw59xfOuVHn3LOpMfxm1nseds59JXWNRPaHzawy9d4POOeGnHPPS/q4pN/KdR6myFxTyeDzlyS91zl3xiWXyP6f9BhT8/Fd59yIc+4lSZ+QdL2Ha3zEOTfsnPuOkkHpvzrnjjvnDiuZtUkXi79L0l865/a75PLknZK2mdmarHP9lXPulHMuKun7krZ5+ZKp3/GM382DuX5/vyPpQ865A6m/Oz91zr08w7nOSVqpZOB+zjn3A+ccG9ICc6DmCihRzrn9SmY5ZGablVwS+6Sktyu5jPPMdJ8zs3dKep+S2RhJalRyecerNZJWmdmprGOVSgYXaS/M8vmLlcxyHMo6dkjJ7M98ZF9zTeoaR8wsfawi/Z5UgPK3SmZamlKvxTxc41jWz4lpnjdmXf9vzezjWa+bkt8x/b2z74Q8m/XZucz63Tx+frbf34x/d6bxN0pmDb+TGstnnHN/5fGzwKJFcAWUAefck2b2eUm/lzr0gpJ1RZOkMieflXSDkpmecTPbo+Q//FIyE3be6ac8f0HSc865DbMNaZbXTiiZ8VijZM2XlFwaOzzLZ7ycO/v4C0oW91/spi9svzP1/lc5506a2Vsk/b2Ha3j1gqSPOee+dAGfnevac323ucz1+0v/3dk714mcc0NK3pn4x2a2VdL3zOzHzrkHLmBcwKLBsiBQgsxss5n9sZm1pZ5fomTG6pHUWz4nqdvMrrak9anAaomS/3i/lPrcb0vamnXqY5LazKxmyrF1Wc8flTRkZn9qZvVmVmlmW722gXDOjSt5h+PHzKwpNa73KZl58+KYpKVmFpjlGkckfUfSx82s2cwqzKzdzNJLf01KFmXHzWy1knVGU6+xThfunyR9wMy2SJKZBczs1z1+drrfQYaH7zaXuX5/n5P0ETPbkPq702FmS7PGlpkXM3tj6u+WSYorWYg/4XEcwKJFcAWUpiFJr5H032Z2Rsmgaq+SWQQ55/5d0sck/X+p935F0kXOuSeUrG96WMl/KF8l6UdZ5/2ekrfsHzWzE6lj/yzpcjM7ZWZfSQVHb1SyRug5JTNRn1PyDjav/lDJmqVnJf0wNc67vXzQOfekknfBPZsa06oZ3vpOJQu2n1Byye8/lKwPkqQ/l3SVkgHBNyRFpnz2LyV9KHX+bq9fKmuM/ynpf0m618wGlfzd3Ozx49P9Dqaa7bvNNba5fn+fUDL4/Y6Sd17+s5I3CEjJJcB7UvPyG5I2SLpfyUD1YUn/6Jz7vsfvCSxaRm0iAABA/pC5AgAAyCOCKwAAgDwiuAIAAMgjgisAAIA8Kqk+VxdffLG79NJLfb/OmTNntGTJEt+vs5gxx/5jjguDefYfc+w/5tgfjz322Ann3LKpx0squLr00ku1e/du36/T19enzs5O36+zmDHH/mOOC4N59h9z7D/m2B9mdmi64ywLAgAA5BHBFQAAQB4RXAEAAOQRwRUAAEAeEVwBAADkEcEVAABAHhFcAQAA5BHBFQAAQB4RXAEAAOQRwRUAAEAeEVwBAADkEcEVAABAHhFcAQAA5FFVsQcAAACQi/7+fkUiEUWjUYVCIYXDYXV0dBR7WBlkrgAAQNno7+9Xb2+vYrGY2traFIvF1Nvbq/7+/mIPLYPgCgAAlI1IJKJgMKhgMKiKiorMz5FIpNhDy2BZEAAAlI1oNKrq6mr19fUpHo8rEAho06ZNikajxR5aBsEVAAAoG7W1terr61Nzc7Oam5uVSCS0a9cudXZ2lkwtFsuCAACgbDjnpj1+4sSJkqnFIrgCAABlY3R0VNddd53q6+s1ODio+vp6XXfddTp48GDJ1GKxLAgAAMpGKBRSLBZTZ2dn5lgsFpNzToFAYNJ7A4FAUWqxCK4AAEBJmq6GKhwOq7e3V1IyeIrH44rFYtqxY4fi8biCwWDm8/F4XKFQqODjZlkQAACUnJn6WUlSd3e3gsGgBgYGFAwG1d3drdtvv12xWEyxWEwTExOZn8PhcMHHTuYKAACUnOx+VpIyf0YiEfX09Ex7F2B3d/ekTFdXV1dR7hYkuAIAACUnGo2qra1t0rG5aqg6OjpKYhsclgUBAEDJCYVCisfjk44Vq4YqV2SuAABAScguYK+trdULL7yg9vb2SYXrXV1dxR7mnAiuAABA0aUL2IPBoNra2hSPx2VmGhkZ0cDAgKcaqlLp0E5wBQAAim66AvZ169YpGAyqp6dnzs9PDc7Sdxd2d3cXPMAiuAIAAEXndUPmmbJTs91dWOjgioJ2AABQdLW1tdq1a5cSicSkDZlra2sz75mp91V/f7+i0aiGh4fV19enr371q+rr69Pw8DAd2gEAwOKSzkQ98MADOnnypKqqqlRXV5d5PXuj5tmyU6dPn9b3vvc9mZkaGho0NjamXbt2Tdomp1AIrgAAQFFk10k1NDSourpax48f17lz59Ta2qpt27ZpdHQ08/5076ujR4/qySefVDweV3NzsyoqKnT48GGNj4+roaFB586d05EjR9TU1DQpOCsUgisAAFAU2ZmolpYWJRIJNTY2qr6+Xp2dnYrFYlq5cmXm/aFQSE899ZT27duniYkJDQ4O6tChQ0okEqqsrFRtba3OnDmjyspK1dXV6aKLLpoUnBUKNVcAAKAootGoAoGAJGnz5s0aHh6Wc06nTp2adm/AcDisffv2aXh4WMePH9fIyIgkaWxsTMPDw6qoqNCSJUtUXV2t1tZWJRIJNm4GAACLR3YX9tbWVl1zzTUyM5lZZkPm7Dv9Ojo6tHbtWg0PD2t0dFTDw8NKJBJyzsk5p1gspsHBQY2Pj+vw4cOqrq4uysbNBFcAAKAowuFwJkM1MTGh2tpabdq0Sffcc8+MmzNv27ZNlZWVqqmp0ZIlSyRJExMTmpiYUEVFMqyJx+NKJBL68Ic/XJQmogRXAACgKDo6OtTd3a1gMKiBgYFps1VThcNhnT17VhMTE6qqqsoEVdXV1ZllwWAwqA0bNuitb31rAb/NKyhoBwAARdPR0ZFTdqmjo0OXXXaZnnjiCZ09ezaTraqoqFB9fb3WrFmjRCKhtrY2v4Y8J4IrAABQFtI9sWKxmJqamnT27FlJytwpWFNTo/r6erW3t2vjxo1FGyfBFQAAKFnpgOrBBx/UE088oYaGBtXV1eno0aOqrq7WpZdeqqNHj2piYkJbt27V0NCQHn30UQUCAfX391NzBQAAkJZuMvr0009neludPXtWJ06cUGVlpSTphRdeUGVlpSoqKvToo49Kkm644QbV1NRktsYpNIIrAABQktJNRg8fPiznnBoaGlRVVaVEIqHa2lqNjIyovr5eW7ZsUX19vSYmJjQ+Pq5HH31UP/3pTzU2NqZIJFLwcbMsCAAASlJ6u5t4PJ7ZLzB9h+CpU6c0Pj6umpoanTlzJnMH4ZEjR7R+/XolEgnt3bs3U5dVSARXAACgpKTrrH7yk59o3759qqysVHNzs44fP57p4j42NqaKigpVVVXp+eefz2SxxsfHZWaqr6/XyMiITp06VfDx+xpcmdnzkoYkjUsac85t9/N6AACgfPX39+uuu+7Sd7/7XS1dulTt7e2Z7W6qqqrU0tKS6bwuSbW1taqqqlJlZaXGxsZkZqqsrJRzTsPDw5qYmFBLS0vBv0chMlc7nXMnCnAdAABQpvr7+/XBD35Qjz/+uEZGRpRIJBSLxXTFFVcoGo0qHo+rtbVVZ8+e1bp167R8+XI99dRTqqurU21trfbv36/GxkYtXbpUg4ODCgQCWr9+vTZs2FDw78KyIAAAKLq77rpLBw8e1OjoqJYsWaKxsTEdP35cP/rRj7R8+XLV19frU5/6VKbP1cjIiKqrq3Xo0CGNj49r2bJlam9v17p16xQIBBSPx8/b+LlQ/A6unKTvmJmT9Gnn3Gd8vh4AAChDjzzyiJqampRIJHTu3Dk55zQyMqLR0VEFg0GdOnVK4XBYV111lY4cOaITJ06oqalJoVBIQ0NDam9vV2dnp772ta/p8OHDWr16td797ncXpc+VOef8O7nZaufcYTNbLum7kv7QObdryntuk3SbJK1YseLqe++917fxpJ0+fVqNjY2+X2cxY479xxwXBvPsP+bYf/OZ40QioVOnTml0dFQ1NTVqaWlRfX19nkcoPfHEE6qoqJBzTqOjo5qYmJAkOedUUVGhmpqazFY36WJ251xmb0Ez09jYmCRljtXW1ioUCvkyXknauXPnY9PVk/saXE26kFmPpNPOud6Z3rN9+3a3e/du38fS19enzs5O36+zmDHH/mOOC4N59h9z7L8LneN0E89gMDhpqW2uzZUvxLve9S49+OCDam5u1tjYmPbv35/ZnHnt2rVaunSpnHMaHBzUyMiInHM6efKkJiYmtGTJEp04cUJnz55VMBjUkiVLNDIyouHhYb3uda/Tv//7v+d1rGlmNm1w5VsTUTNbYmZN6Z8lvU7SXr+uBwAA8ivdxDMYDKqioiLzsx+NOW+//Xa1t7dLksbHx9XY2Kja2lpVVlZqcHBQp0+f1vDwsAKBgOrr6/Xcc89JkhoaGnTu3LlMP6vx8fHMJs51dXX6wQ9+kPexzsXPDu0rJP3QzH4q6VFJ33DO/ZeP1wMAAHkUjUYVCAQmHQsEAopGo3m/VkdHh+68807dfPPNuvTSS9XS0qKLLrpIjY2NGh4e1vPPP69YLKbLLrtMo6Ojcs6pquqV0nHnnMwssz1Ousno6Oho3sc6F98K2p1zz0q6wq/zAwAAf4VCIcViMQWDwcyxeDyuUCg052fTjUCj0ahCoZDC4fCcS4kdHR3q6OhQT0+P1qxZo5GREe3evVvPPvusqqur1dTUpJqaGp06dUqNjY2KxWKqqKhQc3OzampqNDo6qoqKClVWVmayWevXr5/3POSKvQUBAMC0wuGwYrGYYrGYJiYmMj/P1d4gXasVi8XU1tamWCyW0ybK6YxZa2ur3vjGN+o3fuM3tHnzZg0ODmp0dFT19fW6+OKL1dTUpMbGRo2Pj6u6ulpmpqqqqkxhe01NjbZs2TLvecgVwRUAAJhWR0eHuru7FQwGNTAwoGAw6KmYfWqt1ujoqA4cOKB3vvOd6unpmTPICoVCisfjmeetra3atm2b3vGOd0hKBk0vvfRSZikwkUgokUho6dKlma7ty5Yt0xve8AY1NTXNfyJyRBNRAAAwo/RS3VSzLfulN1yWpGPHjumhhx5SbW2tJGWyWLMFaeFwWL29yeYC2Xcp/uIv/qK++MUv6qKLLtLatWt19OhRDQ0Nqbq6Wg0NDZltcSorK/XqV79aq1atmrSkWShkrgAAQE7mWvbLzjzt379fdXV1MjO1tLR4uuNwpozZ3r17tXTpUp09e1Yvv/yyxsfHM5s4p2uuampq5JzT97//fT3zzDNF6dBOcAUAAHKSXvYbHR3Vrl27tGvXLh04cEB33XWXpMm1WqdOncpspLx582ZJM99x2N/fr56eHr3lLW/Re97zHu3Zs2dSViwajeqSSy7R4cOHlUgkVFNTo3PnzmloaEjBYFCXXnppJntlZrrkkkuK0qGdZUEAAJCTaDSq6upqPfLII6qrq1Nzc7MSiYS++93vqr+/P5N5ikQiMjOZma655hq1trZKmnzHYXp5cc+ePXruuefU1tamF154QRUVFTp58qQaGhoyy4ihUEjf+ta31NbWpqGhIQ0PD6u6ujqTvWpsbFRjY6MSiYTq6uo0MjJSlPkhcwUAAHISCoW0Z88e1dXVqb6+PhNALV26NLPcl26pcM8992jTpk2qra09747D7OXFWCwmM9Pu3bvlnMtss/Piiy9mlhHD4bBefvll1dfXa82aNVqzZo1WrlypJUuWKBaL6fnnn9e+ffv0/PPPq6GhwVPLCD8QXAEAgJykgxznnJxzSiQSGh4e1rZt285b7pvtjsPsuwoHBwcVCAQ0MTGhwcFBSVJdXZ3i8bgCgYD27NmjSCSiqqoqHT58WEePHlV9fb1uuOEGbdmyRSMjIxocHFR9fb1aWlp04MABbd26tRjTw7IgAADITUdHh2688Ub96Ec/ygRTbW1tOn36tDZs2DDt+6erfcq+qzAQCCiRSKihoSGzlU16u5uDBw/queee05o1a3T99ddr165dkqRNmzappqZGg4ODeu1rX6uzZ89mgrHVq1dr7969eutb3+rXNMyIzBUAAMjZjTfeqDNnzqi1tVUbN27U2NiYHn744ZyyRdl3FW7evFnDw8OZOwtPnTqlRCKhVatWad++fdq6dauCwaBWrlypzs5ONTc368EHH9RPf/pTnTx5UmfOnNFll12mN7/5zers7FR7e7sv2/R4QXAFAABytnfvXu3YsUMtLS0aGhpSS0uLduzYob1793o+R/ZdhcuXL9eWLVtUV1enLVu2qKWlRevWrdPGjRu1du3azKbOkrRixQqFQiEdP35cx48fV2VlpY4ePaqHHnpIx44dk+R9mx4/sCwIAAByFo1GtX79em3cuDFzbGJiIqdsUfZdhdFoVBs3btT73//+85YQe3p6Ju1xeOzYMX3/+99XTU2Nli1bppdfflmHDx/W8uXL9cQTT6impkaxWExdXV35+bI5IrgCAAA5y2VT59m6uc9Uj5Vtasf2xx9/XOfOndOaNWtkZrr44oslSadPn9aLL76onTt3qqurqyg9riSCKwAAcAFm2qJmarYo3W4hGAxO6ub+pje9SXv37p024JpqaoZrZGRE69ev1+joqJ5//nkNDw+rtrZWlZWVesc73qGenh6/v/6sqLkCAAA587qp89RNnIPBoMbHx/WRj3xkxu1z5tLS0qK6ujoNDAxkOrWfPXtWsVisaO0XshFcAQAA30SjUQUCgUnHBgYGdO7cuUkB12z7DU7dy3D16tV64oknVFtbq5GRER0/flxnz57Vli1bciqo9wvLggAAIGfTLffdcccduuSSSzQyMpJZ6puuNuull17SsmXLJp1vpv0GpcnZL0nasGGDGhoaFI/HVV9fr0AgoObmZg0ODmrPnj3+fWmPyFwBAICcTV3uGxkZ0TPPPKPHH3980lLf1q1bM+0W0tvfVFdXa/Xq1ZPON1vrhOmyX9XV1aqpqdGWLVu0du1aLV26VBUVFTp16pRv39krMlcAAGBGM93pl91dXZKefPJJNTU1aXR0NLPUJyX7YWUXo4dCIX34wx/Wfffdp1gsNm0x/NRr1tbWKh6PT8p+VVZWqqqqKrNJ8/DwsCYmJtTS0lLYCZoGwRUAAMjIDmxqamp0+PBhrVu3blI2qru7+7zlvng8rurq6kkZpvRS33TtFjZu3DgpgEq3TphuufGFF16QmWndunWZYKyxsVGbN2/WsWPHdODAAUnSsmXLtHLlysJN1gwIrgAAgKTz66i+/e1va3BwUG1tbZOyUZFI5LxWDOk9/q6++urM+WZb6pupv9XU+qpgMKj29naNjIwoGAxOyn793d/9nZ5//nlJUm1trU6fPq0XXnhB/f39RetxJVFzBQAAUqbWUY2OjqqpqUn79+/PvCc7G5XdiuHKK6/U+vXrVVNTk6mtisViCofDnq7d39+vnp4efelLX9KePXt09OhRSdLRo0e1Z88ePfDAA5Kk9773verp6dHGjRt15MgRVVVVqaoqmStKZ85muuuwUMhcAQAASTqvjioQCOjs2bOZzZWlydmoqdmnqbVSXrukZ2fMVq1apXg8rocfflgbN27UU089JTPTqlWrJi1LRiIRVVVVadOmTTIzSVIikdDAwIBqa2vzNSUXhOAKAABIOn9Lm82bN+vBBx9Uc3OzJiYmFI/H9eyzz2r16tW69dZbL2grm6n6+/v1nve8R8ePH9fy5cu1YsUKnTp1SmamH//4x2ppaZFzTpdffvmkZcloNKply5bp5MmTOnHihIaGhiQls1eXXXZZHmcldywLAgAAScktbbLbJtTW1qq9vV1XXXWVBgYGNDo6KuecamtrL6iz+lTpjNXx48d18cUXK5FI6MCBA9q0aZMCgYBOnjypQCCga6+9VitWrJD0yrJkKBTS2NiYnnzySR0/flyjo6M6d+6czp49q/3791/wmPKBzBUAAJB0/h5+oVBId955ZyYb1dPTo5qamknF5lIyk3QhBeTpGq/ly5crkUiovr5eknT8+HFt27ZNZqYrrrgic51jx47p8ccf18jIiIaGhvSzn/1MlZWVMjNNTExIklatWqXh4eELHlM+EFwBAICM2Zb2ptZkSbN3Vp9qak3Wnj171NHRoc2bN+vhhx+WlLzr79ChQ4rFYrrooovU19enLVu2qKmpSbt27ZIkXXfdddqzZ4+cc6qoqFBlZWWmqaiZaWRkxPOY/EBwBQAAPJluK5vZ2i1k6+/v1wc/+EEdP35cIyMj2rdvn2KxmBoaGrRx40Zdc801evLJJ3Xo0CElEglde+21am9v1zPPPKO9e/dqeHhYLS0tuvLKK9Xa2qpHH300s+VNc3Ozqqur5ZzT2bNn1dra6mlMfqHmCgAAeDK1JiuXdgt33XWXDh48KEmZRqPnzp3T7t27FYvFtHz5cl1xxRVqbm7WDTfcoA0bNqiiokIbNmxQZ2en6uvr9frXv16tra2Zc7S0tGQyVefOndPw8LCcc1q+fLnnFhB+IHMFAAAkzbzVTdp0NVnZndVn++wjjzyipqamTF1VfX29li1bpsHBwUnNQdeuXav29vZJ4woEAjKzSVvgpLuzp7fHiUajGh8f14033qgPfehDRW0iSnAFAACm3XYm3VNqaoA1NXDx8lnn3LTXraurU09PT+Z5T0/PtEuPO3bsUCwWk5QMttJ3Ml5yySUaGRnRr/zKr5wX0BULwRUAAJh225n08bkCFi+f3bFjhx588EGZWWaj5aGhIV1//fWTzjV1W530ps7d3d2Zc053J2MpIbgCAADT3gk4PDysr371qzMu9c322al3Ed5+++0aGBjQSy+9pHg8nsk83Xjjjerp6Zl0jeylx9raWjU0NOiTn/zkrGMoJRS0AwAAhUKhSdvcHDt2TLt27VJNTc2cDUOnflY6/y7Cjo4O3Xnnnbr55pt11VVX6eabb9Ytt9yi++67T7FYbNI1pOTy4Hvf+16dOXMmb01LC4XMFQAAOG857vHHH5ckXXnllaqoqJh1mXCmpbyurq5J75tar/Wud71LBw4c0OjoqAKBgDZv3qxgMJi5xnyWKouJ4AoAAGTuBLzrrrv0ta99TYcOHdLatWsnvWd4eFhf+cpXFI1GJzXsDIVCetOb3qS9e/d63rS5v79f999/vy666CI1NzcrkUjo4Ycf1o4dOzLLifNtWlosLAsCAICMM2fO6LrrrtOrXvUqjY+P6+GHH9bRo0d19OhR7dq1S7W1taqurtaDDz6ovr4+VVdXKxaL6b777lM4HNbdd9+tcDisSCSiW2+9VT09PdMu40UiES1dulRmJjNTfX296urqtGfPnsxyopflxlJEcAUAACRNvuvv8ssvl3NOZqb9+/frJz/5iSTpqquu0oEDB9Tc3Kzm5mYdOHAg85lIJJJpyzC1jmpqgLVnzx6NjY3p6aef1tNPP62hoSE55/Tyyy9nGoDOp2lpMRFcAQAASclluHT39BUrVujaa69VIBDQiy++qNHRUV133XVasWKF4vG46urqVFdXl8kspZfrsgO0dK1WOvBK6+/v13PPPafx8fHM0uNzzz2nRCKhm266Kc2nKoEAACAASURBVLOcmF6qDAaDGhgYUDAYPK/vVimi5goAAEg6f+/AFStWqKamRjt37pSkSU08E4lE5mfpleW6aDSq6upq9fX1KR6PKxAIaNOmTZPqpCKRiLZs2aJ9+/apqqpK69evVzwel3NOv//7vz9pTLNtJF2qyFwBALBI9Pf3q6enZ8ZaqNmW4bJf27RpkwYHBzU4OKhNmzZNel9NTY127dqlRCKRKVRPt3RIi0ajWr9+va655hrV19drcHBQgUBAa9euLbtAajpkrgAAWAS8bFEz296Bkia9dv3112fuFly5cmXmfWY27fWzj6czZK2trZmNmKdueVPOCK4AAChzc22aLHnf3ma2ZTgvS3QjIyO67rrrdODAgcyy4LZt2zQyMpJ5j9e+WOWK4AoAgDLmdcPlXHtGeQnYppPOSnV2dmaOxWIxrVy5MvN8rgxZuSO4AgCgjHnNSE0tVpdm7hnlNWCbzoV2a0/Xg+UazJUiCtoBAChj2e0T0qbLSOXSM8pLO4WZzNY+YaaCeq+9sWYyV6F+oZG5AgCgjHnNSOWyFDffbWemq82aLRs2nz0E55Nl8wvBFQAAZSyX4nCvPaNyWUL0arYAaj7BXClu7kxwBQBAGfOjONyPu/lmC6BCoZCeeuopvfjii5k7DFetWqWNGzdOev90RfaluLmz78GVmVVK2i3psHPujX5fDwCAxSbfXcz9CNhmy4Zt3bpVX/jCFzL7FZ46dUrRaHRSPdhMy38NDQ2Kx+N5zbLNVyEyV38kab+k5gJcCwCARW9qhuc1r3lNzufId8A2WzYsEonommuu0eHDhxWPx9XS0qItW7Zo7969eutb3ypp5uW/0dHRSdvylELPLF/vFjSzNkm/LOlzfl4HAAAkTXfn3bFjx4p+B91sdxFGo1G1t7ers7NTb37zm9XZ2an29vZJS3sz3RU5MjJScps7m3POv5Ob/Yekv5TUJKl7umVBM7tN0m2StGLFiqvvvfde38aTdvr0aTU2Nvp+ncWMOfYfc1wYzLP/mOP8OnLkiMbGxlRV9criVG1tbWabmlI03ZjTz9Nj9vKeQtu5c+djzrntU4/7tixoZm+UdNw595iZdc70PufcZyR9RpK2b9/usju6+qWvr0+FuM5ixhz7jzkuDObZf8xxft16661qa2tTRcUri1MbN27U/fffr7vvvruII5tZdj1V9tJedgbKy3tKhZ/Lgj8v6U1m9rykeyW91sy+6OP1AABY9EKhkOLx+KRj4+PjRS3wnstsS4a5vKdU+Ja5cs59QNIHJCmVuep2zv0ffl0PAABMXzg+Pj4+bSf2UuKlgD7fRfZ+YfsbAAAWkOkyPCtWrCiLoGShKEgTUedcn6S+QlwLAIDFbmqGp6+vr3iDWYTo0A4AQB5M1z08O8CZ63UsHCwLAgAWnf7+fvX09OjWW29VT0/PvHtATddbqre3N3PeuV7HwkLmCgBQduaTBZppG5X53Hk21+bBpbi58FRk1vKHzBUAoKzMNwuUHehUVFRkfo5EIhc8ppm6h6c7jM/1erGRWcsvgisAQFmZb3DkR6AzXW+p7M2D53q92PwIOBczgisAQFmZb3DkR6ATDocVi8UUi8U0MTGR+TndW2qu14ut1DNr5YbgCgBQVuYbHPkR6MzVPbzUu4uXemat3FDQDgAoK9N1II/FYurq6vL0+XSgk1283dXVNe9AZ67u4aXcXXy+c4rJCK4AAGUlH8FRKQc6xTDTnEpST08PdxDmiOAKAFB2SjE4KvdWBlPn1I+WFYvFnMGVmdU650bmOgYAwGKVr0DE7wAtl/OXQ2+uUuUlc/WwpKs8HAMAYFHKRyDS39+vD37wgzp+/LhGRka0b98+PfbYY/rYxz6mjo6OeQdeuQaA0WhUbW1tk45xB6E3M94taGatZna1pHozu9LMrko9OiU1FGyEAACUuHy0Mrjrrrt08ODBzGcl6eDBg7rrrrvy0uQz115W3EF44WbLXL1e0i2S2iR9XJKljg9KusPfYQEAUD5CoZBisVgmYyXlHog88sgjampqUn19vSSpvr5ezjk98sgjWrFixbwzY7lmoriD8MLNGFw55+6RdI+Z/Zpz7ssFHBMAAGUlH4GIc27G4/lYoss1APSjZUW5F/175aWJ6NVm1pJ+YmZBM/uoj2MCAKCs5KNJ6I4dOzQ0NKREIiHnnBKJhIaGhrRjx468LNFdSPPUjo4O9fT06O6771ZPT8+8A6vFsn+hl4L2m51zmWVA51zMzH5J0of8GxYAAKVntszLfNtD3H777RoYGNBLL72keDyu2tpatbe36/bbb5ekeWfG5puJmm/WaTHdfegluKrMbr1gZvWSav0dFgAApcXvvk8dHR268847Zwxg8rFEN10vKy9NQvPx3RfT3YdegqsvSXrAzP4l9fy3Jd3j35AAACg9hci8zJb9ynfj1FwCpnx893wU/ZeLOYMr59z/MrOfSroxdegjzrlv+zssAMjNYimURfEstMxLLgFTPr77Yrr70EtBuyTtl/RfzrluST8wsyYfxwQAOVlMhbIonoXW9ymX3lz5+O75KPovF162v/ldSbdJukhSu6TVkv5J0g3+Dg0AvFlMhbIonoWWecllmS5f370U94T0g5fM1R9I+nklm4fKOfe0pOV+DgoAcpGP7tjAXBZa5iWX1gwL7bv7zUtB+4hzbtQs2aDdzKokTd/pDACKYDEVyqK4FlLmJdfWDAvpu/vNS3D1oJndoeQegzdJul3S1/wdFgB4t9CWa8oRNxSUJwImf3hZFny/pJck/UzS70n6pmggCqCEsGRRXNxQAEw2Y+bKzB5wzt0g6S+dc38q6bOFGxYA5Ib/Ay8ebigAJpttWXClmV0r6U1mdq8ky37ROfe4ryMDAJSFcu3/xFIm/DLbsuCfSfqwpDZJn5D08axHr/9DAwCUg3Ls/8RSJvw0W3B1xDl3s6S/cc7tnPJ4baEGCAAobbnc0l8qspcyKyoqMj9HIpFiDw0LwGzB1adSf76lEAMBAJSncryhoFR6o6U3Tr711lvV09ND5myBmK3m6pyZfUbSajP71NQXnXPv8W9YAIByUm43FJRCb7RcNk5GeZktc/VGSd+TNCzpsWkeAACUpVJYymRpcuGaMXPlnDsh6V4z2++c+2kBxwQAgK9y7U7uhz179igWi2lwcFCBQECXXXaZli1bVvJ3WWJuXjq0J8zsAUkrnHNbzaxD0puccx/1eWwAgDJTTu0N/FjK9Pr9+/v79dxzz8nMFAgElEgk9NBDD2nr1q3asGFDXseEwvPSof2zkj4g6ZwkOef6Jf2mn4MCAJSfxd7eIJfvH4lEtGXLFjnnNDw8rLq6OpmZ9u7dW9J3WcIbL5mrBufco+mNm1PGfBoPAKBMlVOndj8ybLl8/2g0qvXr16u5uVlPPvmk4vG4AoGAgsFgyc0VcucluDphZu2SnCSZ2VslHfF1VACAslMundr9uksvl++fvluxtbVVra2tknTe3YsoX16WBf9A0qclbTazw5LeK+ldvo4KAFB2yqVT+3zu0putL1Uu378U7laEf+YMrpxzzzrnbpS0TNJm59wvOOcO+T80AEA5KZeA4UIbiM5VU5XL9y/HxqvwzkvmSpLknDvjnBvyczAAgPJVSgFDvjJM2ebKeOX6/Ts6OhQOhxUKhRSNRhWJRBZN8f9C56XmCgAAT0qhU/tcNVXhcFi9vb2SkhmreDyuWCymrq6uWc/rpaYql+9Ph/aFa87MlZnVejkGAEApyHeGKS3fNWV0aF+4vGSuHpZ0lYdjAIACK6emnYWS7wxT2oVmvOYzTpSnGTNXZtZqZldLqjezK83sqtSjU1JDwUYIAJjWYm/aORO/7lrMd01ZudxdidzNlrl6vaRbJLVJ+kTW8SFJd/g4JgCAB+XUtLOQvGaYLiTrN1PG60LOFQ6Hdccdd+ill17SyMiIamtrtWzZMt155505fmOUmhkzV865e5xzOyXd4pzbmfV4k3OOBWEAKLILbSmw0HnJMOUz6zefc03Z/eS85yhPXmquvm5m/0PSpdnvd879hV+DAgDMLd3lO7urN8tKSXPVVOUz63eh54pEIlq3bp2uvvrqzLFYLLboM48LgZc+V1+V9GYl9xM8k/WYlZnVmdmjZvZTM9tnZn8+v6ECALKVS9PO2XpOFUs+s34Xei4yjwuXl+CqzTn3NufcXzvnPp5+ePjciKTXOueukLRN0hvMbMe8RgsAyCilpp0zKdWi+3wWk1/ouShoX7i8BFcPmdmrcj2xSzqdelqderhczwMAmFlHR4d6enp09913q6enp6QCK6l0eznlM+t3oecql8wjcmfOzR7vmNkTktZLek7JbJQpGTvN+V+wmVVKeiz1+X9wzv3pNO+5TdJtkrRixYqr77333ly/Q85Onz6txsZG36+zmDHH/mOOC4N5np9Dhw6purp6UqG2c07nzp3TmjVrJBVvjhOJhE6dOqXR0VHV1NSopaVF9fX1BT1XPscwG/4e+2Pnzp2POee2Tz3uJbhaM93xXDZvNrMWSf8p6Q+dc3tnet/27dvd7t27vZ72gvX19amzs9P36yxmzLH/mOPCYJ7np6en57yi+/Tznp4eScxxITDH/jCzaYOrOZcFU0HUJUrWTx2SdNbL56ac45Sk70t6Qy6fAwCUN5a+sBh52Vvwf0r6U0kfSB2qlvRFD59blspYyczqJd0k6ckLHyoAoNyUQ9E9kG9e+lz9qqQrJT0uSc65F82sycPnVkq6J1V3VSHp35xzX7/gkQIAytKF7OMHlDMvwdWoc86ZmZMkM1vi5cTOuX4lgzIAAIBFw0tw9W9m9mlJLWb2u5JulfRZf4cFAMjVhexvByD/vBS090r6D0lflrRJ0p855/7O74EBALwr1WadwGLkJXMl59x3zey/0+83s4uccyd9HRkAwLN87pUHYH7mDK7M7Pck/bmkYUkTSjURlbTO36EBALyKRqNqa2ubdIx96oDi8JK56pa01Tl3wu/BAAAuTCgUOq9ZJ/vUAcXhpRnoM0o2DgUAlCiadQKlw0vm6gNKbt7830ruLShJcs69x7dRAeDOL+Qk3awz++9MV1cXf2eAIvASXH1a0vck/UzJmisAPkvf+RUMBifd+UVna8yGZp1AafASXFU7597n+0gAZHDnF1CayCjDCy81V98ys9vMbKWZXZR++D4yYBGLRqMKBAKTjnHnF1Bc9BKDV14yV29P/fmBrGO0YgB8xJ1fQOkhowyvvGSuLnPOrc1+SLrc74EBixl3fgGlh4wyvPISXD3k8RiAPEnf+RUMBjUwMKBgMEgxO1BkoVBI8Xh80jEyypjOjMuCZtYqabWkejO7UsnO7JLULKmhAGMDFjXu/AJKSzgcVm9vr6RkxioejysWi6mrq6vII0Opma3m6vWSbpHUJukTWceHJN3h45gAACg59BKDVzMGV865eyTdY2a/5pz7cgHHBABASSKjDC/mvFvQOfdlM/tlSVsk1WUd/ws/BwYAAFCO5ixoN7N/kvQ2SX+oZN3Vr0ta4/O4AAAAypKXuwWvdc69U1LMOffnkq6RtNHfYQEAAJQnL8FVIvXnWTNbJemcpJX+DQkAAKB8eenQ/nUza5H0N5IeV7I7++d8HRUAAECZ8lLQ/pHUj182s69LqnPOxWf7DAAAwGLlpaC9wcw+bGafdc6NSFpuZm8swNgAAADKjpeaq3+RNKJkIbskHZb0Ud9GBAAAUMa8BFftzrm/VrKQXc65s3plKxwAAABk8VLQPmpm9UoWssvM2pXMZAEAsGD09/dP2tomHA7TjR0XxEvm6n9K+i9Jl5jZlyQ9IOlPfB0VAAAF1N/fr97eXsViMbW1tSkWi6m3t1f9/f3FHhrK0KyZKzOrkBSUFJa0Q8nlwD9yzp0owNgAACiISCSiYDCoYDAoSZk/I5EI2SvkbNbMlXNuQtKfOOdeds59wzn3dQIrAMBCE41GFQgEJh0LBAKKRqNFGhHKmZdlwfvNrNvMLjGzi9IP30cGAECBhEIhxeOTWzjG43GFQqEijQjlzEtw9TZJfyBpl6THUo/dfg4KAIBCCofDisViisViOnLkiL71rW/pG9/4ho4dO0bdFXLmJbi6zDm3Nvsh6XK/BwYAQKF0dHSou7tbIyMjeuCBByRJN9xwg2pqaihsR868tGJ4SNJVHo4BAFC2Ojo61Nraql/+5V/OFLSnUdiOXMwYXJlZq6TVkurN7Eq90ji0WVJDAcYGAEBBRaNRtbW1TTpGYTtyNVvm6vWSbpHUJunjeiW4GpR0h7/DAgBgen42+wyFQorFYpMyVxS2I1cz1lw55+5xzu2UdItz7rXOuZ2px5udc5ECjhEAAEn+N/vMLmyfmJjI/BwOh/NyfiwOcxa0O+e+XIiBAAAwl+xmnxUVFZmfI5H8/D9/urA9GAxqYGBAwWBQ3d3d1FshJ14K2gEAKAmFqInq6OggmMK8eGnFAABASaDZJ8rBbHcLzrrATN0VAKDQwuGwent7JSUzVvF4XLFYTF1dXUUeGfCK2ZYFfyX153JJ10r6Xur5TiX7XBFcAcAF8vOOt4UsXROVPXddXV3MHUrKjMGVc+63JcnMviPpcufckdTzlZI+X5DRAcAClL7jLRgMTrrjjcJpb6iJQqnzUnN1STqwSjkmicVtALhAft/xBqC4vNwt+ICZfVvSv6aev03S/f4NCQByV07LbHQBBxY2L32u3i3pnyRdkXp8xjn3h34PDAC88ruxZL5xxxuwsHntc/W4pCHn3P1m1mBmTc65IT8HBgBeZS+zScr8+Y//+I9qbW0tuWwWd7wBC9ucmSsz+11J/yHp06lDqyV9xc9BAUAuotGoAoHApGPDw8O6//77SzKbRRdwYGHzkrn6A0k/J+m/Jck597SZLfd1VACQg+k2292zZ4+WLl16XjYrEomURBDDHW/AwuXlbsER59xo+omZVUlyc33IzC4xs++b2RNmts/M/mg+AwWAmUy32e7LL7+sbdu2TXofReMACsFL5upBM7tDUr2Z3STpdklf8/C5MUl/7Jx73MyaJD1mZt91zj0xj/ECwHmmayx50003qaamZtL7KBovD+V05ycwHS/B1fsldUn6maTfk/RN59xn5/pQqjfWkdTPQ2a2X8l6LYIrAHk3dZktfQehRNF4OaHBKhYCc272FT4z+yPn3N/OdWyOc1wqaZekrc65wSmv3SbpNklasWLF1ffee6/X016w06dPq7Gx0ffrLGbMsf+Y47klEgmdOnVKo6OjqqmpUUtLi+rr63M6B/Psv+w5PnLkiMbGxlRV9cr/+6efr1y5slhDLHv8PfbHzp07H3PObZ963Etw9bhz7qopx37inLvSy4XNrFHSg5I+Ntdmz9u3b3e7d+/2ctp56evrU2dnp+/XWcyYY/8xx4XBPPsve45vvfVWtbW1qaLilZLgiYkJDQwM6O677y7SCMsff4/9YWbTBlczLgua2dsl/Q9Ja83svqyXmiSd9HjRaklflvSluQIrAACmu/OTWjmUm9lqrh5SsmbqYkkfzzo+JGnORjFmZpL+WdJ+59wn5jNIAMDiQINVLAQzBlfOuUOSDkm65gLP/fOSfkvSz8xsT+rYHc65b17g+QAAC9x0d352dXVRzI6yMufdgma2Q9LfSbpMUo2kSklnnHPNs33OOfdDSZaPQQIAFg8arKLceWki+veS3i7paUn1kn5H0j/4OSgAAIBy5SW4knPuoKRK59y4c+5fJL3B32EBAACUJy9NRM+aWY2kPWb210oWuXsKygAAABYbL8HVbylZZ/VuSf+XpEsk/ZqfgwKwcLCVCYDFZs4MlHPukHMu4ZwbdM79uXPufallQgCYVXork1gsNmkrk/7+Obu5AEDZmjO4MrM3mtlPzOykmQ2a2ZCZDc71OQCIRCIKBoMKBoOqqKjI/ByJ0FMYwMLlpXbqk5L+T0lLnXPNzrmmudowAIAkRaNRBQKBSccCgYCi0WiRRgQA/vNSc/WCpL1urk0IAZScYtc7sZUJgMXIS+bqTyR908w+YGbvSz/8HhiA+SmFeqdwOKxYLKZYLKaJiYnMz+FwuGBjAIBC85K5+pik05LqlOzQjjwrdnYBC1N2vZOkzJ+RSKRgf7/YygTAYuQluFrlnNvq+0gWqXR2IRgMTsoudHd38w8Q5iUajaqtrW3SsWLUO7GVCYDFxsuy4DfN7HW+j2SR4m4q+CUUCikej086Rr0TAPjPS3D1+5L+y8wStGLIP+6mgl+odwKA4vDSRLTJOVfhnKunFUP+kV2AX9L1TsFgUAMDAwoGgyw3A0ABzFhzZWabnXNPmtlV073unHvcv2EtHuFwWL29vZKSGat4PK5YLKaurq4ijwwLAfVOAFB4sxW0v0/SbZI+Ps1rTtJrfRnRIsPdVAAALCwzBlfOudtSP97snBvOfs3M6nwd1SJDdiF/aGsBACg2LwXtD3k8BhRVKTTNBABgtpqrVkmrJdWb2ZWSLPVSs6SGAowNyEkpNM0EAGC2mqvXS7pFUpuSdVfp4GpI0h3+DgvIXak0zQQALG6z1VzdI+keM/s159yXCzgm4IKwSTAAoBR4qblqM7NmS/qcmT1Ox3aUIppmAgBKgZfg6lbn3KCk10laKum3JP2Vr6MCLgBNMwEApcDLxs3pWqtfkvQF59w+M7PZPgAUC20tAADF5iVz9ZiZfUfJ4OrbZtYkacLfYQEAAJQnL5mrLknbJD3rnDtrZksl/ba/wwIAAChPXjJXTtLlkt6Ter5EEh3aAQAApuEluPpHSddIenvq+ZCkf/BtRAAAAGXMy7Lga5xzV5nZTyTJORczsxqfxwVA7JUIAOXIS+bqnJlVKrk8KDNbJgraAd+xVyIAlCcvwdWnJP2npOVm9jFJP5R0p6+jAjBpr8SKiorMz5FIpNhDAwDMYs5lQefcl8zsMUk3KNnz6i3Ouf2+jwxY5NgrEQDKk5eaKznnnpT0pM9jAZCFvRIBoDx5WRYEUATslQgA5YngCihR7JUIAOXJ07IggOJgr0QAKD8EVwAwT/QjA5CNZUEAmAf6kQGYiuAKAOaBfmQApiK4AoB5iEajCgQCk47RjwxY3AiuAGAeQqGQ4vH4pGP0IwMWNwragQIpVtEzxdb+CofD6u3tlZTMWMXjccViMXV1dRV5ZACKhcwVUADFKnqm2Np/9CMDMBWZK6AAsoueJWX+jEQivv4jXKzrLjb0IwOQjcwVUADFKnqm2BoACo/gCiiAYhU9U2wNAIVHcAUUQLE2YWbzZwAoPIIroACKVfRMsTUAFJ5vBe1mdrekN0o67pzb6td1gHJRrKJniq0BoLD8zFx9XtIbfDw/AABAyfEtuHLO7ZJ00q/zAwAAlCJzzvl3crNLJX19tmVBM7tN0m2StGLFiqvvvfde38aTdvr0aTU2Nvp+ncWMOfYfc1wYzLP/mGP/Mcf+2Llz52POue1Tjxe9iahz7jOSPiNJ27dvd52dnb5fs6+vT4W4zmLGHPuPOS4M5tl/zLH/mOPC4m5BAACAPCK4AgAAyCPfgisz+1dJD0vaZGYDZsYW8QAAYMHzrebKOfd2v84NAABQqlgWBAAAyCOCKwAAgDwiuAIAAMgjgisAAIA8IrgCAADII4IrAACAPCK4AgAAyCOCKwAAgDwiuAIAAMgjgisAAIA8IrgCAADII4IrAACAPCK4AgAAyKOqYg8AKFX9/f2KRCKKRqMKhUIKh8Pq6Ogo9rAAACWOzBUwjf7+fvX29ioWi6mtrU2xWEy9vb3q7+8v9tAAACWO4AqYRiQSUTAYVDAYVEVFRebnSCRS7KEBAEocwRUwjWg0qkAgMOlYIBBQNBot0ogAAOWC4AqYRigUUjwen3QsHo8rFAoVaUQAgHJBcAVMIxwOKxaLKRaLaWJiIvNzOBwu9tAAACWO4AqYRkdHh7q7uxUMBjUwMKBgMKju7m7uFgQAzIlWDMAMOjo6CKYAADkjcwUAAJBHBFcAAAB5RHAFAACQRwRXAAAAeURwBQAAkEcEVwAAAHlEcAUAAJBHBFcAAAB5RHAFAACQRwRXAAAAeURwBQAAkEcEVwAAAHlEcAUAAJBHBFcAAAB5RHAFAACQRwRXAAAAeURwBQAAkEcEVwAAAHlEcAUAAJBHBFcAAAB5RHAFAACQRwRXAAAAeURwBQAAkEcEVwAAAHlEcAUAAJBHBFcAAAB5RHAFAACQR74GV2b2BjM7YGYHzez9fl4LAACgFPgWXJlZpaR/kHSzpMslvd3MLvfregAAAKXAz8zVz0k66Jx71jk3KuleSW/28XoAAABFZ845f05s9lZJb3DO/U7q+W9Jeo1z7t1T3nebpNskacWKFVffe++9vown2+nTp9XY2Oj7dRYz5th/zHFhMM/+Y479xxz7Y+fOnY8557ZPPV5VjMFkc859RtJnJGn79u2us7PT92v29fWpENdZzJhj/zHHhcE8+4859h9zXFh+LgselnRJ1vO21DEAAIAFy8/g6seSNpjZWjOrkfSbku7z8XoAAABF59uyoHNuzMzeLenbkiol3e2c2+fX9QAAAEqBrzVXzrlvSvqmn9cAAAAoJXRoBwAAyCOCKwAAgDwiuAIAAMgjgisAAIA8IrgCAADII4IrAACAPCK4AgAAyCOCKwAAgDwiuAIAAMgjgisAAIA8IrgCAADII4IrAACAPCK4AgAAyCOCKwAAgDwiuAIAAMgjgisAAIA8IrgCAADII4IrAACAPCK4AgAAyKOqYg+gUPr7+xWJRBSNRnXTTTepv79fHR0dxR4WAABYYBZF5qq/v1+9vb2KxWJqa2vT2NiYent71d/fX+yhAQCABWZRBFeRSETBYFDBYFAVFRWqqqpSMBhUJBIp9tAAAMACsyiCq2g0qkAgMOlYIBBQNBot0ogAAMBCtSiCq1AopHg8PulYPB5XKBQq0ogAAMBCtSiC40SScwAABwdJREFUq3A4rFgsplgspomJCY2NjSkWiykcDhd7aAAAYIFZFMFVR0eHuru7FQwGNTAwoKqqKnV3d3O3IAAAyLtF04qho6MjE0z19fURWAEAAF8siswVAABAoRBcAQAA5BHBFQAAQB4RXAEAAOQRwRUAAEAeEVwBAADkEcEVAABAHhFcAQAA5BHBFQAAQB4RXAEAAOQRwRUAAEAeEVwBAADkkTnnij2GDDN7SdKhAlzqYkknCnCdxYw59h9zXBjMs/+YY/8xx/5Y45xbNvVgSQVXhWJmu51z24s9joWMOfYfc1wYzLP/mGP/MceFxbIgAABAHhFcAQAA5NFiDa4+U+wBLALMsf+Y48Jgnv3HHPuPOS6gRVlzBQAA4JfFmrkCAADwBcEVAABAHi3a4MrMft3M9pnZhJlxe2oemdkbzOyAmR00s/cXezwLjZndbWbHzf53e3ceolUVxnH8+8smFKdFsyB0aoIsGYoUwiamosKkjcSKSjCIiggsC9qoP4IoWiH6JyhpkdCKFokw0KwMKTRlcl+LUjKC9sWiRefpj3uG3qZx3lHveMb7/j5wmHvO3Hue530Z3nnm3Dv3al3uXKpKUoukxZI2pM+JW3PnVEWShkpaLml1ep/vz51TVUkaImmlpPm5c2kEDVtcAeuAy4AluROpEklDgKeAC4E2YJqktrxZVc5s4ILcSVTcTuD2iGgD2oEZ/jkeEH8C50XEqcB44AJJ7ZlzqqpbgY25k2gUDVtcRcTGiNicO48Kmgh8FhGfR8RfwCvAlMw5VUpELAF+yJ1HlUXE1xHxSdr+leKX0ui8WVVPFHakblNq/i+rkkkaA1wMPJs7l0bRsMWVDZjRwJc1/e34l5IdwCS1AhOAj/NmUk3pdNUq4BtgUUT4fS7fk8BdQFfuRBpFpYsrSe9KWtdL80qKmdUlqRl4A7gtIn7JnU8VRcSuiBgPjAEmSjo5d05VIukS4JuI6MydSyM5OHcCAykiJuXOoQF9BbTU9MekMbMDiqQmisJqbkTMy51P1UXET5IWU1xP6H/WKE8HcKmki4ChwGGS5kTE9Mx5VVqlV64sixXAWEnHSzoEuBp4K3NOZntEkoDngI0R8UTufKpK0lGSjkjbw4DzgU15s6qWiLgnIsZERCvF5/H7LqwGXsMWV5KmStoOnAG8LWlh7pyqICJ2AjcDCykuAn41ItbnzapaJL0MLAVOkrRd0vW5c6qgDuAa4DxJq1K7KHdSFXQMsFjSGoo/zBZFhG8VYAc8P/7GzMzMrEQNu3JlZmZmNhBcXJmZmZmVyMWVmZmZWYlcXJmZmZmVyMWVmZmZWYlcXJnZgJC0VdKoXsbvLTlOqfOVGUfSWZLWp1s5DJP0eOo/vj/im1kevhWDmfUp3VBTEbFHzyWTtBU4LSK+6zG+IyKaS4zT63xl25s4kp4GPoyIOan/MzAyInbtj/hmlodXrszsfyS1Stos6UWKR5G0SLpT0gpJayTdX7Pvm5I604rMjXXmfQQYllZy5u5rnN3Mt0nSbElb0tgkSR9J+lTSxHTccEnPS1ouaWX380YlXStpnqQFaf/HeovTy+uaLGmppE8kvSapWdINwJXAAymPt4BmoFPSVenu5G+k17pCUkeaq1nSC5LWpvfg8nrxzWyQiQg3Nze3/zSgFegC2lN/MjALEMUfZfOBs9P3RqavwygKpCNTfyswqpe5d5Qcp+d8O4FT0vGdwPNpvinAm2m/h4DpafsIYAswHLgW+Bw4nOI5bNuAlp5xeryeUcASYHjq3w3cl7ZnA1fs5rW/BJyZto+leNQOwKPAkzX7jegrvpub2+BrlX5ws5ntk20RsSxtT05tZeo3A2MpioqZkqam8ZY0/n3GOF9ExFoASeuB9yIiJK2lKL6641wq6Y7UH0pR4JD2/zkdvwE4Dviyj/zbgTbgo+LMJodQPJ6onklAWzoGigfqNqfxq7sHI+LHfsxlZoOIiysz253farYFPBwRz9TuIOkcimLgjIj4XdIHFIVKzjh/1mx31fS7+PczT8DlEbG5R5zTexy/i/qfk6J4Jt60Ovv1dBDFit0fPXLYw2nMbLDxNVdm1h8LgevSygqSRks6muL02Y+p4BlHsYpTz9+SmkqM09d8fb2eW9JF9EiasA95LwM6JJ2Q5hou6cR+zPcOcEt3R9L4tLkImFEzPqJOfDMbZFxcmVldEfEOxTVCS9PptdeBQ4EFwMGSNgKPUBQa9cwC1vR2YfZextntfH14AGhKx61P/b3KOyK+pbhW62VJayhOCY7rx3wzgdPSResbgJvS+IPACEnrJK0Gzu0rvpkNPr4Vg5mZmVmJvHJlZmZmViIXV2ZmZmYlcnFlZmZmViIXV2ZmZmYlcnFlZmZmViIXV2ZmZmYlcnFlZmZmVqJ/AJDv9nbF2cktAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6ZW3Us6LOkN"
      },
      "source": [
        "## QUESTION 6\n",
        "\n",
        "IS THE DRAGONNET NEURAL NETWORK ESTIMATING INDIVIDUAL TREATMENT EFFECTS WELL? IF YES/NO -WHY SO?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-nUD_2mugxr"
      },
      "source": [
        "# ANSWER\r\n",
        "\r\n",
        "1. Here again i see that for an indivisual which had negative real treatment effect the model predicted positive.\r\n",
        "\r\n",
        "2. One indivisual with positive real treatment effect was esitimated as having negative treatment effect.\r\n",
        "\r\n",
        "2. Estimations are scattered and i don't see 45 degree straight line.\r\n",
        "\r\n",
        "Overall: Still better than Linear Models and causal Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D09tYZt9LOkO"
      },
      "source": [
        "## 1.8 Comparison of the methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pti3CRzkLOkO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "outputId": "10772f21-7ac4-4c52-94fa-f8e6c55ff013"
      },
      "source": [
        "pd.concat([df_S_learner_LR, df_PSW_LR, df_S_learner_RF, df_T_learner_LR, df_T_learner_RF, df_causal_forest, df_dragonnet ], ignore_index=True)"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pehe_score-mean</th>\n",
              "      <th>pehe_score-median</th>\n",
              "      <th>pehe_score-std</th>\n",
              "      <th>mean_absolute-mean</th>\n",
              "      <th>mean_absolute-median</th>\n",
              "      <th>mean_absolute-std</th>\n",
              "      <th>method</th>\n",
              "      <th>train</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.633660</td>\n",
              "      <td>2.623297</td>\n",
              "      <td>8.362125</td>\n",
              "      <td>0.732443</td>\n",
              "      <td>0.238185</td>\n",
              "      <td>1.493276</td>\n",
              "      <td>S-Learner LR</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5.625971</td>\n",
              "      <td>2.635993</td>\n",
              "      <td>8.213626</td>\n",
              "      <td>1.292668</td>\n",
              "      <td>0.396246</td>\n",
              "      <td>2.474603</td>\n",
              "      <td>S-Learner LR</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5.595322</td>\n",
              "      <td>2.537818</td>\n",
              "      <td>8.244302</td>\n",
              "      <td>0.412006</td>\n",
              "      <td>0.284332</td>\n",
              "      <td>0.457697</td>\n",
              "      <td>PSW</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6.837997</td>\n",
              "      <td>3.484394</td>\n",
              "      <td>8.323623</td>\n",
              "      <td>3.783440</td>\n",
              "      <td>2.649187</td>\n",
              "      <td>3.225824</td>\n",
              "      <td>PSW</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3.105226</td>\n",
              "      <td>1.044589</td>\n",
              "      <td>4.789518</td>\n",
              "      <td>0.498694</td>\n",
              "      <td>0.129153</td>\n",
              "      <td>0.938009</td>\n",
              "      <td>S-Learner RF</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>3.312611</td>\n",
              "      <td>1.289761</td>\n",
              "      <td>5.139224</td>\n",
              "      <td>0.444874</td>\n",
              "      <td>0.137630</td>\n",
              "      <td>1.011833</td>\n",
              "      <td>S-Learner RF</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2.276868</td>\n",
              "      <td>1.055409</td>\n",
              "      <td>3.319402</td>\n",
              "      <td>0.149960</td>\n",
              "      <td>0.133955</td>\n",
              "      <td>0.129121</td>\n",
              "      <td>T-Learner LR</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2.337753</td>\n",
              "      <td>1.122464</td>\n",
              "      <td>3.263915</td>\n",
              "      <td>0.263287</td>\n",
              "      <td>0.195850</td>\n",
              "      <td>0.327846</td>\n",
              "      <td>T-Learner LR</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1.775321</td>\n",
              "      <td>0.955023</td>\n",
              "      <td>2.232431</td>\n",
              "      <td>0.125264</td>\n",
              "      <td>0.098101</td>\n",
              "      <td>0.125193</td>\n",
              "      <td>T-Learner RF</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2.508553</td>\n",
              "      <td>1.118640</td>\n",
              "      <td>3.512167</td>\n",
              "      <td>0.211167</td>\n",
              "      <td>0.123882</td>\n",
              "      <td>0.288221</td>\n",
              "      <td>T-Learner RF</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>4.183684</td>\n",
              "      <td>1.867887</td>\n",
              "      <td>6.357736</td>\n",
              "      <td>0.400383</td>\n",
              "      <td>0.167071</td>\n",
              "      <td>0.710400</td>\n",
              "      <td>causal forest</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>4.197395</td>\n",
              "      <td>1.861331</td>\n",
              "      <td>6.171123</td>\n",
              "      <td>0.773380</td>\n",
              "      <td>0.296180</td>\n",
              "      <td>1.380762</td>\n",
              "      <td>causal forest</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2.895922</td>\n",
              "      <td>1.147672</td>\n",
              "      <td>4.997336</td>\n",
              "      <td>0.837232</td>\n",
              "      <td>0.360654</td>\n",
              "      <td>1.772157</td>\n",
              "      <td>Dragonnet</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2.989693</td>\n",
              "      <td>1.080577</td>\n",
              "      <td>4.865770</td>\n",
              "      <td>0.915148</td>\n",
              "      <td>0.361813</td>\n",
              "      <td>2.094658</td>\n",
              "      <td>Dragonnet</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    pehe_score-mean  pehe_score-median  ...         method  train\n",
              "0          5.633660           2.623297  ...   S-Learner LR   True\n",
              "1          5.625971           2.635993  ...   S-Learner LR  False\n",
              "2          5.595322           2.537818  ...            PSW   True\n",
              "3          6.837997           3.484394  ...            PSW  False\n",
              "4          3.105226           1.044589  ...   S-Learner RF   True\n",
              "5          3.312611           1.289761  ...   S-Learner RF  False\n",
              "6          2.276868           1.055409  ...   T-Learner LR   True\n",
              "7          2.337753           1.122464  ...   T-Learner LR  False\n",
              "8          1.775321           0.955023  ...   T-Learner RF   True\n",
              "9          2.508553           1.118640  ...   T-Learner RF  False\n",
              "10         4.183684           1.867887  ...  causal forest   True\n",
              "11         4.197395           1.861331  ...  causal forest  False\n",
              "12         2.895922           1.147672  ...      Dragonnet   True\n",
              "13         2.989693           1.080577  ...      Dragonnet  False\n",
              "\n",
              "[14 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kl0lnoxLOkU"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "## QUESTION 7\n",
        "\n",
        "HOW DO THE DIFFERENT MODELS COMPARE IN TERMS OF MEAN PEHE ACCURACY? WHAT ASPECTS DO YOU THINK DETERMINE WHETHER ONE MODEL PERFOMS BETTER THAN ANOTER?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMINpncTQJed"
      },
      "source": [
        "# Answer\r\n",
        "PEHE score is essentially the root mean squared error of individual treatment effects, the lower the better. \r\n",
        "\r\n",
        "RMSE gives more weight to large errors...\r\n",
        "\r\n",
        "1. With default hyperparameter settings, T-Learner with Linear Regression performed the best among other causal models as it corresponds to lowest mean PEHE score on the test data.\r\n",
        "\r\n",
        "2. Worst performer is PSWEstimator with highest PEHE score on test data. \r\n",
        "\r\n",
        "3. I can see that the lowest pehe_score-std is with Dragonnet, which means that the model's error was a not varing alot across the replications. \r\n",
        "\r\n",
        "\r\n",
        "MAE gives equal weight to large and small errors...\r\n",
        "\r\n",
        "1. T-Learner with random forest gives the best MAE on test set. \r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7c5VyjVaLf_"
      },
      "source": [
        "#  Explanations:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O87TL3S6qchW"
      },
      "source": [
        "T-learners with powerful non-linear model (eg Random Forest) tend to outperform other models because we have different models for treatment t0 and treatment t1 and have the ability to model hetrogenious effects in the data.\r\n",
        "\r\n",
        "In S-learner with Linear Regression there is no interaction between our covariates and treatment variable which is responsible for poor result. We can tell this just by looking at the a particular linear equation. Also, the coefficient of treatment remains same which prevents the model to capture hetrogenious effects. Results with T learner linear regression improved sigficantly as it is able to capture hetrogenious effects.  \r\n",
        "\r\n",
        "Whereas in T-Learners we are explicitly modelling seperatly for t0 and t1 which improves the results even with Linear Regression. Theoratically same as T-learners, DragonNet is a three-headed architecture where from a common representation layer Z the network divides and half of the network learns to model for t=0 and the other t=1."
      ]
    }
  ]
}